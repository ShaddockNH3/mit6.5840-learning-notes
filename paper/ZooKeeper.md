# ZooKeeper: Wait-free coordination for Internet-scale systems

> [ZooKeeper：面向互联网规模系统的无等待协调服务](http://nil.csail.mit.edu/6.5840/2025/papers/zookeeper.pdf)

## 摘要

在本文中，我们描述了 ZooKeeper，这是一种用于协调分布式应用程序进程的服务。由于 ZooKeeper 是关键基础设施的一部分，ZooKeeper 旨在提供一个简单且高性能的内核，以便在客户端构建更复杂的协调原语。

它在一个复制的、中心化的服务中，结合了来自群组消息传递、共享寄存器和分布式锁服务的元素。ZooKeeper 暴露的接口具有共享寄存器的“无等待”（wait-free）特性，并结合了类似于分布式文件系统“缓存失效”（cache invalidations）的事件驱动机制，从而提供了一种简单而强大的协调服务。

ZooKeeper 的接口使得高性能的服务实现成为可能。除了无等待特性外，ZooKeeper 还为每个客户端提供请求的“先进先出”（FIFO）执行保证，并对所有改变 ZooKeeper 状态的请求提供“线性化”（linearizability）保证。这些设计决策使得高性能处理流水线（pipeline）的实现成为可能，其中读请求可以由本地服务器满足。

我们展示了对于目标工作负载（读写比为 2:1 到 100:1），ZooKeeper 每秒可以处理数万到数十万次事务。这种性能使得 ZooKeeper 能够被客户端应用程序广泛使用。

## 1. 简介

大规模分布式应用需要不同形式的协调。配置（Configuration）是协调最基本的形式之一。在最简单的形式中，配置仅仅是系统进程的一份操作参数列表，而更复杂的系统则拥有动态配置参数。组成员管理（Group membership）和领导者选举（Leader election）在分布式系统中也很常见：进程通常需要知道哪些其他进程是存活的，以及这些进程负责什么。锁（Locks）构成了强大的协调原语 **[注：此处原文混入了作者 Flavio P. Junqueira 和 Benjamin Reed 及其雅虎研究院的联系方式，已在翻译中整理以保持连贯]**，用于实现对关键资源的互斥访问。

协调的一种方法是为每种不同的协调需求开发专门的服务。例如，Amazon Simple Queue Service [3] 专门专注于队列。其他服务则是专门为领导者选举 [25] 和配置 [27] 而开发的。实现更强大原语的服务可以用来实现功能较弱的原语。例如，Chubby [6] 是一个具有强同步保证的锁服务。锁随后可以被用来实现领导者选举、组成员管理等。

在设计我们的协调服务时，我们不再坚持在服务端实现特定的原语，而是选择暴露一个 API，使应用开发者能够实现他们自己的原语。这种选择导向了一个协调内核（coordination kernel）的实现，它允许在不更改服务核心的情况下启用新的原语。这种方法支持适应应用需求的多种协调形式，而不是将开发者限制在一组固定的原语中。

在设计 ZooKeeper 的 API 时，我们摒弃了阻塞原语（blocking primitives），例如锁。用于协调服务的阻塞原语可能会引发问题，比如慢速或故障的客户端会对快速客户端的性能产生负面影响。如果处理请求依赖于其他客户端的响应和故障检测，服务本身的实现也会变得更加复杂。因此，ZooKeeper 实现了一个 API，用于操作按层次结构组织（类似于文件系统）的简单“无等待”（wait-free）数据对象。事实上，ZooKeeper 的 API 类似于任何其他文件系统的 API，仅看 API 签名的话，ZooKeeper 就像是没有锁方法、打开（open）和关闭（close）方法的 Chubby。然而，实现无等待数据对象使 ZooKeeper 与基于锁等阻塞原语的系统有了显著区别。

尽管无等待特性对性能和容错性很重要，但仅靠它不足以进行协调。我们还必须为操作提供顺序保证。特别是，我们发现保证所有操作的 **客户端 FIFO 顺序（FIFO client ordering）** 和 **线性化写入（linearizable writes）** 能够实现服务的高效实现，并且足以实现我们应用程序感兴趣的协调原语。事实上，我们可以利用我们的 API 为任意数量的进程实现共识（consensus），且根据 Herlihy 的层级结构，ZooKeeper 实现了一个通用对象（universal object）[14]。

ZooKeeper 服务包含一组服务器（ensemble），它们使用复制（replication）来实现高可用性和高性能。其高性能使得包含大量进程的应用程序能够使用这样一个协调内核来管理协调的方方面面。我们能够使用简单的流水线架构来实现 ZooKeeper，这使我们能够在拥有数百或数千个未决请求（outstanding requests）的同时，仍然保持低延迟。这种流水线自然地支持按 FIFO 顺序执行来自单个客户端的操作。保证客户端 FIFO 顺序使得客户端能够异步地提交操作。通过异步操作，一个客户端可以一次拥有多个未决操作。这一特性是非常理想的，例如，当一个新的客户端成为领导者（leader）并需要操作元数据并相应地更新它时。如果没有多个未决操作的可能性，初始化的时间可能是秒级的，而不是亚秒级的。

为了保证更新操作满足线性化（linearizability），我们实现了一个基于领导者的原子广播协议（leader-based atomic broadcast protocol）[23]，称为 Zab [24]。然而，ZooKeeper 应用程序的典型工作负载以读操作为主，因此扩展读取吞吐量变得非常可取。在 ZooKeeper 中，服务器在本地处理读操作，我们不使用 Zab 来对它们进行全序排序。

在客户端缓存数据是提高读取性能的一项重要技术。例如，对于一个进程来说，缓存当前领导者的标识符比每次需要知道领导者时都去探测 ZooKeeper 要有用得多。ZooKeeper 使用一种监视机制（watch mechanism），使客户端能够在不直接管理客户端缓存的情况下缓存数据。通过这种机制，客户端可以监视给定数据对象的更新，并在更新时收到通知。Chubby 直接管理客户端缓存。它通过阻塞更新来使所有缓存了正被更改数据的客户端的缓存失效。在这种设计下，如果这些客户端中的任何一个是慢速的或故障的，更新就会被延迟。Chubby 使用租约（leases）来防止故障客户端无限期地阻塞系统。然而，租约只能限制慢速或故障客户端的影响，而 ZooKeeper 的监视机制（watches）则完全避免了这个问题。

在本文中，我们讨论了 ZooKeeper 的设计和实现。通过 ZooKeeper，我们能够实现我们的应用程序所需的所有协调原语，即使只有写入是线性化的。为了验证我们的方法，我们展示了如何使用 ZooKeeper 实现一些协调原语。

综上所述，本文的主要贡献如下：

**协调内核（Coordination kernel）：** 我们提出了一种用于分布式系统的、具有放宽的一致性保证的无等待协调服务。特别是，我们描述了协调内核的设计和实现，我们已经在许多关键应用中使用该内核来实现各种协调技术。

**协调配方（Coordination recipes）：** 我们展示了如何使用 ZooKeeper 构建更高级别的协调原语，甚至是分布式应用中经常使用的阻塞和强一致性原语。

**协调经验（Experience with Coordination）：** 我们分享了一些使用 ZooKeeper 的方式，并评估了它的性能。

## 2. ZooKeeper 服务

客户端通过使用 ZooKeeper 客户端库的客户端 API 向 ZooKeeper 提交请求。除了通过客户端 API 暴露 ZooKeeper 服务接口外，客户端库还管理客户端与 ZooKeeper 服务器之间的网络连接。

在本节中，我们首先提供 ZooKeeper 服务的高层视图。然后我们讨论客户端用来与 ZooKeeper 交互的 API。

**术语。** 在本文中，我们使用 **客户端（client）** 表示 ZooKeeper 服务的用户，**服务器（server）** 表示提供 ZooKeeper 服务的进程，**znode** 表示 ZooKeeper 数据中的内存数据节点，这些节点组织在一个被称为 **数据树（data tree）** 的层级命名空间中。我们还使用术语 **更新（update）** 和 **写入（write）** 来指代任何修改数据树状态的操作。客户端在连接到 ZooKeeper 时建立一个 **会话（session）**，并获得一个会话句柄，通过该句柄发出请求。

### 2.1 服务概览

ZooKeeper 为其客户端提供了一组 **数据节点（znodes）** 的抽象，这些节点按照层级命名空间进行组织。这个层级结构中的 znodes 是客户端通过 ZooKeeper API 进行操作的数据对象。层级命名空间在文件系统中通常被使用。这是一种组织数据对象的理想方式，因为用户习惯于这种抽象，且它能更好地组织应用程序的元数据。为了引用给定的 znode，我们使用标准 UNIX 文件系统路径表示法。例如，我们使用 `/A/B/C` 来表示 znode C 的路径，其中 C 的父节点是 B，B 的父节点是 A。所有的 znodes 都存储数据，除 **临时（ephemeral）** znodes 外，所有的 znodes 都可以拥有子节点。

客户端可以创建两种类型的 znodes：

* **常规（Regular）：** 客户端通过显式地创建和删除来操作常规 znodes；
* **临时（Ephemeral）：** 客户端创建此类 znodes，它们要么被显式删除，要么在创建它们的会话终止（无论是故意终止还是由于故障）时让系统自动移除它们。

此外，在创建一个新的 znode 时，客户端可以设置一个 **顺序（sequential）** 标志。设置了顺序标志创建的节点，其名称后会附加一个单调递增计数器的值。如果 \( n \) 是新的 znode，而 \( p \) 是父 znode，那么 \( n \) 的序列值永远不会小于在 \( p \) 下创建的任何其他顺序 znode 名称中的值。

ZooKeeper 实现了 **监视（watches）** 机制，允许客户端在无需轮询（polling）的情况下及时接收变更通知。当客户端发出一个设置了监视标志（watch flag）的读操作时，该操作会像往常一样完成，但服务器承诺在返回的信息发生变化时通知客户端。监视是与会话关联的 **一次性触发器（one-time triggers）**；一旦被触发或会话关闭，它们就会被注销。监视表明发生了变更，但不会提供变更的具体内容。例如，如果客户端在 `/foo` 发生两次变更之前发出了 `getData(‘‘/foo’’, true)`，客户端将收到一个监视事件，告知客户端 `/foo` 的数据已变更。会话事件（如连接丢失事件）也会被发送到监视回调中，以便客户端知道监视事件可能会延迟。

**数据模型。** ZooKeeper 的数据模型本质上是一个具有简化 API 且仅支持完整数据读取和写入的文件系统，或者是一个具有层级键的键/值表。层级命名空间对于为不同应用程序的命名空间分配子树以及为这些子树设置访问权限非常有用。我们还利用客户端侧的目录概念来构建更高级别的原语，这将在 2.4 节中看到。

与文件系统中的文件不同，znodes 并非设计用于通用数据存储。相反，znodes 映射到客户端应用程序的抽象，通常对应于用于协调目的的元数据。为了说明这一点，在图 1 中我们有两个子树，一个用于应用程序 1（`/app1`），另一个用于应用程序 2（`/app2`）。应用程序 1 的子树实现了一个简单的组成员协议：每个客户端进程 \( p_i \) 在 `/app1` 下创建一个 znode \( p_i \)，只要该进程在运行，该节点就一直存在。

尽管 znodes 不是为通用数据存储设计的，但 ZooKeeper 确实允许客户端存储一些可用于分布式计算中的元数据或配置的信息。例如，在基于领导者（leader-based）的应用程序中，对于一个刚刚启动的应用服务器来说，了解当前哪个其他服务器是领导者是非常有用的。为了实现这一目标，我们可以让当前的领导者将此信息写入 znode 空间中的一个已知位置。Znodes 还拥有关联的元数据，包括时间戳和版本计数器，这允许客户端跟踪 znodes 的变更并根据 znode 的版本执行条件更新。

**会话。** 客户端连接到 ZooKeeper 并启动一个会话。会话有一个关联的超时时间。如果 ZooKeeper 在超过该超时时间内未从其会话接收到任何信息，则认为该客户端发生故障。当客户端显式关闭会话句柄或 ZooKeeper 检测到客户端故障时，会话结束。在一个会话内，客户端观察到一系列反映其操作执行的状态变更。会话使得客户端能够在 ZooKeeper 集合体（ensemble）内的服务器之间透明地移动，从而在跨 ZooKeeper 服务器时保持持久性。

### 2.2 客户端 API

我们在下面展示了 ZooKeeper API 的一个相关子集，并讨论了每个请求的语义。

* **create(path, data, flags)：** 创建一个路径名为 `path` 的 znode，将 `data[]` 存储在其中，并返回新 znode 的名称。`flags` 使客户端能够选择 znode 的类型：常规（regular）、临时（ephemeral），并设置顺序（sequential）标志；
* **delete(path, version)：** 如果 znode `path` 处于预期版本，则将其删除；
* **exists(path, watch)：** 如果路径名为 `path` 的 znode 存在，则返回 true，否则返回 false。`watch` 标志使客户端能够在 znode 上设置监视；
* **getData(path, watch)：** 返回与 znode 关联的数据和元数据（如版本信息）。`watch` 标志的工作方式与 `exists()` 中的相同，不同之处在于如果 znode 不存在，ZooKeeper 不会设置监视；
* **setData(path, data, version)：** 如果版本号是 znode 的当前版本，则将 `data[]` 写入 znode `path`；
* **getChildren(path, watch)：** 返回 znode 的子节点名称集合；
* **sync(path)：** 等待操作开始时所有未决的更新传播到客户端所连接的服务器。目前忽略 `path` 参数。

所有方法通过 API 都有同步和异步两个版本可用。当应用程序需要执行单个 ZooKeeper 操作且没有并发任务要执行时，它使用同步 API，因此它进行必要的 ZooKeeper 调用并阻塞。然而，异步 API 使应用程序能够并行执行多个未决的 ZooKeeper 操作和其他任务。ZooKeeper 客户端保证每个操作的相应回调按顺序被调用。

请注意，ZooKeeper 不使用句柄（handles）来访问 znodes。相反，每个请求包含被操作 znode 的完整路径。这种选择不仅简化了 API（没有 `open()` 或 `close()` 方法），而且还消除了服务器需要维护的额外状态。

每个更新方法都接受一个预期的版本号，这使得条件更新的实现成为可能。如果 znode 的实际版本号与预期版本号不匹配，更新将失败并出现意外版本错误。如果版本号为 \( -1 \)，则不执行版本检查。

### 2.3 ZooKeeper 的保证 (ZooKeeper guarantees)

ZooKeeper 有两个基本的顺序保证：

* **线性化写入（Linearizable writes）：** 所有更新 ZooKeeper 状态的请求都是可序列化的（serializable），并且遵循先后关系（respect precedence）；
* **FIFO 客户端顺序（FIFO client ordering）：** 来自给定客户端的所有请求都按照客户端发送的顺序执行。

请注意，我们对线性化的定义与 Herlihy [15] 最初提出的定义不同，我们将我们的定义称为 **A-线性化（异步线性化，A-linearizability）**。在 Herlihy 的线性化原始定义中，一个客户端一次只能有一个未决操作（客户端是单线程的）。在我们的定义中，我们允许一个客户端有多个未决操作，因此我们可以选择不保证同一客户端未决操作的特定顺序，或者保证 FIFO 顺序。我们选择了后者作为我们的属性。

值得注意的是，所有适用于线性化对象的结果也适用于 A-线性化对象，因为满足 A-线性化的系统也满足线性化。由于只有更新请求是 A-线性化的，ZooKeeper 在每个副本（replica）本地处理读请求。这允许服务随着向系统中添加服务器而线性扩展。

为了理解这两个保证是如何相互作用的，考虑以下场景。一个包含多个进程的系统选举出一个领导者（leader）来指挥工作进程（worker processes）。当一个新的领导者接管系统时，它必须更改大量的配置参数，并在完成后通知其他进程。此时我们有两个重要的需求：

* 当新领导者开始进行更改时，我们不希望其他进程开始使用正在被更改的配置；
* 如果新领导者在配置完全更新之前死亡，我们不希望进程使用这个部分的配置。

请注意，分布式锁（如 Chubby 提供的锁）可以帮助满足第一个需求，但不足以满足第二个需求。使用 ZooKeeper，新领导者可以将一个路径指定为 `ready`（就绪）znode；其他进程只有在该 znode 存在时才使用配置。新领导者通过删除 `ready`，更新各种配置 znode，然后创建 `ready` 来进行配置更改。所有这些更改都可以通过流水线（pipeline）方式异步发出，以快速更新配置状态。尽管一次更改操作的延迟约为 2 毫秒，但如果一个必须更新 5000 个不同 znode 的新领导者按顺序一个接一个地发出请求，将需要 10 秒钟；通过异步发出请求，这些请求将在不到一秒的时间内完成。由于顺序保证，如果一个进程看到了 `ready` znode，它也必须能看到新领导者所做的所有配置更改。如果新领导者在创建 `ready` znode 之前死亡，其他进程就会知道配置尚未最终确定，并且不会使用它。

上述方案仍然存在一个问题：如果一个进程在新领导者开始进行更改之前看到 `ready` 存在，然后在更改进行期间开始读取配置，会发生什么？这个问题通过 **通知的顺序保证（ordering guarantee for the notifications）** 得到解决：如果客户端正在监视（watching）变更，客户端将在看到更改后的新系统状态之前看到通知事件。因此，如果读取 `ready` znode 的进程请求在该 znode 变更时收到通知，它将在读取任何新配置之前看到告知客户端发生变更的通知。

当客户端除了 ZooKeeper 之外还有自己的通信通道时，可能会出现另一个问题。例如，考虑两个客户端 A 和 B，它们在 ZooKeeper 中有一个共享配置，并通过共享通信通道进行通信。如果 A 更改了 ZooKeeper 中的共享配置，并通过共享通信通道告诉 B 这一更改，B 会期望在重新读取配置时看到该更改。如果 B 的 ZooKeeper 副本稍微落后于 A 的副本，它可能看不到新配置。使用上述保证，B 可以通过在重新读取配置之前发出写入操作来确保它看到最新的信息。为了更有效地处理这种情况，ZooKeeper 提供了 `sync`（同步）请求：当 `sync` 后跟一个读操作时，构成了一个“慢速读取”（slow read）。`sync` 会导致服务器在处理读操作之前应用所有挂起的写请求，而没有完整写入的开销。这个原语在理念上类似于 ISIS [5] 的 `flush` 原语。

ZooKeeper 还具有以下两个 **活性（liveness）** 和 **持久性（durability）** 保证：如果大多数 ZooKeeper 服务器处于活动状态并正在通信，则服务将可用；如果 ZooKeeper 服务成功响应了一个更改请求，只要服务器的法定人数（quorum）最终能够恢复，该更改就会在任意数量的故障中持久存在。

### 2.4 原语示例

在本节中，我们展示如何使用 ZooKeeper API 来实现更强大的原语。ZooKeeper 服务对这些更强大的原语一无所知，因为它们完全是使用 ZooKeeper 客户端 API 在客户端实现的。一些常见的原语，如组成员管理（Group Membership）和配置管理（Configuration Management），也是无等待的。对于其他原语，如集合点（Rendezvous），客户端需要等待一个事件。尽管 ZooKeeper 是无等待的，但我们可以用 ZooKeeper 实现高效的阻塞原语。ZooKeeper 的顺序保证允许对系统状态进行有效的推理，而监视（watches）则允许高效的等待。

#### 配置管理 (Configuration Management)

ZooKeeper 可用于在分布式应用程序中实现动态配置。在最简单的形式中，配置存储在一个 znode \( z_c \) 中。进程启动时带有 \( z_c \) 的完整路径名。启动进程通过读取 \( z_c \) 并将监视标志（watch flag）设置为 true 来获取其配置。如果 \( z_c \) 中的配置被更新，进程会收到通知并读取新配置，再次将监视标志设置为 true。

请注意，在这个方案中，正如在大多数其他使用监视的方案中一样，监视用于确保进程拥有最新的信息。例如，如果监视 \( z_c \) 的进程收到了 \( z_c \) 变更的通知，而在它可以发出读取 \( z_c \) 的请求之前，\( z_c \) 又发生了三次变更，该进程不会再收到三个通知事件。这不会影响进程的行为，因为那三个事件只是通知进程一件它已经知道的事情：它所拥有的 \( z_c \) 信息已经过时了。

#### 集合点 (Rendezvous)

有时在分布式系统中，最终的系统配置在事前并不总是清晰的。例如，一个客户端可能想要启动一个主进程（master process）和几个工作进程（worker processes），但启动进程是由调度器完成的，所以客户端无法提前知道诸如地址和端口等信息，以便提供给工作进程去连接主进程。我们使用 ZooKeeper 通过一个集合点 znode \( z_r \) 来处理这种场景，该节点由客户端创建。客户端将 \( z_r \) 的完整路径名作为启动参数传递给主进程和工作进程。当主进程启动时，它将自己使用的地址和端口信息填入 \( z_r \)。当工作进程启动时，它们读取 \( z_r \) 并将监视设置为 true。如果 \( z_r \) 尚未被填充，工作进程将等待 \( z_r \) 更新时的通知。如果 \( z_r \) 是一个临时节点，主进程和工作进程可以监视 \( z_r \) 的删除，并在客户端结束时自行清理。

#### 组成员管理 (Group Membership)

我们利用临时节点来实现组成员管理。具体来说，我们要利用这一事实：临时节点允许我们要看到创建该节点的会话状态。我们首先指定一个 znode \( z_g \) 来代表该组。当该组的一个成员进程启动时，它在 \( z_g \) 下创建一个临时子 znode。如果每个进程都有唯一的名称或标识符，那么该名称就被用作子 znode 的名称；否则，进程在创建 znode 时使用 `SEQUENTIAL`（顺序）标志来获得一个唯一的名称分配。进程可以将进程信息放入子 znode 的数据中，例如进程使用的地址和端口。

在 \( z_g \) 下创建子 znode 后，进程正常启动。它不需要做任何其他事情。如果进程失败或结束，代表它的 \( z_g \) 下的 znode 将被自动移除。

进程可以通过简单地列出 \( z_g \) 的子节点来获取组信息。如果进程想要监控组成员的变化，可以将监视标志设置为 true，并在收到变更通知时刷新组信息（始终将监视标志设置为 true）。

#### 简单锁 (Simple Locks)

虽然 ZooKeeper 不是一个锁服务，但它可以用来实现锁。使用 ZooKeeper 的应用程序通常使用为其需求量身定制的同步原语，例如上面展示的那些。在这里，我们展示如何用 ZooKeeper 实现锁，以表明它可以实现各种各样的通用同步原语。

最简单的锁实现使用“锁文件”。锁由一个 znode 表示。为了获取锁，客户端尝试使用 `EPHEMERAL`（临时）标志创建指定的 znode。如果创建成功，客户端就持有了锁。否则，客户端可以读取该 znode 并设置监视标志，以便在当前领导者死亡时收到通知。客户端在死亡或显式删除 znode 时释放锁。其他正在等待锁的客户端一旦观察到 znode 被删除，就会再次尝试获取锁。

虽然这个简单的锁定协议通过了，但它确实存在一些问题。首先，它受 **羊群效应（herd effect）** 的影响。如果有许多客户端在等待获取锁，当锁被释放时，即使只有一个客户端能获取锁，它们也会全部争抢锁。其次，它只实现了排他锁（exclusive locking）。接下来的两个原语展示了如何克服这两个问题。

#### 无羊群效应的简单锁 (Simple Locks without Herd Effect)

我们定义一个锁 znode \( l \) 来实现这种锁。直观地说，我们将所有请求锁的客户端排成一队，每个客户端按照请求到达的顺序获得锁。因此，希望获得锁的客户端执行以下操作：

Lock (加锁)

1. `n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)`
2. `C = getChildren(l, false)`
3. `if n is lowest znode in C, exit` （如果 n 是 C 中序号最小的 znode，退出/成功）
4. `p = znode in C ordered just before n` （p 是 C 中排序在 n 紧前面的 znode）
5. `if exists(p, true) wait for watch event` （如果 p 存在，等待监视事件）
6. `goto 2`

Unlock (解锁)

1. `delete(n)`

在 Lock 的第 1 行中使用 `SEQUENTIAL` 标志，将客户端获取锁的尝试与所有其他尝试进行了排序。如果客户端的 znode 在第 3 行拥有最小的序列号，则客户端持有锁。否则，客户端等待持有锁的 znode 或将在此客户端 znode 之前接收锁的 znode 被删除。通过仅监视该客户端 znode **之前** 的那个 znode，我们避免了羊群效应，因为当一个锁被释放或一个锁请求被放弃时，我们只唤醒一个进程。一旦客户端监视的 znode 消失，客户端必须检查它现在是否持有锁。（之前的锁请求可能已被放弃，并且仍有一个具有更低序列号的 znode 在等待或持有锁。）

释放锁就像删除代表锁请求的 znode \( n \) 一样简单。通过在创建时使用 `EPHEMERAL` 标志，崩溃的进程将自动清理任何锁请求或释放它们可能拥有的任何锁。

总之，这种锁定方案具有以下优点：

1. 移除一个 znode 只会导致一个客户端被唤醒，因为每个 znode 恰好被另一个客户端监视，所以我们没有羊群效应；
2. 没有轮询或超时；
3. 由于我们要实现锁定的方式，我们可以通过浏览 ZooKeeper 数据来查看锁争用的数量、打破锁以及调试锁定问题。

#### 读/写锁 (Read/Write Locks)

为了实现读/写锁，我们稍微改变一下锁定过程，并拥有独立的读锁和写锁过程。解锁过程与全局锁的情况相同。

Write Lock (写锁)

1. `n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)`
2. `C = getChildren(l, false)`
3. `if n is lowest znode in C, exit`
4. `p = znode in C ordered just before n`
5. `if exists(p, true) wait for event`
6. `goto 2`

Read Lock (读锁)

1. `n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)`
2. `C = getChildren(l, false)`
3. `if no write znodes lower than n in C, exit` （如果 C 中没有比 n 序号更小的写 znode，退出/成功）
4. `p = write znode in C ordered just before n`
5. `if exists(p, true) wait for event`
6. `goto 3`

这个锁定过程与之前的锁略有不同。写锁仅在命名上有所不同。由于读锁可以共享，第 3 行和第 4 行略有变化，因为只有较早的写锁 znode 才会阻止客户端获得读锁。这看起来似乎我们在有多个客户端等待读锁时会产生“羊群效应”，因为当具有较低序列号的“write-” znode 被删除时，它们都会收到通知；事实上，这是我们期望的行为，所有那些读客户端都应该被释放，因为它们现在可能拥有了锁。

#### 双栅栏 (Double Barrier)

双栅栏允许客户端同步计算的开始和结束。当足够多的进程（由栅栏阈值定义）加入了栅栏时，进程开始其计算，并在完成后离开栅栏。我们在 ZooKeeper 中用一个 znode（称为 \( b \)）来表示栅栏。每个进程 \( p \) 通过在 \( b \) 下创建一个 znode 作为子节点来注册 \( b \)（进入），并在准备离开时注销（移除该子节点）。当 \( b \) 的子 znode 数量超过栅栏阈值时，进程可以进入栅栏。当所有进程都移除了它们的子节点后，进程可以离开栅栏。

我们使用监视来高效地等待进入和退出条件得到满足。为了进入，进程监视 \( b \) 的某个 `ready` 子节点的存在，该子节点将由导致子节点数量超过栅栏阈值的那个进程创建。为了离开，进程监视特定的子节点消失，并且只有在该 znode 被移除后才检查退出条件。

## 3. ZooKeeper 应用程序

我们现在描述一些使用 ZooKeeper 的应用程序，并简要解释它们是如何使用它的。我们将每个示例中的 **原语（primitives）** 用粗体显示。

**获取服务 (The Fetching Service)**
爬取是搜索引擎的重要组成部分，Yahoo! 爬取了数十亿的网页文档。获取服务（FS）是 Yahoo! 爬虫的一部分，目前正在生产环境中使用。本质上，它拥有指挥页面获取进程（page-fetching processes）的主进程（master processes）。主进程为获取者（fetchers）提供配置，而获取者回写信息以通报它们的状态和健康情况。FS 使用 ZooKeeper 的主要优势在于从主进程的故障中恢复、在故障发生时确保持用性，以及将客户端与服务器解以此允许客户端仅通过从 ZooKeeper 读取状态就能将请求导向健康的服务器。因此，FS 主要使用 ZooKeeper 来管理 **配置元数据（configuration metadata）**，尽管它也使用 ZooKeeper 来选举主进程（**领导者选举 leader election**）。

图 2 展示了 FS 使用的 ZooKeeper 服务器在三天内的读写流量。为了生成此图，我们统计了期间每秒的操作数量，每个点对应那一秒的操作数。我们观察到，与写流量相比，读流量要高得多。在速率高于每秒 1,000 次操作的时段，读:写比例在 10:1 到 100:1 之间变化。在这个工作负载中，读操作按普遍程度递增排序依次为 `getData()`、`getChildren()` 和 `exists()`。

**Katta**
Katta [17] 是一个使用 ZooKeeper 进行协调的分布式索引器，它是非 Yahoo! 应用程序的一个例子。Katta 使用分片（shards）来划分索引工作。主服务器（master server）将分片分配给从服务器（slaves）并跟踪进度。从服务器可能会发生故障，因此主服务器必须在从服务器加入和离开时重新分配负载。主服务器也可能发生故障，因此其他服务器必须准备好在故障发生时接管。Katta 使用 ZooKeeper 来跟踪从服务器和主服务器的状态（**组成员管理 group membership**），并处理主服务器故障转移（**领导者选举 leader election**）。Katta 还使用 ZooKeeper 来跟踪和传播分片到从服务器的分配（**配置管理 configuration management**）。

**Yahoo! 消息代理 (Yahoo! Message Broker)**
Yahoo! 消息代理（YMB）是一个分布式发布-订阅系统。该系统管理着成千上万个主题（topics），客户端可以向这些主题发布消息并从中接收消息。这些主题分布在一组服务器中以提供可扩展性。每个主题都使用主-备（primary-backup）方案进行复制，确保消息被复制到两台机器上以保证可靠的消息传递。组成 YMB 的服务器使用无共享（shared-nothing）分布式架构，这使得协调对于正确运行至关重要。YMB 使用 ZooKeeper 来管理主题的分布（**配置元数据 configuration metadata**），处理系统中机器的故障（**故障检测 failure detection** 和 **组成员管理 group membership**），并控制系统运行。

图 3 展示了 YMB 的部分 znode 数据布局。每个代理域（broker domain）都有一个名为 `nodes` 的 znode，对于组成 YMB 服务的每个活动服务器，`nodes` 下都有一个临时 znode。每个 YMB 服务器在 `nodes` 下创建一个包含负载和状态信息的临时 znode，通过 ZooKeeper 提供 **组成员管理（group membership）** 和状态信息。诸如 `shutdown`（关闭）和 `migration prohibited`（禁止迁移）之类的节点由组成服务的所有服务器监控，并允许对 YMB 进行集中控制。`topics` 目录对于 YMB 管理的每个主题都有一个子 znode。这些主题 znode 拥有子 znode，指示每个主题的主服务器和备份服务器以及该主题的订阅者。主服务器和备份服务器 znode 不仅允许服务器发现负责某个主题的服务器，而且它们还管理 **领导者选举（leader election）** 和服务器崩溃。

## 4 ZooKeeper 实现 (ZooKeeper Implementation)

ZooKeeper 通过在组成服务的每台服务器上复制 ZooKeeper 数据来提供高可用性。我们假设服务器通过崩溃来发生故障，并且此类故障服务器稍后可能会恢复。图 4 展示了 ZooKeeper 服务的高层组件。在接收到请求时，服务器会对其进行执行准备（请求处理器 request processor）。如果该请求需要服务器之间的协调（写请求），那么它们使用一种协议（原子广播 atomic broadcast 的一种实现）达成一致，最后服务器将更改提交到在集合体（ensemble）的所有服务器之间完全复制的 ZooKeeper 数据库中。对于读请求，服务器只是简单地读取本地数据库的状态并生成对请求的响应。

复制的数据库是一个包含整个数据树的内存数据库。树中的每个 znode 默认最多存储 1MB 的数据，但这个最大值是一个可以在特定情况下更改的配置参数。为了可恢复性，我们将更新高效地记录到磁盘，并且我们强制在写入应用到内存数据库之前先写入磁盘介质。事实上，像 Chubby [8] 一样，我们保留已提交操作的重放日志（在我们的案例中是预写日志 write-ahead log），并生成内存数据库的定期快照。

每个 ZooKeeper 服务器都为客户端提供服务。客户端确切地连接到一台服务器以提交其请求。正如我们之前指出的，读请求由每个服务器数据库的本地副本提供服务。改变服务状态的请求，即写请求，由一致性协议处理。

作为一致性协议的一部分，写请求被转发到单台服务器，称为 **领导者（leader）** [1]。其余的 ZooKeeper 服务器，称为 **跟随者（followers）**，接收来自领导者的包含状态变更的消息提议（message proposals）并对状态变更达成一致。

*[1] 作为一致性协议的一部分，关于领导者和跟随者的详细信息超出了本文的范围。*

### 4.1 请求处理器 (Request Processor)

由于消息层是原子的，我们保证本地副本永远不会发生分歧，尽管在任何时间点，某些服务器可能比其他服务器应用了更多的事务。与客户端发送的请求不同，事务是 **幂等的（idempotent）**。当领导者接收到一个写请求时，它会计算当写入应用时系统的状态将会是什么，并将其转换为捕获此新状态的事务。必须计算未来状态，因为可能存在尚未应用到数据库的未决事务。例如，如果客户端执行一个条件 `setData`，并且请求中的版本号与被更新 znode 的未来版本号匹配，服务将生成一个 `setDataTXN`，其中包含新数据、新版本号和更新的时间戳。如果发生错误，例如版本号不匹配或要更新的 znode 不存在，则会生成一个 `errorTXN`。

### 4.2 原子广播 (Atomic Broadcast)

所有更新 ZooKeeper 状态的请求都被转发给领导者。领导者执行请求并通过 Zab [24]（一种原子广播协议）广播对 ZooKeeper 状态的变更。接收客户端请求的服务器在投递相应的状态变更时向客户端响应。Zab 默认使用简单多数法定人数（simple majority quorums）来决定提议，因此 Zab 以及 ZooKeeper 只有在大多数服务器正确时才能工作（即，使用 \( 2f + 1 \) 台服务器，我们可以容忍 \( f \) 次故障）。

为了实现高吞吐量，ZooKeeper 试图保持请求处理流水线（pipeline）处于满载状态。它可能在处理流水线的不同部分有数千个请求。因为状态变更依赖于先前状态变更的应用，Zab 提供了比常规原子广播更强的顺序保证。更具体地说，Zab 保证领导者广播的变更按照发送顺序被投递，并且来自先前领导者的所有变更在当前领导者广播其自己的变更之前，都会被投递给已确立的领导者。

有一些实现细节简化了我们的实现并为我们提供了卓越的性能。我们使用 TCP 进行传输，因此消息顺序由网络维护，这使我们能够简化实现。我们使用 Zab 选出的领导者作为 ZooKeeper 的领导者，以便创建事务的同一个进程也负责提议它们。我们使用用于跟踪提议的日志作为内存数据库的预写日志（write-ahead log），这样我们就不必将消息向磁盘写入两次。

在正常操作期间，Zab 确实按顺序且恰好一次（exactly once）投递所有消息，但由于 Zab 不持久记录每个已投递消息的 ID，Zab 可能会在恢复期间重新投递消息。因为我们使用幂等事务，只要它们按顺序投递，多次投递是可以接受的。事实上，ZooKeeper 要求 Zab 至少重新投递自上次快照开始后投递的所有消息。

### 4.3 复制数据库 (Replicated Database)

每个副本在内存中都有一份 ZooKeeper 状态的拷贝。当 ZooKeeper 服务器从崩溃中恢复时，它需要恢复这个内部状态。在服务器运行一段时间后，重放所有已投递的消息来恢复状态将耗费过长的时间，因此 ZooKeeper 使用定期快照（periodic snapshots），并且只要求重新投递自快照开始以来的消息。我们将 ZooKeeper 的快照称为 **模糊快照（fuzzy snapshots）**，因为我们不会锁定 ZooKeeper 状态来获取快照；相反，我们对树进行深度优先扫描，原子地读取每个 znode 的数据和元数据并将它们写入磁盘。由于生成的模糊快照可能应用了在生成快照期间投递的状态变更的一个子集，因此结果可能不对应于 ZooKeeper 在任何时间点的状态。然而，由于状态变更是幂等的（idempotent），只要我们按顺序应用状态变更，我们就可以将它们应用两次。

例如，假设在一个 ZooKeeper 数据树中，两个节点 `/foo` 和 `/goo` 分别具有值 \( f1 \) 和 \( g1 \)，并且当模糊快照开始时它们都处于版本 1。随后到达了具有 `transactionType, path, value, new-version` 形式的如下状态变更流：

$$ \text{SetDataTXN}, /foo, f2, 2$$
$$\text{SetDataTXN}, /goo, g2, 2$$
$$\text{SetDataTXN}, /foo, f3, 3$$

在处理完这些状态变更后，`/foo` 和 `/goo` 分别拥有值 \( f3 \) 和 \( g2 \)，版本分别为 3 和 2。然而，模糊快照可能记录了 `/foo` 和 `/goo` 分别拥有值 \( f3 \) 和 \( g1 \)，版本分别为 3 和 1，这不是 ZooKeeper 数据树的一个有效状态。如果服务器崩溃并使用此快照恢复，且 Zab 重新投递了状态变更，结果状态将对应于崩溃前的服务状态。

### 4.4 客户端-服务器交互 (Client-Server Interactions)

当服务器处理写请求时，它也会发送并清除与该更新相对应的任何监视（watch）通知。服务器按顺序处理写入，并且不并发处理其他写入或读取。这确保了通知的严格演替（strict succession）。请注意，服务器在本地处理通知。只有客户端连接到的那台服务器才会为该客户端跟踪并触发通知。

读请求在每个服务器上本地处理。每个读请求被处理并标记一个 `zxid`，该 `zxid` 对应于服务器看到的最后一个事务。这个 `zxid` 定义了读请求相对于写请求的偏序关系。通过在本地处理读取，我们获得了卓越的读取性能，因为这只是本地服务器上的内存操作，没有磁盘活动或需要运行的一致性协议。这一设计选择是实现我们在读主导（read-dominant）工作负载下获得卓越性能目标的关键。

使用快速读取的一个缺点是不保证读操作的先后顺序（precedence order）。也就是说，读操作可能会返回一个陈旧的值，即使对同一 znode 的更近期的更新已经提交。并非我们要所有的应用程序都要求先后顺序，但对于那些确实需要的应用程序，我们实现了 `sync`（同步）。该原语异步执行，并在其本地副本的所有挂起写入之后由领导者进行排序。为了保证给定的读操作返回最新的更新值，客户端先调用 `sync`，随后调用读操作。客户端操作的 FIFO 顺序保证与 `sync` 的全局保证相结合，使得读操作的结果能够反映在发出 `sync` 之前发生的任何变更。

在我们的实现中，我们不需要原子广播 `sync`，因为我们使用基于领导者的算法，我们只需将 `sync` 操作放置在领导者与执行 `sync` 调用的服务器之间的请求队列末尾。为了使其工作，跟随者必须确信领导者仍然是领导者。如果有挂起的事务提交，则服务器不会怀疑领导者。如果挂起队列为空，领导者需要发出一个空事务（null transaction）来提交，并将 `sync` 排序在该事务之后。这有一个很好的特性，即当领导者处于高负载时，不会产生额外的广播流量。在我们的实现中，超时设置使得领导者在跟随者抛弃它们之前就意识到自己不再是领导者，因此我们不发出空事务。

ZooKeeper 服务器按 FIFO 顺序处理来自客户端的请求。响应中包含了该响应所相关的 `zxid`。即使是在没有活动的间隔期间的心跳消息，也包含客户端所连接的服务器看到的最后一个 `zxid`。如果客户端连接到一个新服务器，该新服务器通过将客户端的最后一个 `zxid` 与其自己的最后一个 `zxid` 进行检查，确保其 ZooKeeper 数据视图至少与客户端的视图一样新。如果客户端的视图比服务器的更新，服务器在赶上之前不会与客户端重新建立会话。客户端保证能够找到另一台拥有近期系统视图的服务器，因为客户端只看到已复制到大多数 ZooKeeper 服务器的变更。这种行为对于保证持久性很重要。

为了检测客户端会话故障，ZooKeeper 使用超时。如果在会话超时内没有其他服务器收到来自客户端会话的任何信息，领导者就确定发生了故障。如果客户端发送请求足够频繁，则无需发送任何其他消息。否则，客户端在低活动期间发送心跳消息。如果客户端无法与服务器通信以发送请求或心跳，它将连接到不同的 ZooKeeper 服务器以重新建立会话。为了防止会话超时，ZooKeeper 客户端库在会话空闲 \( s/3 \) 毫秒后发送心跳，并在 \( 2s/3 \) 毫秒未收到服务器消息时切换到新服务器，其中 \( s \) 是以毫秒为单位的会话超时时间。

## 5. 评估

我们在一个由 50 台服务器组成的集群上进行了所有的评估。每台服务器配备一个 Xeon 双核 2.1GHz 处理器、4GB 内存、千兆以太网和两个 SATA 硬盘。我们将接下来的讨论分为两部分：吞吐量和请求延迟。

### 5.1 吞吐量

为了评估我们的系统，我们在系统饱和时以及注入各种故障时，对吞吐量及其变化进行了基准测试。我们改变了组成 ZooKeeper 服务的服务器数量，但始终保持客户端数量不变。为了模拟大量客户端，我们使用 35 台机器来模拟 250 个并发客户端。

我们拥有 ZooKeeper 服务器的 Java 实现，以及 Java 和 C 客户端 [2]。对于这些实验，我们使用的 Java 服务器配置为在一个专用磁盘上记录日志，在另一个磁盘上拍摄快照。我们的基准测试客户端使用异步 Java 客户端 API，每个客户端至少有 100 个未决请求（outstanding requests）。每个请求包含 1K 数据的读取或写入。我们不展示其他操作的基准测试，因为所有修改状态的操作性能大致相同，而不修改状态的操作（不包括 `sync`）性能也大致相同。（`sync` 的性能近似于轻量级写入，因为请求必须到达领导者，但不会被广播。）

客户端每 300 毫秒发送一次已完成操作数量的计数，我们每 6 秒采样一次。为了防止内存溢出，服务器限制（throttle）系统中的并发请求数量。ZooKeeper 使用请求节流来防止服务器不堪重负。对于这些实验，我们将 ZooKeeper 服务器配置为处理中的总请求数最多为 2,000 个。

在图 5 中，我们展示了随着读写请求比例变化时的吞吐量，每条曲线对应提供 ZooKeeper 服务的不同服务器数量。表 1 显示了读取负载极端情况下的数值。读取吞吐量高于写入吞吐量，因为读取不使用原子广播。该图还显示，服务器数量也会对广播协议的性能产生负面影响。从这些图中，我们观察到系统中的服务器数量不仅影响服务可以处理的故障数量，还影响服务可以处理的工作负载。请注意，三台服务器的曲线在 60% 左右与其他曲线相交。这种情况并非三台服务器配置所独有，而是由于本地读取启用的并行性，所有配置都会发生这种情况。然而，我们在图中无法观察到其他配置的这种情况，因为为了可读性，我们限制了 y 轴的最大吞吐量。

写请求比读请求耗时更长有两个原因。首先，写请求必须通过原子广播，这需要一些额外的处理并增加了请求的延迟。写请求处理时间较长的另一个原因是，服务器在向领导者发送确认之前，必须确保事务已记录到非易失性存储中。原则上，这一要求是过分的，但对于我们的生产系统，我们以性能换取可靠性，因为 ZooKeeper 构成了应用程序的基础事实（ground truth）。我们使用更多的服务器来容忍更多的故障。我们通过将 ZooKeeper 数据分区到多个 ZooKeeper 集合体（ensembles）中来增加写入吞吐量。Gray 等人 [12] 之前已经观察到了这种在复制和分区之间的性能权衡。

ZooKeeper 能够通过将负载分布在组成服务的服务器上来实现如此高的吞吐量。由于我们放宽了一致性保证，我们可以通过这种方式分配负载。相比之下，Chubby 客户端将所有请求直接发送给领导者。图 6 显示了如果我们不利用这种放宽，强制客户端只连接到领导者会发生什么。正如预期的那样，读主导（read-dominant）工作负载的吞吐量要低得多，但即使对于写主导（write-dominant）工作负载，吞吐量也较低。服务客户端所造成的额外 CPU 和网络负载影响了领导者协调提议广播的能力，进而对整体写入性能产生不利影响。

原子广播协议完成了系统的大部分工作，因此比任何其他组件都更能限制 ZooKeeper 的性能。图 7 显示了原子广播组件的吞吐量。为了对其性能进行基准测试，我们通过直接在领导者处生成事务来模拟客户端，因此没有客户端连接或客户端请求和回复。在最大吞吐量下，原子广播组件受限于 CPU（CPU bound）。理论上，图 7 的性能应与 100% 写入的 ZooKeeper 性能相匹配。然而，ZooKeeper 客户端通信、ACL 检查以及请求到事务的转换都需要 CPU。对 CPU 的争用使得 ZooKeeper 的吞吐量大大低于隔离状态下的原子广播组件。由于 ZooKeeper 是一个关键的生产组件，到目前为止，我们对 ZooKeeper 的开发重点一直是正确性和健壮性。通过消除额外的拷贝、同一对象的多次序列化、使用更高效的内部数据结构等，还有很多显著提高性能的机会。

为了展示系统在注入故障时随时间变化的行为，我们运行了一个由 5 台机器组成的 ZooKeeper 服务。我们运行了与之前相同的饱和基准测试，但这次我们将写入百分比保持在恒定的 30%，这是我们预期工作负载的一个保守比例。我们会定期杀死一些服务器进程。图 8 显示了系统吞吐量随时间的变化。图中标记的事件如下：

1. 一个跟随者（follower）故障并恢复；
2. 另一个跟随者故障并恢复；
3. 领导者（leader）故障；
4. 在前两个标记处两个跟随者（a, b）故障，并在第三个标记处（c）恢复；
5. 领导者故障；
6. 领导者恢复。

从这个图中可以得出几个重要的观察结果。首先，如果跟随者故障并快速恢复，ZooKeeper 能够在故障期间维持高吞吐量。单个跟随者的故障不会阻止服务器形成法定人数（quorum），只会减少相当于该服务器在故障前处理的读请求份额的吞吐量。其次，我们的领导者选举算法能够足够快地恢复，以防止吞吐量大幅下降。在我们的观察中，ZooKeeper 选举新领导者所需的时间不到 200 毫秒。因此，尽管服务器会停止服务请求一小部分秒，但由于我们的采样周期是秒级的，我们没有观察到吞吐量为零的情况。第三，即使跟随者需要更多时间来恢复，一旦它们开始处理请求，ZooKeeper 也能再次提高吞吐量。我们在事件 1、2 和 4 之后没有恢复到完全吞吐量水平的一个原因是，客户端只有在与跟随者的连接断开时才会切换跟随者。因此，在事件 4 之后，直到事件 3 和 5 中领导者发生故障，客户端才会重新分布。在实践中，随着客户端的来来去去，这种不平衡会随着时间的推移自行解决。

### 5.2 请求延迟 (Latency of requests)

为了评估请求的延迟，我们创建了一个模仿 Chubby 基准测试 [6] 的基准测试。我们创建一个工作进程，它只是简单地发送一个 `create`，等待它完成，发送一个新节点的异步 `delete`，然后开始下一个 `create`。我们相应地改变工作进程的数量，并且对于每次运行，我们让每个工作进程创建 50,000 个节点。我们通过将完成的 `create` 请求数除以所有工作进程完成所需的总时间来计算吞吐量。

表 2 显示了我们的基准测试结果。`create` 请求包含 1K 数据，而不是 Chubby 基准测试中的 5 字节，以更好地符合我们的预期用途。即使有这些更大的请求，ZooKeeper 的吞吐量也比 Chubby 公布的吞吐量高出 3 倍以上。单个 ZooKeeper 工作进程基准测试的吞吐量表明，对于三台服务器，平均请求延迟为 1.2 毫秒，对于 9 台服务器为 1.4 毫秒。

### 5.3 栅栏性能 (Performance of barriers)

在这个实验中，我们按顺序执行多个栅栏（barriers），以评估用 ZooKeeper 实现的原语的性能。对于给定数量的栅栏 \( b \)，每个客户端首先进入所有 \( b \) 个栅栏，然后连续离开所有 \( b \) 个栅栏。由于我们使用 2.4 节的双栅栏算法，客户端首先等待所有其他客户端执行 `enter()` 过程，然后再进行下一次调用（对于 `leave()` 也是如此）。

我们在表 3 中报告了我们的实验结果。在这个实验中，我们要么有 50、100 或 200 个客户端连续进入 \( b \) 个栅栏，其中 \( b \in \{200, 400, 800, 1600\} \)。尽管一个应用程序可能有数千个 ZooKeeper 客户端，但通常只有更小的一个子集参与每个协调操作，因为客户端通常根据应用程序的具体情况进行分组。

从这个实验中得出的两个有趣的观察结果是：处理所有栅栏的时间随栅栏数量大致呈线性增加，这表明对数据树同一部分的并发访问没有产生任何意外的延迟；并且延迟随客户端数量成比例增加。这是由于没有使 ZooKeeper 服务饱和的结果。事实上，我们观察到即使客户端步调一致地进行，在所有情况下栅栏操作（进入和离开）的吞吐量都在每秒 1,950 到 3,100 次操作之间。在 ZooKeeper 操作中，这对应于每秒 10,700 到 17,000 次操作的吞吐量值。由于在我们的实现中读写比为 4:1（80% 的读操作），我们的基准测试代码使用的吞吐量远低于 ZooKeeper 可以实现的原始吞吐量（根据图 5 超过 40,000）。这是由于客户端在等待其他客户端。

## 6 相关工作 (Related work)

ZooKeeper 的目标是提供一种服务，以减轻分布式应用程序中协调进程的问题。为了实现这一目标，其设计借鉴了以前的协调服务、容错系统、分布式算法和文件系统的思想。

我们并非第一个提出用于分布式应用程序协调系统的人。一些早期的系统提出了用于事务性应用程序 [13] 以及用于在计算机集群中共享信息 [19] 的分布式锁服务。最近，Chubby 提出了一个管理分布式应用程序咨询锁（advisory locks）的系统 [6]。Chubby 与 ZooKeeper 有几个共同的目标。它也有一个类似文件系统的接口，并且使用一致性协议来保证副本的一致性。然而，ZooKeeper 不是一个锁服务。客户端可以使用它来实现锁，但其 API 中没有锁操作。与 Chubby 不同，ZooKeeper 允许客户端连接到任何 ZooKeeper 服务器，而不仅仅是领导者。ZooKeeper 客户端可以使用其本地副本提供数据服务并管理监视（watches），因为其一致性模型比 Chubby 宽松得多。这使得 ZooKeeper 能够提供比 Chubby 更高的性能，允许应用程序更广泛地使用 ZooKeeper。

文献中已经提出了旨在减轻构建容错分布式应用程序问题的容错系统。一个早期的系统是 ISIS [5]。ISIS 系统将抽象类型规范转换为容错分布式对象，从而使用户对容错机制透明。Horus [30] 和 Ensemble [31] 是从 ISIS 演变而来的系统。ZooKeeper 采纳了 ISIS 的 **虚拟同步（virtual synchrony）** 概念。最后，Totem 在利用局域网硬件广播的架构中保证消息投递的全序（total order）[22]。ZooKeeper 适用于各种各样的网络拓扑，这促使我们依赖服务器进程之间的 TCP 连接，而不假设任何特殊的拓扑或硬件特性。我们也不暴露 ZooKeeper 内部使用的任何集合体通信（ensemble communication）。

构建容错服务的一项重要技术是 **状态机复制（state-machine replication）** [26]，而 Paxos [20] 是一种能够为异步系统高效实现复制状态机的算法。我们使用了一种算法，它具有 Paxos 的某些特征，但结合了共识所需的事务日志记录和数据树恢复所需的预写日志（write-ahead logging），以实现高效的实现。已经有人提出了用于拜占庭容错（Byzantine-tolerant）复制状态机的实际实现的协议 [7, 10, 18, 1, 28]。ZooKeeper 不假设服务器可能是拜占庭式的（即恶意的），但我们确实采用了校验和及完整性检查等机制来捕获非恶意的拜占庭故障。Clement 等人讨论了一种在不修改当前服务器代码库的情况下使 ZooKeeper 完全具备拜占庭容错能力的方法 [9]。迄今为止，我们在生产环境中尚未观察到通过使用完全拜占庭容错协议可以预防的故障 [29]。

Boxwood [21] 是一个使用分布式锁服务器的系统。Boxwood 为应用程序提供了更高级别的抽象，它依赖于基于 Paxos 的分布式锁服务。像 Boxwood 一样，ZooKeeper 是用于构建分布式系统的组件。然而，ZooKeeper 具有高性能要求，并且更广泛地用于客户端应用程序中。ZooKeeper 暴露了应用程序用来实现更高级别原语的低级别原语。

ZooKeeper 类似于一个小型的文件系统，但它只提供了文件系统操作的一小部分子集，并增加了大多数文件系统中不存在的功能，如顺序保证和条件写入。然而，ZooKeeper 的监视（watches）在精神上类似于 AFS 的缓存回调 [16]。

Sinfonia [2] 引入了 **迷你事务（mini-transactions）**，这是一种构建可扩展分布式系统的新范式。Sinfonia 被设计用于存储应用程序数据，而 ZooKeeper 存储应用程序元数据。ZooKeeper将其状态完全复制并保存在内存中，以实现高性能和一致的延迟。我们对类似文件系统操作和排序的使用，实现了类似于迷你事务的功能。Znode 是一个方便的抽象，我们在其上添加了监视（watches），这是 Sinfonia 所缺少的功能。Dynamo [11] 允许客户端在分布式键值存储中获取（get）和放置（put）相对较小（小于 1M）的数据量。与 ZooKeeper 不同，Dynamo 中的键空间不是层级的。Dynamo 也不为写入提供强持久性和一致性保证，而是解决读取时的冲突。

DepSpace [4] 使用元组空间（tuple space）来提供拜占庭容错服务。像 ZooKeeper 一样，DepSpace 使用简单的服务器接口在客户端实现强同步原语。虽然 DepSpace 的性能远低于 ZooKeeper，但它提供了更强的容错能力和机密性保证。

## 7. 结论 (Conclusions)

ZooKeeper 对分布式系统中的进程协调问题采取了一种 **无等待（wait-free）** 的方法，即向客户端暴露无等待对象。我们发现 ZooKeeper 在 Yahoo! 内部和外部的多个应用程序中都非常有用。通过使用带有监视（watches）的快速读取（两者都由本地副本提供服务），ZooKeeper 在 **读主导（read-dominant）** 的工作负载下实现了每秒数十万次操作的吞吐量值。

尽管我们对读取和监视的一致性保证看起来很弱，但我们通过用例表明，这种组合允许我们在客户端实现高效且复杂的协调协议，即使读取没有先后顺序且数据对象的实现是无等待的。无等待属性已被证明对高性能至关重要。

尽管我们只描述了少数几个应用程序，但还有许多其他应用程序在使用 ZooKeeper。我们相信，这种成功归功于其简单的接口以及可以通过该接口实现的强大抽象。此外，由于 ZooKeeper 的高吞吐量，应用程序可以广泛地使用它，而不仅仅是用于 **粗粒度（coarse-grained）** 的锁定。

## 致谢 (Acknowledgements)

我们要感谢 Andrew Kornev 和 Runping Qi 对 ZooKeeper 的贡献；感谢 Zeke Huang 和 Mark Marchukov 提供的宝贵反馈；感谢 Brian Cooper 和 Laurence Ramontianu 对 ZooKeeper 的早期贡献；Brian Bershad 和 Geoff Voelker 对演示文稿提出了重要意见。
