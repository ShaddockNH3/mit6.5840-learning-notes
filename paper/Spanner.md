# Spanner: Google’s Globally-Distributed Database

> [Spanner：Google 的全球分布式数据库](http://nil.csail.mit.edu/6.5840/2025/papers/spanner.pdf)

James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang, Dale Woodford

Google, Inc.

## 摘要

Spanner 是 Google 研发的可扩展、多版本、全球分布且同步复制的数据库。它是第一个在全球范围内分发数据并支持外部一致性分布式事务的系统。本文描述了 Spanner 的架构、功能集、各种设计决策背后的基本原理，以及一个暴露时钟不确定性的新型时间 API。该 API 及其实现对于支持外部一致性以及整个 Spanner 的各种强大功能至关重要，这些功能包括：对过去数据的非阻塞读取、无锁只读事务以及原子模式变更。

## 1. 介绍

Spanner 是 Google 设计、构建和部署的可扩展、全球分布式数据库。在最高抽象层面上，它是一个将数据分片到遍布全球的数据中心的许多 Paxos [21] 状态机集上的数据库。复制用于实现全球可用性和地理局部性；客户端会在副本之间自动进行故障转移。随着数据量或服务器数量的变化，Spanner 会自动在机器之间重新分片数据，并自动在机器之间（甚至跨数据中心）迁移数据以平衡负载和应对故障。Spanner 旨在扩展到跨越数百个数据中心的数百万台机器和数万亿行数据库记录。

应用程序可以利用 Spanner 实现高可用性，即使面对大范围的自然灾害，也能通过在其所在的洲内甚至跨洲复制数据来保证服务。我们的初始客户是 F1 [35]，即 Google 广告后端的重写版。F1 使用分布在美国各地的五个副本。大多数其他应用程序可能会在一个地理区域内的 3 到 5 个数据中心复制其数据，但具有相对独立的故障模式。也就是说，只要能经受住 1 或 2 个数据中心的故障，大多数应用程序会选择更低的延迟而不是更高的可用性。

Spanner 的主要关注点是管理跨数据中心的复制数据，但我们也花费了大量时间在分布式系统基础设施之上设计和实现重要的数据库功能。尽管许多项目愉快地使用 Bigtable [9]，但也一直收到用户抱怨 Bigtable 难用于某些类型的应用：那些具有复杂、演变模式的应用，或者那些在大范围复制下需要强一致性的应用。（其他作者也提出了类似的说法 [37]。）Google 的许多应用程序选择使用 Megastore [5]，因为它具有半关系数据模型和对同步复制的支持，尽管其写入吞吐量相对较差。因此，Spanner 已从类似 Bigtable 的版本化键值存储演变为时态多版本数据库。数据存储在模式化的半关系表中；数据是版本化的，每个版本都会自动标记其提交时间戳；旧版本的数据受可配置的垃圾收集策略约束；应用程序可以读取旧时间戳的数据。Spanner 支持通用事务，并提供基于 SQL 的查询语言。

作为全球分布式数据库，Spanner 提供了几个有趣的功能。首先，数据的复制配置可以由应用程序进行细粒度的动态控制。应用程序可以指定约束来控制哪些数据中心包含哪些数据，数据距离其用户多远（以控制读取延迟），副本之间距离多远（以控制写入延迟），以及维护多少个副本（以控制持久性、可用性和读取性能）。系统还可以动态且透明地在数据中心之间移动数据，以平衡各数据中心的资源使用。其次，Spanner 拥有两个在分布式数据库中难以实现的功能：它提供外部一致性 [16] 的读写，以及在某个时间戳上的全库全局一致性读取。这些功能使 Spanner 能够支持一致性备份、一致性 MapReduce 执行 [12] 和原子模式更新，所有这些都在全球范围内进行，甚至在有正在进行的事务时也能实现。

这些功能得以实现，是因为 Spanner 为事务分配了具有全球意义的提交时间戳，即使事务可能是分布式的。时间戳反映了序列化顺序。此外，序列化顺序满足外部一致性（或等效的线性一致性 [20]）：如果事务 T1 在另一个事务 T2 开始之前提交，则 T1 的提交时间戳小于 T2 的。Spanner 是第一个在全球范围内提供此类保证的系统。实现这些属性的关键推动因素是新的 TrueTime API 及其实现。该 API 直接暴露时钟不确定性，Spanner 时间戳的保证取决于实现提供的界限。如果不确定性很大，Spanner 会放慢速度以等待该不确定性过去。Google 的集群管理软件提供了 TrueTime API 的实现。该实现通过使用多个现代时钟参考（GPS 和原子钟）将不确定性保持在很小（通常小于 10ms）。

第 2 节描述了 Spanner 实现的结构、功能集以及设计中的工程决策。第 3 节描述了我们需要的新 TrueTime API 并概述了其实现。第 4 节描述了 Spanner 如何使用 TrueTime 来实现外部一致的分布式事务、无锁只读事务和原子模式更新。第 5 节提供了关于 Spanner 性能和 TrueTime 行为的一些基准测试，并讨论了 F1 的经验。第 6、7 和 8 节描述了相关工作和未来工作，并总结了我们的结论。

## 2. 实现

本节描述了 Spanner 实现的结构及其背后的基本原理。然后描述了目录抽象（directory abstraction），它用于管理复制和局部性，并且是数据移动的单元。最后，我们将描述数据模型，解释为什么 Spanner 看起来像关系数据库而不是键值存储，以及应用程序如何控制数据局部性。

Spanner 的部署称为“宇宙”（universe）。鉴于 Spanner 在全球范围内管理数据，运行中的宇宙只有寥寥几个。目前我们运行着一个测试/游乐场宇宙，一个开发/生产宇宙，和一个仅用于生产的宇宙。

Spanner 被组织成一组“区域”（zone），每个区域大致类似于 Bigtable 服务器 [9] 的一次部署。区域是行政部署的单元。区域集合也是数据可以被复制到的位置集合。随着新数据中心投入使用或旧数据中心关闭，可以分别向运行中的系统添加或移除区域。区域也是物理隔离的单元：例如，如果不同应用程序的数据必须在同一数据中心的不同服务器集上进行分区，那么一个数据中心可能有一个或多个区域。

图 1 展示了 Spanner 宇宙中的服务器。一个区域有一个区域管理器（zonemaster）和一百到几千个 Spanserver。前者将数据分配给 Spanserver；后者向客户端提供数据。客户端使用每个区域的位置代理（location proxies）来定位分配给服务其数据的 Spanserver。宇宙管理器（universe master）和放置驱动器（placement driver）目前是单例的。宇宙管理器主要是一个控制台，显示所有区域的状态信息以便于交互式调试。放置驱动器处理区域间分钟级时间尺度的数据自动移动。放置驱动器定期与 Spanserver 通信，以查找需要移动的数据，以满足更新的复制约束或平衡负载。由于篇幅原因，我们将仅详细描述 Spanserver。

### 2.1 Spanserver 软件栈

本节重点介绍 Spanserver 的实现，以说明复制和分布式事务是如何分层构建在我们基于 Bigtable 的实现之上的。软件栈如图 2 所示。在底部，每个 Spanserver 负责 100 到 1000 个称为“tablet”的数据结构实例。Tablet 类似于 Bigtable 的 tablet 抽象，因为它实现了一组如下的映射：

$ (\text{key:string}, \text{timestamp:int64}) \to \text{string} $

与 Bigtable 不同，Spanner 为数据分配时间戳，这是 Spanner 更像多版本数据库而不是键值存储的一个重要方式。Tablet 的状态存储在一组类似 B 树的文件和预写日志（write-ahead log）中，所有这些都位于名为 Colossus（Google 文件系统 [15] 的继任者）的分布式文件系统上。

为了支持复制，每个 Spanserver 在每个 tablet 之上实现了一个 Paxos 状态机。（早期的 Spanner 版本支持每个 tablet 多个 Paxos 状态机，这允许更灵活的复制配置。但该设计的复杂性导致我们放弃了它。）每个状态机将其元数据和日志存储在其对应的 tablet 中。我们的 Paxos 实现支持基于时间的领导者租约（leader leases）的长寿命领导者，租约长度默认为 10 秒。当前的 Spanner 实现将每次 Paxos 写入记录两次：一次在 tablet 的日志中，一次在 Paxos 日志中。这种选择是出于权宜之计，我们可能会最终修正这一点。我们的 Paxos 实现是流水线式的，以便在存在广域网（WAN）延迟的情况下提高 Spanner 的吞吐量；但 Paxos 会按顺序应用写入（我们在第 4 节中将依赖这一事实）。

Paxos 状态机用于实现一致复制的映射包。每个副本的键值映射状态存储在其对应的 tablet 中。写入必须在领导者处启动 Paxos 协议；读取则可以直接访问任何足够新的副本上的底层 tablet 状态。这组副本统称为一个 Paxos 组。

在作为领导者的每个副本上，每个 Spanserver 实现了一个锁表（lock table）来实现并发控制。锁表包含两阶段锁定（two-phase locking）的状态：它将键的范围映射到锁状态。（注意，拥有长寿命的 Paxos 领导者对于有效管理锁表至关重要。）在 Bigtable 和 Spanner 中，我们都针对长寿命事务（例如，可能需要几分钟的报表生成）进行了设计，这些事务在存在冲突的情况下，在乐观并发控制下表现不佳。需要同步的操作（如事务性读取）在锁表中获取锁；其他操作则绕过锁表。

在作为领导者的每个副本上，每个 Spanserver 还实现了一个事务管理器（transaction manager）以支持分布式事务。事务管理器用于实现参与者领导者（participant leader）；该组中的其他副本将被称为参与者从属（participant slaves）。如果一个事务仅涉及一个 Paxos 组（大多数事务都是这种情况），它可以绕过事务管理器，因为锁表和 Paxos 共同提供了事务性。如果一个事务涉及多个 Paxos 组，这些组的领导者会协调执行两阶段提交（two-phase commit）。其中一个参与者组被选为协调者：该组的参与者领导者将被称为协调者领导者（coordinator leader），该组的从属将被称为协调者从属（coordinator slaves）。每个事务管理器的状态存储在底层的 Paxos 组中（因此是被复制的）。

### 2.2 目录与放置

在键值映射包之上，Spanner 实现支持一种称为“目录”（directory）的桶抽象，它是一组共享相同前缀的连续键。（选择“目录”这个术语是一个历史遗留的意外；更好的术语可能是“桶”（bucket）。）我们将在 2.3 节解释该前缀的来源。支持目录允许应用程序通过精心选择键来控制数据的局部性。

目录是数据放置的单元。一个目录中的所有数据具有相同的复制配置。当数据在 Paxos 组之间移动时，是按目录逐个移动的，如图 3 所示。Spanner 移动目录可能是为了减轻某个 Paxos 组的负载；为了将经常被一起访问的目录放入同一个组；或者为了将目录移动到离其访问者更近的组中。在客户端操作进行的同时，目录可以被移动。预计移动一个 50MB 的目录只需几秒钟。

由于一个 Paxos 组可能包含多个目录，这意味着 Spanner 的 tablet 与 Bigtable 的 tablet 不同：前者不一定是行空间中单个按字典序连续的分区。相反，Spanner 的 tablet 是一个容器，可以封装行空间的多个分区。我们做出这一决定是为了能够将经常一起访问的多个目录并置在一起。

Movedir 是用于在 Paxos 组之间移动目录的后台任务 [14]。Movedir 也用于向 Paxos 组添加或移除副本 [25]，因为 Spanner 尚未支持 Paxos 内部的配置更改。为了避免在大量数据移动时阻塞正在进行的读写操作，Movedir 没有实现为单个事务。相反，Movedir 注册它开始移动数据的事实，并在后台移动数据。当它移动了除少量数据以外的所有数据后，它会使用一个事务来原子地移动那少量剩余数据并更新两个 Paxos 组的元数据。

目录也是应用程序可以指定其地理复制属性（或简称“放置”）的最小单元。我们的放置规范语言的设计分离了管理复制配置的责任。管理员控制两个维度：副本的数量和类型，以及这些副本的地理位置。他们在这些维度中创建了一个命名选项菜单（例如，“北美，5 路复制，1 个见证者”）。应用程序通过用这些选项的组合标记每个数据库和/或单个目录来控制数据的复制方式。例如，应用程序可能会将每个最终用户的数据存储在自己的目录中，这将使通过配置让用户 A 的数据在欧洲有三个副本，而用户 B 的数据在北美有五个副本成为可能。

为了说明清晰，我们进行了过度简化。实际上，如果目录变得太大，Spanner 会将其分片为多个片段（fragments）。片段可能由不同的 Paxos 组（因此是不同的服务器）提供服务。Movedir 实际上是在组之间移动片段，而不是整个目录。

### 2.3 数据模型

Spanner 向应用程序暴露了以下一组数据特性：基于模式化的半关系表的数据模型、查询语言和通用事务。

支持这些特性的举措受到多种因素的驱动。Megastore [5] 的流行支持了对模式化半关系表和同步复制的需求。Google 内部至少有 300 个应用程序使用 Megastore（尽管其性能相对较低），因为它的数据模型比 Bigtable 更易于管理，并且它支持跨数据中心的同步复制。（Bigtable 仅支持跨数据中心的最终一致性复制。）使用 Megastore 的知名 Google 应用程序包括 Gmail、Picasa、Calendar、Android Market 和 AppEngine。鉴于 Dremel [28] 作为交互式数据分析工具的流行，Spanner 对支持类似 SQL 的查询语言的需求也很明确。最后，Bigtable 缺乏跨行事务导致了频繁的抱怨；Percolator [32] 的构建部分就是为了解决这一缺陷。一些作者声称，由于通用的两阶段提交（two-phase commit）带来的性能或可用性问题，支持它的代价太高 [9, 10, 19]。我们认为，最好是让应用程序员在瓶颈出现时处理因滥用事务而导致的性能问题，而不是总是为了绕过事务的缺失而编写复杂的代码。在 Paxos 之上运行两阶段提交减轻了可用性问题。

应用数据模型分层构建在实现层支持的目录分桶键值映射之上。应用程序在宇宙（universe）中创建一个或多个数据库。每个数据库可以包含无限数量的模式化表。这些表看起来像关系数据库表，具有行、列和版本化的值。我们将不详细介绍 Spanner 的查询语言。它看起来像 SQL，但带有一些扩展以支持协议缓冲区（protocol-buffer）值的字段。

Spanner 的数据模型并非纯粹的关系型，因为行必须有名称。更确切地说，每个表都必须有一组有序的一个或多个主键列。这一要求是 Spanner 仍然像键值存储的地方：主键构成了行的名称，每个表定义了从主键列到非主键列的映射。只有当为行的键定义了某个值（即使是 NULL）时，该行才存在。强制实施这种结构很有用，因为它允许应用程序通过选择键来控制数据局部性。

图 4 包含了一个示例 Spanner 模式，用于以每个用户、每个相册为基础存储照片元数据。该模式语言类似于 Megastore 的，但有一个额外的要求，即每个 Spanner 数据库必须由客户端划分为一个或多个表的层次结构。客户端应用程序通过 `INTERLEAVE IN` 声明在数据库模式中声明层次结构。层次结构顶部的表是目录表。目录表中具有键 K 的每一行，连同后代表中所有以 K 开头（按字典序）的行，构成一个目录。`ON DELETE CASCADE` 表示删除目录表中的一行会删除任何关联的子行。该图还说明了示例数据库的交错布局：例如，`Albums(2,1)` 代表 `Albums` 表中 `user_id` 为 2，`album_id` 为 1 的行。这种表的交错以形成目录的做法非常重要，因为它允许客户端描述存在于多个表之间的局部性关系，这对于在分片、分布式数据库中获得良好性能是必要的。如果没有它，Spanner 将不知道最重要的局部性关系。

## 3. TrueTime

本节描述了 TrueTime API 并概述了其实现。我们将大部分细节留给另一篇论文：我们的目标是展示拥有这样一个 API 的力量。表 1 列出了 API 的方法。TrueTime 将时间显式表示为 `TTinterval`，这是一个具有有界时间不确定性的区间（不同于不给客户端任何不确定性概念的标准时间接口）。`TTinterval` 的端点类型为 `TTstamp`。`TT.now()` 方法返回一个 `TTinterval`，该区间保证包含调用 `TT.now()` 时的绝对时间。时间纪元（time epoch）类似于带有闰秒平滑（leap-second smearing）的 UNIX 时间。定义瞬时误差界限为 $ \epsilon $，它是区间宽度的一半，平均误差界限为 $ \bar{\epsilon} $。`TT.after()` 和 `TT.before()` 方法是 `TT.now()` 的便捷包装器。

用函数 $ t_{abs}(e) $ 表示事件 $ e $ 的绝对时间。用更形式化的术语来说，TrueTime 保证对于一个调用 $ tt = \text{TT.now()} $，有 $ tt.earliest \le t_{abs}(e_{now}) \le tt.latest $，其中 $ e_{now} $ 是调用事件。

TrueTime 使用的底层时间参考是 GPS 和原子钟。TrueTime 使用这两种形式的时间参考是因为它们具有不同的故障模式。GPS 参考源的弱点包括天线和接收器故障、本地无线电干扰、相关故障（例如设计缺陷，如错误的闰秒处理和欺骗）以及 GPS 系统中断。原子钟的故障方式与 GPS 无关，彼此之间也不相关，并且在很长一段时间内可能会由于频率误差而发生显著漂移。

TrueTime 由每个数据中心的一组时间主控机（time master）机器和每台机器上的时间从属（timeslave）守护进程实现。大多数主控机都配备了带有专用天线的 GPS 接收器；这些主控机在物理上是分离的，以减少天线故障、无线电干扰和欺骗的影响。其余的主控机（我们要称之为“末日主控机” / Armageddon masters）配备了原子钟。原子钟并没有那么昂贵：末日主控机的成本与 GPS 主控机的成本处于同一数量级。所有主控机的时间参考都会定期相互比较。每个主控机还会根据自己的本地时钟交叉检查其参考推进时间的速度，如果存在实质性偏差，则将其自身驱逐。在同步之间，末日主控机广播一个缓慢增加的时间不确定性，该不确定性源自保守应用的最坏情况时钟漂移。GPS 主控机广播的不确定性通常接近于零。

每个守护进程轮询各种主控机 [29]，以减少因任何单一主控机错误而导致的脆弱性。有些是从附近数据中心选择的 GPS 主控机；其余的是来自较远数据中心的 GPS 主控机，以及一些末日主控机。守护进程应用 Marzullo 算法 [27] 的变体来检测和拒绝说谎者，并将本地机器时钟同步到非说谎者。为了防止本地时钟损坏，如果机器表现出的频率偏移大于根据组件规格和运行环境推导出的最坏情况界限，则该机器会被驱逐。

在同步之间，守护进程会广播一个缓慢增加的时间不确定性 $ \epsilon $。$ \epsilon $ 源自保守应用的最坏情况本地时钟漂移。$ \epsilon $ 还取决于时间主控机的不确定性和到时间主控机的通信延迟。在我们的生产环境中，$ \epsilon $ 通常是时间的锯齿函数，在每个轮询间隔内从约 1 变化到 7 毫秒。因此 $ \bar{\epsilon} $ 大多数时间是 4 毫秒。守护进程的轮询间隔目前为 30 秒，当前应用的漂移率设置为 200 微秒/秒，这两者共同导致了 0 到 6 毫秒的锯齿界限。剩余的 1 毫秒来自到时间主控机的通信延迟。在存在故障的情况下，可能会出现偏离此锯齿的情况。例如，偶尔的时间主控机不可用可能导致整个数据中心的 $ \epsilon $ 增加。同样，过载的机器和网络链路可能导致偶尔的局部峰值。

## 4. 并发控制

本节描述了 TrueTime 如何被用于保证并发控制的正确性属性，以及这些属性如何被用于实现诸如外部一致性事务、无锁只读事务和过去的非阻塞读取等功能。这些功能使得（例如）保证在时间戳 $ t $ 进行的全库审计读取能够准确看到截至 $ t $ 时已提交的所有事务的影响成为可能。

在接下来的内容中，区分 Paxos 看到的写入（除非上下文清楚，否则我们将称为 Paxos 写入）与 Spanner 客户端写入非常重要。例如，两阶段提交会为准备阶段生成一个 Paxos 写入，但这并没有对应的 Spanner 客户端写入。

### 4.1 时间戳管理

表 2 列出了 Spanner 支持的操作类型。Spanner 实现支持读写事务、只读事务（预声明的快照隔离事务）和快照读取。独立写入被实现为读写事务；非快照独立读取被实现为只读事务。两者都在内部进行重试（客户端无需编写自己的重试循环）。

只读事务是一种具有快照隔离 [6] 性能优势的事务。只读事务必须预先声明为没有任何写入；它不仅仅是一个没有任何写入的读写事务。只读事务中的读取在系统选择的时间戳处无锁执行，因此不会阻塞传入的写入。只读事务中的读取可以在任何足够新的副本上继续执行（第 4.1.3 节）。

快照读取是对过去的读取，它在无锁的情况下执行。客户端可以为快照读取指定一个时间戳，或者提供所需时间戳过时性的上限并让 Spanner 选择一个时间戳。在任何一种情况下，快照读取都在任何足够新的副本上执行。

对于只读事务和快照读取，一旦选择了时间戳，提交就是不可避免的，除非该时间戳的数据已被垃圾回收。因此，客户端可以避免在重试循环内缓冲结果。当服务器发生故障时，客户端可以通过重复时间戳和当前读取位置，在不同的服务器上内部继续查询。

#### 4.1.1 Paxos 领导者租约

Spanner 的 Paxos 实现使用定时租约（timed leases）使领导权长期存在（默认为 10 秒）。潜在的领导者发送定时租约投票请求；在收到法定人数的租约投票后，领导者知道它拥有了租约。副本在成功写入时隐式延长其租约投票，如果租约投票接近过期，领导者会请求延长租约投票。定义领导者的租约间隔为：从它发现自己拥有法定人数的租约投票时开始，到它不再拥有法定人数的租约投票（因为某些投票已过期）时结束。

Spanner 依赖于以下的不相交性不变量（disjointness invariant）：对于每个 Paxos 组，每个 Paxos 领导者的租约间隔与任何其他领导者的租约间隔都是不相交的。附录 A 描述了如何强制执行此不变量。

Spanner 实现允许 Paxos 领导者通过释放其从属的租约投票来退位。为了保持不相交性不变量，Spanner 限制了何时允许退位。定义 $ s_{max} $ 为领导者使用的最大时间戳。后续章节将描述 $ s_{max} $ 何时推进。在退位之前，领导者必须等待直到 $ \text{TT.after}(s_{max}) $ 为真。

#### 4.1.2 为读写事务分配时间戳

事务性读写使用两阶段锁定。因此，它们可以在所有锁都已获取之后、但在任何锁释放之前的任何时间被分配时间戳。对于给定的事务，Spanner 将 Paxos 分配给表示事务提交的 Paxos 写入的时间戳分配给该事务。

Spanner 依赖于以下的单调性不变量（monotonicity invariant）：在每个 Paxos 组内，Spanner 以单调递增的顺序为 Paxos 写入分配时间戳，即使跨越不同的领导者也是如此。单个领导者副本可以简单地以单调递增的顺序分配时间戳。通过利用不相交性不变量，在领导者之间强制执行此不变量：领导者必须仅在其领导者租约的间隔内分配时间戳。请注意，每当分配时间戳 $ s $ 时，$ s_{max} $ 都会推进到 $ s $ 以保持不相交性。

Spanner 还强制执行以下的外部一致性不变量（external consistency invariant）：如果事务 $ T_2 $ 的开始发生在事务 $ T_1 $ 提交之后，则 $ T_2 $ 的提交时间戳必须大于 $ T_1 $ 的提交时间戳。定义事务 $ T_i $ 的开始和提交事件为 $ e_{start_i} $ 和 $ e_{commit_i} $；事务 $ T_i $ 的提交时间戳为 $ s_i $。不变量变为：
\[ t_{abs}(e_{commit_1}) < t_{abs}(e_{start_2}) \Rightarrow s_1 < s_2 \]

执行事务和分配时间戳的协议遵循两条规则，这两条规则共同保证了此不变量，如下所示。定义写入 $ T_i $ 的提交请求在协调者领导者处的到达事件为 $ e_{server_i} $。

* **开始（Start）**：写入 $ T_i $ 的协调者领导者分配一个提交时间戳 $ s_i $，该时间戳不小于在 $ e_{server_i} $ 之后计算的 $ \text{TT.now().latest} $。注意，参与者领导者在这里并不重要；第 4.2.1 节描述了它们如何参与下一条规则的实现。
* **提交等待（Commit Wait）**：协调者领导者确保客户端在 $ \text{TT.after}(s_i) $ 为真之前无法看到 $ T_i $ 提交的任何数据。提交等待确保 $ s_i $ 小于 $ T_i $ 的绝对提交时间，即 $ s_i < t_{abs}(e_{commit_i}) $。提交等待的实现将在 4.2.1 节中描述。

**证明：**
$$
\begin{aligned}
s_1 & < t_{abs}(e_{commit_1}) & (\text{提交等待}) \\
t_{abs}(e_{commit_1}) & < t_{abs}(e_{start_2}) & (\text{假设}) \\
t_{abs}(e_{start_2}) & \le t_{abs}(e_{server_2}) & (\text{因果关系}) \\
t_{abs}(e_{server_2}) & \le s_2 & (\text{开始规则}) \\
s_1 & < s_2 & (\text{传递性})
\end{aligned}
$$

#### 4.1.3 在特定时间戳提供读取服务

第 4.1.2 节中描述的单调性不变量允许 Spanner 正确确定副本的状态是否足够新以满足读取请求。每个副本跟踪一个称为安全时间 $ t_{safe} $ 的值，这是副本处于最新状态的最大时间戳。如果 $ t \le t_{safe} $，副本可以在时间戳 $ t $ 满足读取。

定义 $ t_{safe} = \min(t_{safe}^{Paxos}, t_{safe}^{TM}) $，其中每个 Paxos 状态机都有一个安全时间 $ t_{safe}^{Paxos} $，每个事务管理器都有一个安全时间 $ t_{safe}^{TM} $。$ t_{safe}^{Paxos} $ 比较简单：它是最高已应用的 Paxos 写入的时间戳。因为时间戳单调增加且写入按顺序应用，所以相对于 Paxos，不再会有发生在 $ t_{safe}^{Paxos} $ 或其之前的写入。

如果在副本上没有准备好（但未提交）的事务——即处于两阶段提交的两个阶段之间的事务，则 $ t_{safe}^{TM} $ 为 $ \infty $。(对于参与者从属，$ t_{safe}^{TM} $ 实际上是指副本的领导者的事务管理器，从属可以通过 Paxos 写入传递的元数据推断其状态。) 如果有任何此类事务，则受这些事务影响的状态是不确定的：参与者副本还不知道此类事务是否会提交。正如我们在 4.2.1 节中讨论的，提交协议确保每个参与者都知道准备好的事务的时间戳下限。对于事务 $ T_i $ 的每个参与者领导者（针对组 $ g $），都会为其准备记录分配一个准备时间戳 $ s_{prepare_{i,g}} $。协调者领导者确保在所有参与者组 $ g $ 上，事务的提交时间戳 $ s_i \ge s_{prepare_{i,g}} $。因此，对于组 $ g $ 中的每个副本，在所有于 $ g $ 处准备的事务 $ T_i $ 上，
\[ t_{safe}^{TM} = \min_i(s_{prepare_{i,g}}) - 1 \]
（针对所有在 $ g $ 处准备的事务）。

#### 4.1.4 为只读事务分配时间戳

只读事务分两个阶段执行：分配时间戳 $ s_{read} $ [8]，然后作为 $ s_{read} $ 处的快照读取来执行事务的读取。快照读取可以在任何足够新的副本上执行。

在事务开始后的任何时间简单的分配 $ s_{read} = \text{TT.now().latest} $，通过类似于 4.1.2 节中针对写入提出的论证，可以保持外部一致性。然而，如果 $ t_{safe} $ 尚未充分推进，这样的时间戳可能需要 $ s_{read} $ 处的数据读取被阻塞。（此外，请注意，选择 $ s_{read} $ 的值也可能会推进 $ s_{max} $ 以保持不相交性。）为了减少阻塞的机会，Spanner 应分配保持外部一致性的最旧时间戳。第 4.2.2 节解释了如何选择这样的时间戳。

## 4.2 细节

本节解释了之前略过的读写事务和只读事务的一些实践细节，以及用于实现原子模式变更（atomic schema changes）的一种特殊事务类型的实现。随后，我们将描述对上述基本方案的一些改进。

### 4.2.1 读写事务

与 Bigtable 类似，事务中发生的写入会在客户端缓冲，直到提交。因此，事务中的读取不会看到该事务写入的效果。这种设计在 Spanner 中运作良好，因为读取返回任何已读取数据的时间戳，而未提交的写入尚未分配时间戳。

读写事务中的读取使用 wound-wait [33] 机制来避免死锁。客户端向适当组的领导者副本发出读取请求，该副本获取读锁然后读取最新数据。当客户端事务保持开启状态时，它会发送心跳（keepalive）消息以防止参与者领导者将该事务超时。当客户端完成了所有读取并缓冲了所有写入后，它开始两阶段提交。客户端选择一个协调者组，并向每个参与者的领导者发送提交消息，其中包含协调者的身份和任何缓冲的写入。由客户端驱动两阶段提交避免了在广域网链路上传输两次数据。

非协调者参与者领导者首先获取写锁。然后它选择一个必须大于其分配给先前事务的任何时间戳的准备时间戳（以保持单调性），并通过 Paxos 记录准备记录。每个参与者随后将其准备时间戳通知协调者。

协调者领导者也首先获取写锁，但跳过准备阶段。在收到所有其他参与者领导者的回复后，它为整个事务选择一个时间戳。提交时间戳 $ s $ 必须大于或等于所有准备时间戳（以满足 4.1.3 节讨论的约束），大于协调者收到提交消息时的 $ TT.now().latest $，并且大于领导者分配给先前事务的任何时间戳（同样是为了保持单调性）。然后，协调者领导者通过 Paxos 记录提交记录（或者如果在等待其他参与者时超时则记录中止）。

在允许任何协调者副本应用提交记录之前，协调者领导者会等待直到 $ TT.after(s) $ 为真，以遵守 4.1.2 节中描述的提交等待规则。因为协调者领导者是基于 $ TT.now().latest $ 选择的 $ s $，而现在要等待直到该时间戳保证位于过去，所以预期的等待时间至少为 $ 2\bar{\epsilon} $。这段等待时间通常与 Paxos 通信重叠。在提交等待之后，协调者将提交时间戳发送给客户端和所有其他参与者领导者。每个参与者领导者通过 Paxos 记录事务的结果。所有参与者在相同的时间戳应用变更，然后释放锁。

### 4.2.2 只读事务

分配时间戳需要在所有涉及读取的 Paxos 组之间进行协商阶段。因此，Spanner 要求每个只读事务都有一个作用域表达式（scope expression），该表达式总结了整个事务将要读取的键。Spanner 会自动推断独立查询的作用域。

如果作用域内的值由单个 Paxos 组提供服务，则客户端向该组的领导者发出只读事务。（当前的 Spanner 实现仅在 Paxos 领导者处为只读事务选择时间戳。）该领导者分配 $ s_{read} $ 并执行读取。对于单站点读取，Spanner 通常比 $ TT.now().latest $ 做得更好。定义 $ LastTS() $ 为 Paxos 组中最后一次提交写入的时间戳。如果没有准备好的事务，分配 $ s_{read} = LastTS() $ 简单地满足外部一致性：事务将看到最后一次写入的结果，因此被排序在其之后。

如果作用域内的值由多个 Paxos 组提供服务，则有几种选择。最复杂的选择是与所有组的领导者进行一轮通信，根据 $ LastTS() $ 协商 $ s_{read} $。Spanner 目前实现了一个更简单的选择。客户端避免协商轮次，直接让其读取在 $ s_{read} = TT.now().latest $ 处执行（这可能需要等待安全时间推进）。事务中的所有读取都可以发送到足够新的副本。

### 4.2.3 模式变更事务

TrueTime 使 Spanner 能够支持原子模式变更。使用标准事务是不可行的，因为参与者的数量（数据库中的组数量）可能达到数百万。Bigtable 支持在一个数据中心内的原子模式变更，但其模式变更会阻塞所有操作。

Spanner 的模式变更事务是标准事务的一种通常不阻塞的变体。首先，它被显式分配了一个未来的时间戳，该时间戳在准备阶段注册。因此，跨越数千台服务器的模式变更可以在对其他并发活动干扰最小的情况下完成。其次，隐式依赖于模式的读写操作在时间 $ t $ 与任何已注册的模式变更时间戳同步：如果它们的时间戳早于 $ t $，则可以继续进行；但如果它们的时间戳在 $ t $ 之后，则必须阻塞在模式变更事务之后。如果没有 TrueTime，定义模式变更发生在 $ t $ 将毫无意义。

### 4.2.4 改进 (Refinements)

上文定义的 $ t_{safe}^{TM} $ 有一个弱点，即单个准备好的（prepared）事务会阻止 $ t_{safe} $ 的推进。结果是，即使读取操作与该事务不冲突，也无法在更晚的时间戳进行读取。这种虚假冲突可以通过增强 $ t_{safe}^{TM} $ 来消除：使用一个从键范围（key ranges）到准备好的事务时间戳的细粒度映射。这些信息可以存储在锁表中，锁表已经将键范围映射到了锁的元数据。当一个读取到达时，它只需要根据与其冲突的键范围的细粒度安全时间进行检查。

上文定义的 $ LastTS() $ 也有类似的弱点：如果一个事务刚刚提交，一个不冲突的只读事务仍然必须被分配 $ s_{read} $ 以便跟随在该事务之后。结果是，读取的执行可能会被延迟。这个弱点可以通过类似的方法补救：在锁表中增强 $ LastTS() $，增加一个从键范围到提交时间戳的细粒度映射。（我们尚未实现此优化。）当一个只读事务到达时，除非存在冲突的准备好的事务（这可以通过细粒度安全时间确定），否则可以通过取该事务冲突的键范围的 $ LastTS() $ 的最大值来分配其时间戳。

上文定义的 $ t_{safe}^{Paxos} $ 有一个弱点，即在没有 Paxos 写入的情况下它无法推进。也就是说，在 $ t $ 时刻的快照读取无法在最后一次写入发生在 $ t $ 之前的 Paxos 组执行。Spanner 通过利用领导者租约间隔的不相交性来解决这个问题。每个 Paxos 领导者通过保持一个阈值来推进 $ t_{safe}^{Paxos} $，未来的写入时间戳将发生在该阈值之上：它维护一个从 Paxos 序列号 $ n $ 到可能分配给 Paxos 序列号 $ n + 1 $ 的最小时间戳的映射 `MinNextTS(n)`。当副本已应用到 $ n $ 时，它可以将 $ t_{safe}^{Paxos} $ 推进到 `MinNextTS(n) - 1`。

单个领导者可以轻松地强制执行其 `MinNextTS()` 承诺。因为 `MinNextTS()` 承诺的时间戳位于领导者的租约内，不相交性不变量强制执行了跨领导者的 `MinNextTS()` 承诺。如果领导者希望将 `MinNextTS()` 推进到其领导者租约结束之后，它必须先延长其租约。注意，为了保持不相交性，$ s_{max} $ 总是被推进到 `MinNextTS()` 中的最大值。

默认情况下，领导者每 8 秒推进一次 `MinNextTS()` 值。因此，在没有准备好的事务的情况下，空闲 Paxos 组中的健康从属副本在最坏情况下可以提供 8 秒前的读取服务。领导者也可以根据从属副本的请求按需推进 `MinNextTS()` 值。

## 5. 评估 (Evaluation)

我们首先测量 Spanner 在复制、事务和可用性方面的性能。然后我们提供一些关于 TrueTime 行为的数据，以及我们的第一个客户 F1 的案例研究。

### 5.1 微基准测试 (Microbenchmarks)

表 3 展示了 Spanner 的一些微基准测试。这些测量是在分时共享的机器上进行的：每个 Spanserver 运行在 4GB RAM 和 4 核 (AMD Barcelona 2200MHz) 的调度单元上。客户端运行在独立的机器上。每个区域包含一个 Spanserver。客户端和区域被放置在一组网络距离小于 1ms 的数据中心中。（这种布局应该是常见的：大多数应用程序不需要将其所有数据分布到世界各地。）测试数据库创建了 50 个 Paxos 组和 2500 个目录。操作是 4KB 的独立读写。所有的读取都在压缩（compaction）后从内存中服务，以便我们只测量 Spanner 调用栈的开销。此外，首先进行了一轮未测量的读取以预热任何位置缓存。

对于延迟实验，客户端发出的操作足够少，以避免在服务器排队。从 1 个副本的实验来看，提交等待大约是 5ms，Paxos 延迟大约是 9ms。随着副本数量的增加，延迟大致保持不变，标准差更小，因为 Paxos 在组的副本处并行执行。随着副本数量的增加，获得法定人数（quorum）的延迟对单个从属副本的缓慢变得不那么敏感。

对于吞吐量实验，客户端发出的操作足够多，以使服务器的 CPU 饱和。快照读取可以在任何足够新的副本上执行，因此其吞吐量随副本数量几乎线性增加。单次读取的只读事务仅在领导者处执行，因为时间戳分配必须在领导者处发生。只读事务的吞吐量随副本数量增加而增加，因为有效的 Spanserver 数量增加了：在实验设置中，Spanserver 的数量等于副本的数量，并且领导者随机分布在区域中。写入吞吐量受益于同样的实验假象（这解释了从 3 到 5 个副本时吞吐量的增加），但这种益处被每次写入执行的工作量随副本数量增加而线性增加所抵消。

表 4 表明两阶段提交可以扩展到合理数量的参与者：它总结了一组跨 3 个区域运行的实验，每个区域有 25 个 Spanserver。扩展到 50 个参与者在平均值和第 99 百分位上都是合理的，而在 100 个参与者时延迟开始明显上升。

### 5.2 可用性 (Availability)

图 5 说明了在多个数据中心运行 Spanner 的可用性优势。它显示了在存在数据中心故障的情况下吞吐量的三个实验结果，所有结果都叠加在同一时间轴上。测试宇宙由 5 个区域 $ Z_i $ 组成，每个区域有 25 个 Spanserver。测试数据库被分片为 1250 个 Paxos 组，100 个测试客户端不断发出非快照读取，总速率为 50K 读取/秒。所有的领导者都被显式放置在 $ Z_1 $。在每个测试开始 5 秒后，一个区域中的所有服务器被杀死：

* **非领导者 (non-leader)**：杀死 $ Z_2 $。
* **领导者-硬 (leader-hard)**：杀死 $ Z_1 $。
* **领导者-软 (leader-soft)**：杀死 $ Z_1 $，但它会向所有服务器发出通知，告知它们应首先移交领导权。

杀死 $ Z_2 $ 对读取吞吐量没有影响。在给予领导者时间将领导权移交给不同区域的情况下杀死 $ Z_1 $ 只有轻微影响：吞吐量下降在图中不可见，大约为 3-4%。另一方面，毫无预警地杀死 $ Z_1 $ 会产生严重影响：完成率几乎下降到 0。然而，随着领导者重新选举，系统的吞吐量上升到大约 100K 读取/秒，这是由于我们实验的两个假象：系统中有额外的容量，并且在领导者不可用时操作被排队。结果，系统的吞吐量在再次平稳到其稳态速率之前会上升。

我们还可以看到 Paxos 领导者租约设置为 10 秒的影响。当我们杀死区域时，各组的领导者租约到期时间应该均匀分布在接下来的 10 秒内。在每个死掉的领导者的租约到期后不久，新的领导者就会被选举出来。在杀死时间大约 10 秒后，所有的组都有了领导者，吞吐量也恢复了。较短的租约时间会减少服务器死亡对可用性的影响，但需要更大量的租约续期网络流量。我们正在设计和实现一种机制，该机制将导致从属副本在领导者故障时释放 Paxos 领导者租约。

### 5.3 TrueTime

关于 TrueTime 必须回答两个问题：$ \epsilon $ 是否真的是时钟不确定性的界限？$ \epsilon $ 会变得多糟糕？

对于前者，最严重的问题是如果本地时钟的漂移大于 200us/sec：这将破坏 TrueTime 所做的假设。我们的机器统计数据表明，坏的 CPU 比坏的时钟出现的可能性高 6 倍。也就是说，相对于更严重的硬件问题，时钟问题极为罕见。因此，我们相信 TrueTime 的实现与 Spanner 依赖的任何其他软件一样值得信赖。

图 6 展示了在相距高达 2200 公里的数据中心中的数千台 Spanserver 机器上获取的 TrueTime 数据。它绘制了 $ \epsilon $ 的第 90、99 和 99.9 百分位数，这是在时间从属守护进程轮询时间主控机后立即采样的。这种采样忽略了由于本地时钟不确定性导致的 $ \epsilon $ 锯齿，因此测量的是时间主控机的不确定性（通常为 0）加上到时间主控机的通信延迟。

数据显示，决定 $ \epsilon $ 基准值的这两个因素通常不是问题。然而，可能会有显著的长尾延迟问题导致更高的 $ \epsilon $ 值。从 3 月 30 日开始的长尾延迟减少是由于网络改进减少了瞬态网络链路拥塞。4 月 13 日 $ \epsilon $ 的增加持续了大约一小时，这是由于为了例行维护关闭了一个数据中心的 2 个时间主控机。我们继续调查并消除 TrueTime 峰值的原因。

### 5.4 F1

Spanner 于 2011 年初开始在生产工作负载下进行实验性评估，作为 Google 广告后端重写项目 F1 [35] 的一部分。该后端最初基于一个手动进行了多种分片的 MySQL 数据库。未压缩的数据集有数十 TB，与许多 NoSQL 实例相比虽然较小，但对于分片的 MySQL 来说已大到足以造成困难。MySQL 的分片方案将每个客户及其所有相关数据分配给一个固定的分片。这种布局支持基于每个客户使用索引和复杂的查询处理，但需要在应用程序业务逻辑中了解分片情况。随着客户数量及其数据的增长，对这个涉及关键营收的数据库进行重新分片（resharding）代价极高。上一次重新分片花费了两年多的高强度努力，并涉及数十个团队的协调和测试以将风险降至最低。这种操作太复杂了，无法定期进行：结果，团队不得不通过将一些数据存储在外部 Bigtable 中来限制 MySQL 数据库的增长，但这牺牲了事务行为和跨所有数据查询的能力。

F1 团队选择使用 Spanner 有几个原因。首先，Spanner 免除了手动重新分片的需要。其次，Spanner 提供了同步复制和自动故障转移。使用 MySQL 主从复制，故障转移很困难，并且存在数据丢失和停机的风险。第三，F1 需要强事务语义，这使得使用其他 NoSQL 系统变得不切实际。应用程序语义需要跨任意数据的事务以及一致性读取。F1 团队还需要数据的二级索引（因为 Spanner 尚未提供对二级索引的自动支持），并且能够使用 Spanner 事务实现自己的一致性全局索引。

现在，所有应用程序的写入默认都通过 F1 发送到 Spanner，而不是基于 MySQL 的应用程序栈。F1 在美国西海岸有 2 个副本，在东海岸有 3 个副本。选择这些副本站点是为了应对潜在的重大自然灾害导致的中断，同时也考虑了其前端站点的选择。据传，Spanner 的自动故障转移对他们来说几乎是不可见的。尽管在过去几个月中出现了计划外的集群故障，但 F1 团队最多只需要更新其数据库的模式，告诉 Spanner 优先将 Paxos 领导者放置在哪里，以便使它们靠近其前端移动到的位置。

Spanner 的时间戳语义使得 F1 能够高效地维护从数据库状态计算出的内存数据结构。F1 维护所有更改的逻辑历史日志，该日志作为每个事务的一部分写入 Spanner 本身。F1 在某个时间戳获取数据的完整快照以初始化其数据结构，然后读取增量更改以更新它们。

表 5 展示了 F1 中每个目录的片段（fragment）数量分布。每个目录通常对应于 F1 上层应用程序栈中的一个客户。绝大多数目录（因此也是绝大多数客户）仅由 1 个片段组成，这意味着对这些客户数据的读写保证仅发生在单个服务器上。包含超过 100 个片段的目录都是包含 F1 二级索引的表：对此类表的超过少数几个片段进行写入是极其罕见的。F1 团队仅在作为事务进行未调优的批量数据加载时才观察到这种行为。

表 6 展示了从 F1 服务器测量的 Spanner 操作延迟。东海岸数据中心的副本在选择 Paxos 领导者时被给予更高的优先级。表中的数据是从这些数据中心的 F1 服务器测量的。写入延迟的巨大标准差是由锁冲突导致的相当明显的长尾效应（fat tail）引起的。读取延迟甚至更大的标准差部分是由于 Paxos 领导者分布在两个数据中心，其中只有一个拥有配备 SSD 的机器。此外，测量包括了来自两个数据中心的系统中的每一次读取：读取字节的平均值和标准差分别约为 1.6KB 和 119KB。

## 6. 相关工作

Megastore [5] 和 DynamoDB [3] 提供了跨数据中心的一致性复制存储服务。DynamoDB 提供键值接口，并且仅在一个区域内进行复制。Spanner 追随 Megastore，提供了半关系数据模型，甚至类似的模式语言。Megastore 并未实现高性能。它分层构建在 Bigtable 之上，这带来了高昂的通信成本。它也不支持长寿命的领导者：多个副本可能会发起写入。来自不同副本的所有写入必然会在 Paxos 协议中发生冲突，即使它们在逻辑上并不冲突：在每秒几次写入的情况下，Paxos 组的吞吐量就会崩溃。Spanner 提供了更高的性能、通用事务和外部一致性。

Pavlo 等人 [31] 比较了数据库和 MapReduce [12] 的性能。他们指出了其他几项试图在分布式键值存储 [1, 4, 7, 41] 之上构建数据库功能的努力，以此作为两个世界正在融合的证据。我们同意这一结论，但也证明了集成多个层级有其优势：例如，将并发控制与复制集成在一起减少了 Spanner 中提交等待（commit wait）的成本。

在复制存储之上分层构建事务的概念至少可以追溯到 Gifford 的论文 [16]。Scatter [17] 是最近的一个基于 DHT 的键值存储，它在一致性复制之上分层构建了事务。Spanner 专注于提供比 Scatter 更高层级的接口。Gray 和 Lamport [18] 描述了一种基于 Paxos 的非阻塞提交协议。他们的协议比两阶段提交产生更多的消息成本，这将加剧在广泛分布的组上进行提交的成本。Walter [36] 提供了一种快照隔离的变体，它适用于数据中心内部，但不适用于跨数据中心。相比之下，我们的只读事务提供了更自然的语义，因为我们支持所有操作的外部一致性。

最近有大量关于减少或消除锁定开销的工作。Calvin [40] 消除了并发控制：它预先分配时间戳，然后按时间戳顺序执行事务。H-Store [39] 和 Granola [11] 各自支持其自己的事务类型分类，其中一些可以避免锁定。这些系统都不提供外部一致性。Spanner 通过提供对快照隔离的支持来解决争用问题。

VoltDB [42] 是一个分片的内存数据库，支持广域网范围内的主从复制以进行灾难恢复，但不支持更通用的复制配置。它是被称为 NewSQL 的一个例子，这是对支持可扩展 SQL [38] 的市场推动。许多商业数据库实现了对过去数据的读取，例如 MarkLogic [26] 和 Oracle 的 Total Recall [30]。Lomet 和 Li [24] 描述了此类时态数据库的实现策略。

Farsite 推导出了相对于可信时钟参考的时钟不确定性界限（比 TrueTime 的要宽松得多）[13]：Farsite 中的服务器租约维护方式与 Spanner 维护 Paxos 租约的方式相同。松散同步的时钟在先前的工作中已被用于并发控制目的 [2, 23]。我们已经表明，TrueTime 让人可以跨 Paxos 状态机集对全局时间进行推理。

## 7. 未来工作

我们在过去一年中花费了大部分时间与 F1 团队合作，将 Google 的广告后端从 MySQL 过渡到 Spanner。我们正在积极改进其监控和支持工具，并调整其性能。此外，我们一直致力于改进备份/恢复系统的功能和性能。我们目前正在实现 Spanner 模式语言、二级索引的自动维护以及基于负载的自动重新分片。从长远来看，我们计划研究几个特性。乐观地并行执行读取可能是一个有价值的策略，但初步实验表明正确的实现并非易事。此外，我们计划最终支持直接更改 Paxos 配置 [22, 34]。

鉴于我们预计许多应用程序会将其数据复制到彼此相对较近的数据中心，TrueTime 可能会显着影响性能。我们认为将 $ \epsilon $ 降低到 1ms 以下没有不可逾越的障碍。时间主控机查询间隔可以减少，更好的晶振也相对便宜。通过改进的网络技术，可以减少时间主控机的查询延迟，甚至可能通过替代的时间分发技术来避免这种延迟。

最后，还有明显的改进领域。尽管 Spanner 在节点数量上具有可扩展性，但节点本地数据结构在复杂 SQL 查询上的性能相对较差，因为它们是为简单的键值访问而设计的。数据库文献中的算法和数据结构可以极大地提高单节点性能。其次，根据客户端负载的变化在数据中心之间自动移动数据一直我们的目标，但要有效地实现这一目标，我们还需要能够以自动化、协调的方式在数据中心之间移动客户端应用程序进程。移动进程引发了更困难的问题，即管理数据中心之间的资源获取和分配。

## 8. 结论

总而言之，Spanner 结合并扩展了两个研究社区的理念：来自数据库社区的熟悉、易于使用的半关系接口、事务和基于 SQL 的查询语言；来自系统社区的可扩展性、自动分片、容错、一致性复制、外部一致性和广域分布。自 Spanner 构想以来，我们花了 5 年多的时间迭代到当前的设计和实现。这个漫长的迭代阶段部分是因为我们要慢慢意识到，Spanner 不应仅仅解决全球复制命名空间的问题，还应专注于 Bigtable 所缺失的数据库功能。

我们设计的一个方面非常突出：Spanner 功能集的关键（linchpin）是 TrueTime。我们已经表明，在时间 API 中将时钟不确定性具体化，使得构建具有更强时间语义的分布式系统成为可能。此外，随着底层系统对时钟不确定性实施更严格的界限，更强语义的开销也会减少。作为一个社区，我们在设计分布式算法时，不应再依赖松散同步的时钟和弱时间 API。

## 致谢 (Acknowledgement)

许多人帮助完善了这篇论文：我们的导师 Jon Howell，他承担了超出其职责的工作；匿名审稿人；以及许多 Google 员工：Atul Adya, Fay Chang, Frank Dabek, Sean Dorward, Bob Gruber, David Held, Nick Kline, Alex Thomson 和 Joel Wein。我们的管理层非常支持我们的工作以及这篇论文的发表：Aristotle Balogh, Bill Coughran, Urs Hölzle, Doron Meyer, Cos Nicolaou, Kathy Polizzi, Sridhar Ramaswany 和 Shivakumar Venkataraman。

我们建立在 Bigtable 和 Megastore 团队的工作基础之上。F1 团队，特别是 Jeff Shute，在开发我们的数据模型方面与我们密切合作，并在追踪性能和正确性错误方面提供了巨大帮助。Platforms 团队，特别是 Luiz Barroso 和 Bob Felderman，帮助实现了 TrueTime。最后，许多曾经在我们团队的 Google 员工：Ken Ashcraft, Paul Cychosz, Krzysztof Ostrowski, Amir Voskoboynik, Matthew Weaver, Theo Vassilakis 和 Eric Veach；或者最近加入我们团队的成员：Nathan Bales, Adam Beberg, Vadim Borisov, Ken Chen, Brian Cooper, Cian Cullinan, Robert-Jan Huijsman, Milind Joshi, Andrey Khorlin, Dawid Kuroczko, Laramie Leavitt, Eric Li, Mike Mammarella, Sunil Mushran, Simon Nielsen, Ovidiu Platon, Ananth Shrinivas, Vadim Suvorov 和 Marcel van der Holst。

*(注：此处省略参考文献列表 [1]-[42]，因其均为引用的学术著作名称)*

## 附录 A：Paxos 领导者租约管理

确保 Paxos 领导者租约间隔不相交的最简单方法是，每当领导者要延长租约时，就发出一个包含租约间隔的同步 Paxos 写入。随后的领导者将读取该间隔并等待直到该间隔过去。

TrueTime 可用于在没有这些额外日志写入的情况下确保不相交性。潜在的第 $ i $ 个领导者保留一个从副本 $ r $ 获得的租约投票开始时间的下限 $ v_{leader}^{i,r} = TT.now().earliest $，该值在 $ e_{send}^{i,r} $（定义为领导者发送租约请求的时间）之前计算。每个副本 $ r $ 在 $ t_{grant}^{i,r} $ 授予租约，这发生在 $ e_{receive}^{i,r} $（副本收到租约请求的时间）之后；租约在 $ t_{end}^{i,r} = TT.now().latest + 10 $ 结束，该值在 $ e_{receive}^{i,r} $ 之后计算。副本 $ r $ 遵守**单次投票规则 (single vote rule)**：在 $ TT.after(t_{end}^{i,r}) $ 为真之前，它不会授予另一个租约投票。为了在 $ r $ 的不同实例之间强制执行此规则，Spanner 在授予租约之前会在授予副本处记录一次租约投票；这个日志写入可以捎带（piggybacked）在现有的 Paxos 协议日志写入上。

当第 $ i $ 个领导者收到法定人数（quorum）的投票（事件 $ e_{quorum}^i $）时，它将其租约间隔计算为 $ lease_i = [TT.now().latest, \min_r(v_{leader}^{i,r}) + 10] $。当 $ TT.before(\min_r(v_{leader}^{i,r}) + 10) $ 为假时，领导者认为租约已过期。为了证明不相交性，我们利用第 $ i $ 个和第 $ i+1 $ 个领导者在其法定人数中必须有一个共同副本的事实。称该副本为 $ r_0 $。

**证明：**

$$
\begin{aligned}
lease_i.end & = \min_r(v_{leader}^{i,r}) + 10 & (\text{根据定义}) \\
\min_r(v_{leader}^{i,r}) + 10 & \le v_{leader}^{i,r_0} + 10 & (\text{最小值性质}) \\
v_{leader}^{i,r_0} + 10 & \le t_{abs}(e_{send}^{i,r_0}) + 10 & (\text{根据定义}) \\
t_{abs}(e_{send}^{i,r_0}) + 10 & \le t_{abs}(e_{receive}^{i,r_0}) + 10 & (\text{因果关系}) \\
t_{abs}(e_{receive}^{i,r_0}) + 10 & \le t_{end}^{i,r_0} & (\text{根据定义}) \\
t_{end}^{i,r_0} & < t_{abs}(e_{grant}^{i+1,r_0}) & (\text{单次投票规则}) \\
t_{abs}(e_{grant}^{i+1,r_0}) & \le t_{abs}(e_{quorum}^{i+1}) & (\text{因果关系}) \\
t_{abs}(e_{quorum}^{i+1}) & \le lease_{i+1}.start & (\text{根据定义})
\end{aligned}
$$

由此可证：$ lease_i.end < lease_{i+1}.start $，即租约间隔是不相交的。
