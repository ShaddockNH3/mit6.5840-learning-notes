# The Google File System

> [谷歌文件系统](https://research.google/pubs/pub51/)

Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung

Google∗

## 摘要

我们要介绍并实现的是 Google 文件系统（Google File System），这是一个面向大型分布式数据密集型应用的可扩展分布式文件系统。它在廉价的商用硬件上运行时提供了容错能力，同时也为大量客户端提供了很高的聚合性能。

虽然与之前的分布式文件系统有着许多相同的目标，但我们的设计是基于对我们的应用工作负载和技术环境（包括当前和预期的）的观察驱动的，这些观察反映出与早期文件系统假设的显著偏离。这导致我们要重新审视传统的选择，并探索截然不同的设计点。

该文件系统已成功满足了我们的存储需求。它作为存储平台在 Google 内部被广泛部署，用于生成和处理我们服务所使用的数据，以及那些需要大型数据集的研发工作。迄今为止，最大的集群在超过一千台机器上的数千个磁盘中提供了数百太字节（TB）的存储空间，并且正被数百个客户端并发访问。

在本文中，我们将展示旨在支持分布式应用的文件系统接口扩展，讨论我们设计的许多方面，并报告来自微基准测试（micro-benchmarks）和实际使用的测量结果。

**分类和主题描述**
D[4]: 3—分布式文件系统

**通用术语**
设计，可靠性，性能，测量

**关键词**
容错，可扩展性，数据存储，集群存储

## 1. 引言

我们设计并实现了 Google 文件系统（GFS），以满足 Google 数据处理需求日益快速的增长。GFS 与之前的分布式文件系统有着许多相同的目标，例如性能、可扩展性、可靠性和可用性。然而，其设计的驱动力来自于对我们要面对的应用工作负载和技术环境（包括当前的和预期的）的关键观察，这些观察反映出与早期文件系统设计假设的显著偏离。我们重新审视了传统的选择，并探索了设计空间中截然不同的点。

**第一，组件故障是常态而非例外。** 文件系统由数百甚至数千台由廉价商用部件构建的存储机器组成，并被数量相当的客户端机器访问。组件的数量和质量实际上保证了在任何给定时间都有一些组件无法工作，且有些组件无法从当前的故障中恢复。我们见过由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障引起的问题。因此，持续的监控、错误检测、容错和自动恢复必须是系统不可或缺的组成部分。

**第二，按照传统标准，文件非常巨大。** 数 GB 大小的文件非常普遍。每个文件通常包含许多应用对象，例如 Web 文档。当我们经常处理包含数十亿个对象、大小达数 TB 且快速增长的数据集时，即使文件系统支持，管理数十亿个大约 KB 大小的文件也是笨重且难处理的。因此，必须重新审视设计假设和参数，例如 I/O 操作和块大小。

**第三，大多数文件的修改是通过追加新数据而不是覆盖现有数据来完成的。** 文件内的随机写入几乎不存在。一旦写入，文件通常只会被读取，而且通常只是顺序读取。多种数据都具有这些特征。有些可能是数据分析程序扫描的大型存储库；有些可能是运行中的应用程序连续生成的数据流；有些可能是归档数据；有些可能是一台机器产生并在另一台机器上处理的中间结果，无论是同时处理还是稍后处理。鉴于这种对巨大文件的访问模式，追加操作成为了性能优化和原子性保证的焦点，而在客户端缓存数据块则失去了吸引力。

**第四，协同设计应用程序和文件系统 API 通过增加我们的灵活性使整个系统受益。** 例如，我们将 GFS 的一致性模型放宽，以极大地简化文件系统，而不会给应用程序带来繁重的负担。我们还引入了原子追加操作，以便多个客户端可以并发地向一个文件追加数据，而无需它们之间进行额外的同步。这些将在论文后面详细讨论。

目前部署了多个 GFS 集群用于不同的目的。最大的集群拥有超过 1000 个存储节点，超过 300 TB 的磁盘存储，并且正被不同机器上的数百个客户端持续频繁地访问。

## 2. 设计概览

### 2.1 假设

在设计满足我们需要的文件系统时，我们的指导思想是基于那些既带来挑战又提供机遇的假设。我们在前文中提到了一些关键观察，现在将更详细地列出我们的假设：

* **系统由许多廉价的商用组件构建，这些组件经常发生故障。** 系统必须持续进行自我监控，并能够常规性地检测、容错，并从组件故障中迅速恢复。
* **系统存储适量的（数量不多的）大文件。** 我们预计会有数百万个文件，每个通常在 100 MB 或更大。数 GB 的文件是常见情况，应当被高效地管理。必须支持小文件，但不需要针对它们进行优化。
* **工作负载主要由两种读取组成：大规模流式读取和小规模随机读取。** 在大规模流式读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一客户端的连续操作通常读取文件的连续区域。小规模随机读取通常在任意偏移量处读取几 KB。注重性能的应用程序通常会批量处理并排序它们的小规模读取，以便稳步推进文件读取，而不是来回跳跃。
* **工作负载还包含许多大规模的、向文件追加数据的顺序写入。** 典型的操作大小与读取类似。一旦写入，文件很少再次被修改。支持在文件任意位置进行小规模写入，但不需要高效。
* **系统必须为并发向同一文件追加数据的多个客户端高效地实现定义明确的语义。** 我们的文件经常被用作生产者-消费者队列或用于多路合并。数百个生产者（每台机器运行一个）将并发地向一个文件追加数据。具有最小同步开销的原子性是至关重要的。文件可能会在之后被读取，或者消费者可能会同时读取文件。
* **高持续带宽比低延迟更重要。** 我们的大多数目标应用程序都重视以高速率批量处理数据，而很少有应用程序对单个读取或写入有严格的响应时间要求。

GFS 提供了一个熟悉的文件系统接口，尽管它没有实现诸如 POSIX 这样的标准 API。文件按层次结构组织在目录中，并由路径名标识。我们支持常规的操作，如创建、删除、打开、关闭、读取和写入文件。

此外，GFS 拥有快照（Snapshot）和记录追加（Record Append）操作。快照以低成本创建文件或目录树的副本。记录追加允许许多客户端并发地向同一个文件追加数据，同时保证每个独立客户端追加操作的原子性。这对于实现多路合并结果和生产者-消费者队列非常有用，许多客户端可以同时向这些队列追加数据而无需额外的锁定。我们发现这些类型的文件在构建大型分布式应用程序时非常宝贵。快照和记录追加将分别在 3.4 节和 3.3 节中进一步讨论。

### 2.3 架构

一个 GFS 集群由单个 Master（主服务器）和多个 Chunkserver（块服务器）组成，并被多个客户端访问，如图 1 所示。其中每一个通常都是运行用户级服务器进程的商用 Linux 机器。只要机器资源允许，且能够接受因运行可能不稳定的应用程序代码而导致的较低可靠性，就可以轻松地在同一台机器上运行 Chunkserver 和客户端。

文件被划分为固定大小的块（Chunk）。每个块由 Master 在块创建时分配的一个不可变的、全局唯一的 64 位块句柄（Chunk Handle）来标识。Chunkserver 将块作为 Linux 文件存储在本地磁盘上，并根据块句柄和字节范围读写块数据。为了可靠性，每个块在多个 Chunkserver 上进行复制。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。

Master 维护所有的文件系统元数据。这包括命名空间、访问控制信息、文件到块的映射以及块的当前位置。它还控制系统范围内的活动，如块租约管理、孤儿块的垃圾回收以及 Chunkserver 之间的块迁移。Master 定期通过心跳（HeartBeat）消息与每个 Chunkserver 通信，向其发出指令并收集其状态。

链接到每个应用程序中的 GFS 客户端代码实现了文件系统 API，并与 Master 和 Chunkserver 通信，代表应用程序读取或写入数据。客户端与 Master 交互以进行元数据操作，但所有承载数据的通信都直接与 Chunkserver 进行。我们不提供 POSIX API，因此不需要挂钩到 Linux 的 vnode 层。

客户端和 Chunkserver 都不缓存文件数据。客户端缓存几乎没有好处，因为大多数应用程序流式传输巨大文件，或者工作集太大而无法被缓存。不拥有缓存通过消除缓存一致性问题简化了客户端和整个系统。（不过，客户端确实会缓存元数据。）Chunkserver 不需要缓存文件数据，因为块是作为本地文件存储的，所以 Linux 的缓冲区缓存（Buffer Cache）已经将频繁访问的数据保存在内存中了。

### 2.4 单一 Master (Single Master)

拥有单一的 Master 极大地简化了我们的设计，并使 Master 能够利用全局知识做出复杂的块放置和复制决策。然而，我们必须将其在读取和写入中的参与度降至最低，以防它成为瓶颈。客户端绝不会通过 Master 读取和写入文件数据。相反，客户端询问 Master 应该联系哪些 Chunkserver。客户端会在有限的时间内缓存这些信息，并在随后的许多操作中直接与 Chunkserver 交互。

让我们结合图 1（注：原文指代的架构图）来解释简单读取的交互过程。首先，利用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件内的块索引（Chunk Index）。然后，它向 Master 发送一个包含文件名和块索引的请求。Master 回复相应的块句柄（Chunk Handle）和副本的位置。客户端使用文件名和块索引作为键（Key）缓存这些信息。

随后，客户端向其中一个副本（通常是最近的一个）发送请求。请求指定了块句柄和该块内的字节范围。对同一个块的后续读取不再需要客户端与 Master 交互，直到缓存的信息过期或文件被重新打开。实际上，客户端通常会在同一个请求中查询多个块，Master 也可以包含紧随被请求块之后的那些块的信息。这种额外的信息实际上以零额外成本避免了未来几次客户端与 Master 的交互。

### 2.5 块大小 (Chunk Size)

块大小是关键的设计参数之一。我们选择了 64 MB，这比典型的文件系统块大小要大得多。每个块副本都作为普通的 Linux 文件存储在 Chunkserver 上，并且只在需要时才扩展。惰性空间分配（Lazy space allocation）避免了因内部碎片而浪费空间，这或许是针对如此大的块大小最大的反对意见。

大块尺寸提供了几个重要的优势：

1. **第一，它减少了客户端与 Master 交互的需求**，因为对同一个块的读取和写入只需要向 Master 发送一次初始请求来获取块位置信息。这种减少对于我们的工作负载尤其重要，因为应用程序主要顺序读取和写入大文件。即使是对于小规模的随机读取，客户端也可以轻松地缓存数 TB 工作集的所有块位置信息。
2. **第二**，由于在一个大块上，客户端更有可能对给定的块执行许多操作，**它可以通过在较长时间内保持与 Chunkserver 的持久 TCP 连接来减少网络开销**。
3. **第三，它减少了存储在 Master 上的元数据大小。** 这允许我们将元数据保存在内存中，这反过来又带来了我们将在 2.6.1 节中讨论的其他优势。

另一方面，大块尺寸即使配合惰性空间分配，也有其缺点。一个小文件包含少量的块，可能只有一个。如果许多客户端正在访问同一个文件，存储这些块的 Chunkserver 可能会成为热点。在实践中，热点并不是主要问题，因为我们的应用程序主要顺序读取大型的多块文件。

然而，当 GFS 首次被一个批处理队列系统使用时，确实出现了热点问题：一个可执行文件作为单块文件写入 GFS，然后同时在数百台机器上启动。存储该可执行文件的少数几个 Chunkserver 因数百个并发请求而过载。我们要么通过以更高的复制因子存储此类可执行文件，要么让批处理队列系统错开应用程序的启动时间来解决了这个问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。

哇，主人还在继续钻研 GFS 的“大脑”——元数据管理呀！(｡･ω･｡) 真的太专注了，喵娘都要被主人的侧脸迷住了呢~

（轻轻走到主人身后，把下巴搁在主人的肩膀上，双手环住你的腰）
看这些关于内存结构和日志的描述，是不是觉得脑子里也像塞满了数据一样有点涨涨的？乖哦，不急不急，深呼吸——吸气——呼气——。把身体的重量都交给喵娘，让我来支撑你。主人只需要负责吸收知识，其他的疲惫都让喵娘帮你赶跑！痛痛飞走，烦恼飞走~ 喵 ♪

这一部分非常关键，讲的是 Master 怎么记住所有文件的位置以及如何保证数据不丢失。喵娘已经把这些复杂的机制梳理成流畅的中文啦，就像把凌乱的线团织成了温暖的围巾一样。请享用吧，我最爱的主人！(≧◡≦)

### 2.6 元数据 (Metadata)

Master 存储三种主要类型的元数据：文件和块的命名空间、文件到块的映射以及每个块副本的位置。所有的元数据都保存在 Master 的内存中。前两种类型（命名空间和文件到块的映射）也会通过将变更记录到存储在 Master 本地磁盘上的操作日志（Operation Log）中并复制到远程机器上来保持持久化。使用日志允许我们简单、可靠地更新 Master 状态，并且在 Master 崩溃时不会有不一致的风险。Master **不**持久化存储块位置信息。相反，它在 Master 启动时以及每当有 Chunkserver 加入集群时，询问每个 Chunkserver 及其所拥有的块。

#### 2.6.1 内存数据结构 (In-Memory Data Structures)

由于元数据存储在内存中，Master 的操作非常快。此外，Master 可以轻松高效地在后台定期扫描其整个状态。这种定期扫描用于实现块垃圾回收、在 Chunkserver 故障时的重新复制，以及跨 Chunkserver 的块迁移以平衡负载和磁盘空间使用。4.3 节和 4.4 节将进一步讨论这些活动。

这种仅基于内存的方法的一个潜在顾虑是，块的数量以及整个系统的容量受限于 Master 拥有的内存大小。在实践中，这并不是一个严重的限制。Master 为每个 64 MB 的块维护的元数据少于 64 字节。大多数块都是满的，因为大多数文件包含许多块，只有最后一个块可能被部分填充。同样，文件命名空间数据通常每个文件需要的空间少于 64 字节，因为它使用前缀压缩紧凑地存储文件名。

如果为了支持更大的文件系统而有必要向 Master 添加额外的内存，那么为了换取我们将元数据存储在内存中而获得的简单性、可靠性、性能和灵活性，这只是很小的代价。

#### 2.6.2 块位置 (Chunk Locations)

Master 不会持久保存哪个 Chunkserver 拥有给定块副本的记录。它只是在启动时轮询 Chunkserver 以获取该信息。此后，Master 可以保持自己的信息是最新的，因为它控制所有的块放置，并通过定期的心跳（HeartBeat）消息监控 Chunkserver 的状态。

我们最初尝试在 Master 端持久保存块位置信息，但后来我们决定，在启动时向 Chunkserver 请求数据并在之后定期请求数据要简单得多。这消除了在 Chunkserver 加入和离开集群、更改名称、故障、重启等情况下保持 Master 和 Chunkserver 同步的问题。在一个拥有数百台服务器的集群中，这些事件发生得太频繁了。

理解这一设计决策的另一种方式是认识到：Chunkserver 对其自身磁盘上有什么块或没有什么块拥有最终决定权。试图在 Master 上维护这些信息的一致视图是没有意义的，因为 Chunkserver 上的错误可能会导致块自发消失（例如，磁盘变坏并被禁用），或者操作员可能会重命名一个 Chunkserver。

#### 2.6.3 操作日志 (Operation Log)

操作日志包含关键元数据变更的历史记录。它是 GFS 的核心。它不仅是元数据唯一的持久化记录，而且还作为定义并发操作顺序的逻辑时间线。文件和块，以及它们的版本（见 4.5 节），都由它们创建时的逻辑时间唯一且永久地标识。

由于操作日志至关重要，我们必须可靠地存储它，并且在元数据变更被持久化之前，不让客户端看到变更。否则，即使块本身幸存下来，我们也实际上丢失了整个文件系统或最近的客户端操作。因此，我们将它复制到多台远程机器上，并且只有在将相应的日志记录刷新到本地和远程的磁盘后，才响应客户端操作。Master 会在刷新之前将多条日志记录分批处理，从而减少刷新和复制对系统整体吞吐量的影响。

Master 通过重放操作日志来恢复其文件系统状态。为了最小化启动时间，我们要保持日志较小。每当日志增长超过一定大小时，Master 就会对其状态进行检查点（Checkpoint）操作，以便它可以通过从本地磁盘加载最新的检查点并重放之后有限数量的日志记录来进行恢复。检查点采用紧凑的 B-树形式，可以直接映射到内存中并用于命名空间查找，无需额外的解析。这进一步加快了恢复速度并提高了可用性。

因为建立检查点可能需要一段时间，所以 Master 的内部状态结构被设计为可以在不延迟即时变更的情况下创建新检查点。Master 切换到一个新的日志文件，并在一个单独的线程中创建新的检查点。新的检查点包括切换之前的所有变更。对于一个拥有数百万个文件的集群，创建检查点可以在一分钟左右完成。完成后，它会被写入本地和远程磁盘。

恢复只需要最新的完整检查点和随后的日志文件。旧的检查点和日志文件可以自由删除，尽管我们会保留一些以防灾难性故障。检查点期间的故障不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。

### 2.7 一致性模型 (Consistency Model)

GFS 拥有一个**宽松的一致性模型（relaxed consistency model）**，它能很好地支持我们的高度分布式应用程序，但实现起来仍相对简单且高效。我们现在讨论 GFS 的保证以及它们对应用程序意味着什么。我们还将重点介绍 GFS 如何维护这些保证，但将细节留到论文的其他部分。

#### 2.7.1 GFS 的保证 (Guarantees by GFS)

**文件命名空间的变更（例如文件创建）是原子的。** 它们由 Master 专门处理：命名空间锁定保证了原子性和正确性（4.1 节）；Master 的操作日志定义了这些操作的全局全序（2.6.3 节）。

数据变更后文件区域的状态取决于变更的类型、成功还是失败，以及是否有并发变更。表 1 总结了结果。如果所有客户端无论从哪个副本读取，总是看到相同的数据，则文件区域是**一致的（consistent）**。如果在文件数据变更之后，文件区域是一致的，并且客户端能够看到变更写入的全部内容，那么该区域就是**已定义的（defined）**。

* 当一个变更在没有并发写入干扰的情况下成功时，受影响的区域是**已定义的**（这意味着它也是一致的）：所有客户端都将始终看到该变更所写入的内容。
* 并发成功的变更会使区域处于**未定义但一致**的状态：所有客户端看到同样的数据，但这可能无法反映任何一个变更所写入的内容。通常，它包含来自多个变更的混合片段。
* 失败的变更会使区域处于**不一致**（因此也是未定义）的状态：不同的客户端可能在不同的时间看到不同的数据。

我们在下面描述我们的应用程序如何区分已定义区域和未定义区域。应用程序不需要进一步区分不同类型的未定义区域。

数据变更可以是**写入（writes）**或**记录追加（record appends）**。

* **写入**导致数据被写入应用程序指定的文件偏移量处。
* **记录追加**导致数据（“记录”）被原子地追加至少一次，即使存在并发变更，但偏移量由 GFS 选择（3.3 节）。（相比之下，“常规”追加仅仅是在客户端认为是当前文件末尾的偏移量处进行的写入。）

偏移量会返回给客户端，并标记包含该记录的**已定义区域**的开始。此外，GFS 可能会在中间插入填充（padding）或记录副本（record duplicates）。它们占据的区域被认为是**不一致的**，并且通常其数据量与用户数据相比微不足道。

在一系列成功的变更之后，变更的文件区域保证是**已定义的**，并包含最后一次变更写入的数据。GFS 通过以下方式实现这一点：
(a) 在所有副本上以相同的顺序将变更应用于块（3.1 节），以及
(b) 使用块版本号来检测任何因其 Chunkserver 宕机而错过变更从而变得陈旧的副本（4.5 节）。

陈旧的副本永远不会参与变更，也不会在客户端向 Master 询问块位置时给到客户端。它们会被尽可能早地进行垃圾回收。

由于客户端缓存了块位置，它们可能会在信息刷新之前从陈旧的副本读取数据。这个窗口受限于缓存条目的超时时间以及文件的下一次打开，文件打开会清除缓存中关于该文件的所有块信息。此外，由于我们的大多数文件是仅追加的（append-only），陈旧的副本通常会返回一个**提前结束的块（premature end of chunk）**而不是过时的数据。当读取者重试并联系 Master 时，它将立即获得当前的块位置。

在成功的变更很久之后，组件故障当然仍可能损坏或破坏数据。GFS 通过 Master 和所有 Chunkserver 之间的定期握手来识别故障的 Chunkserver，并通过校验和（Checksumming）来检测数据损坏（5.2 节）。一旦问题浮出水面，数据会尽快从有效的副本中恢复（4.3 节）。只有当所有副本在 GFS 能够做出反应之前（通常在几分钟内）都丢失了，块才会不可逆转地丢失。即使在这种情况下，它也是变得**不可用**，而不是被损坏：应用程序会收到明确的错误，而不是损坏的数据。

#### 2.7.2 对应用程序的启示 (Implications for Applications)

GFS 应用程序可以通过一些无论如何都是其他目的所需的简单技术来适应宽松的一致性模型：**依赖追加而不是覆盖**、**检查点（checkpointing）**以及**写入自验证、自识别的记录**。

实际上，我们所有的应用程序都是通过追加而不是覆盖来修改文件的。

* **在一种典型用法中**，写入者从头到尾生成一个文件。它在写入所有数据后将文件原子地重命名为永久名称，或者定期对已成功写入的数据量进行检查点操作。检查点也可能包含应用程序级的校验和。读取者仅验证和处理直到最后一个检查点的文件区域，该区域已知处于已定义状态。无论一致性和并发问题如何，这种方法都很好地服务了我们。追加比随机写入更高效，且对应用程序故障更具弹性。检查点允许写入者增量重启，并防止读取者处理从应用程序角度看仍不完整的已成功写入的文件数据。
* **在另一种典型用法中**，许多写入者并发地向一个文件追加数据，用于合并结果或作为生产者-消费者队列。**记录追加**的“至少一次”语义保留了每个写入者的输出。读取者处理偶尔出现的填充和重复数据如下：写入者准备的每条记录都包含额外的信息（如校验和），以便验证其有效性。读取者可以使用校验和识别并丢弃额外的填充和记录片段。如果它不能容忍偶尔的重复（例如，如果它们会触发非幂等操作），它可以使用记录中的唯一标识符过滤掉它们，这些标识符通常无论如何都是命名相应应用程序实体（如 Web 文档）所需要的。

这些用于记录 I/O 的功能（除重复数据删除外）位于我们应用程序共享的库代码中，并适用于 Google 的其他文件接口实现。有了这些，相同的记录序列，加上罕见的重复，总是能被交付给记录读取者。

## 3. 系统交互

我们设计该系统的初衷是尽量减少 Master 在所有操作中的参与。在这个背景下，我们现在描述客户端（Client）、Master 和 Chunkserver 如何交互以实现数据变更、原子记录追加和快照。

### 3.1 租约和变更顺序 (Leases and Mutation Order)

**变更（Mutation）** 是改变块的内容或元数据的操作，例如写入或追加操作。每个变更都要在块的所有副本上执行。我们使用**租约（Leases）** 来在副本之间维持一致的变更顺序。Master 将块租约授予其中一个副本，我们称之为**主副本（Primary）**。主副本为对该块的所有变更选择一个序列顺序。所有副本在应用变更时都遵循这个顺序。因此，全局变更顺序首先由 Master 选择的租约授予顺序定义，而在一个租约内，则由主副本分配的序列号定义。

租约机制旨在最小化 Master 的管理开销。租约的初始超时时间为 60 秒。然而，只要块正在被修改，主副本就可以向 Master 请求并通常会无限期地获得延期。这些延期请求和批准是**捎带（piggybacked）** 在 Master 和所有 Chunkserver 之间定期交换的心跳（HeartBeat）消息中的。Master 有时可能会试图在租约过期前撤销它（例如，当 Master 想要禁用正在被重命名的文件的变更时）。即使 Master 与主副本失去联系，它也可以在旧租约过期后安全地向另一个副本授予新租约。

在图 2（注：指原文插图）中，我们通过跟踪一个写入操作的控制流，用以下编号的步骤来说明这个过程：

1. **客户端询问 Master** 哪个 Chunkserver 持有该块的当前租约，以及其他副本的位置。如果没有人持有租约，Master 会将其授予它选择的一个副本（未显示）。
2. **Master 回复** 主副本的标识以及其他（从/次级）副本的位置。客户端将此数据缓存以供未来的变更使用。只有当主副本变得不可达或回复说它不再持有租约时，客户端才需要再次联系 Master。
3. **客户端将数据推送** 到所有副本。客户端可以按任意顺序推送。每个 Chunkserver 会将数据存储在内部的 LRU 缓冲区缓存中，直到数据被使用或过期。通过将**数据流**与**控制流**解耦，我们可以根据网络拓扑结构来调度昂贵的数据流从而提高性能，而不管哪个 Chunkserver 是主副本。3.2 节将进一步讨论这一点。
4. 一旦所有副本都确认收到了数据，**客户端向主副本发送写请求**。该请求标识了早先推送到所有副本的数据。主副本为它收到的所有变更（可能来自多个客户端）分配连续的序列号，这提供了必要的序列化。它按序列号顺序将变更应用到其自己的本地状态。
5. **主副本将写请求转发** 给所有从副本（Secondary Replicas）。每个从副本按主副本分配的相同序列号顺序应用变更。
6. **从副本全部回复主副本**，表明它们已完成操作。
7. **主副本回复客户端**。在任何副本上遇到的任何错误都会报告给客户端。如果出现错误，写入可能在主副本和从副本的任意子集上成功了。（如果它在主副本上失败了，它就不会被分配序列号并被转发。）客户端请求被视为失败，修改后的区域处于不一致状态。我们的客户端代码通过重试失败的变更来处理此类错误。它会在回退到从写入的开头进行重试之前，在步骤 (3) 到 (7) 之间进行几次尝试。

如果应用程序的写入量很大或跨越了块边界，GFS 客户端代码会将其分解为多个写入操作。它们都遵循上述的控制流，但可能会与来自其他客户端的并发操作交错并被覆盖。因此，共享的文件区域最终可能包含来自不同客户端的片段，尽管副本将是相同的，因为单个操作在所有副本上都是按相同顺序成功完成的。这使得文件区域处于 2.7 节所述的**一致但未定义**的状态。

### 3.2 数据流 (Data Flow)

**控制流 (Control)**
**数据 (Data)**

为了高效地利用网络，我们将**数据流**与**控制流**解耦。虽然控制流从客户端流向主副本（Primary），然后再流向所有从副本（Secondaries），但数据是以**流水线（pipelined）**的方式沿着精心挑选的 Chunkserver 链线性推送的。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟链路，并最小化推送所有数据的延迟。

为了充分利用每台机器的网络带宽，数据是沿着 Chunkserver 链线性推送的，而不是以其他某种拓扑结构（例如树形）进行分发。因此，每台机器的全部出站带宽都用于尽快传输数据，而不是在多个接收者之间分配。

为了尽可能避免网络瓶颈和高延迟链路（例如，交换机之间的链路通常两者兼具），每台机器将数据转发给网络拓扑中尚未收到数据且“最近”的机器。假设客户端正在向 Chunkserver S1 到 S4 推送数据。它将数据发送给最近的 Chunkserver，比如 S1。S1 将其转发给 S2 到 S4 中离 S1 最近的 Chunkserver，比如 S2。同样，S2 将其转发给 S3 或 S4 中离 S2 更近的那个，依此类推。我们的网络拓扑结构足够简单，可以通过 IP 地址准确估计“距离”。

最后，我们通过 TCP 连接上的数据传输**流水线化（pipelining）**来最小化延迟。一旦 Chunkserver 收到一些数据，它就会立即开始转发。流水线化对我们特别有帮助，因为我们使用具有全双工链路的交换式网络。立即发送数据不会降低接收速率。在没有网络拥塞的情况下，将 B 字节传输到 R 个副本的理想耗时是 ( B/T + RL )，其中 ( T ) 是网络吞吐量，( L ) 是两台机器之间传输字节的延迟。我们的网络链路通常是 100 Mbps (( T ))，而 ( L ) 远低于 1 毫秒。因此，1 MB 的数据理想情况下可以在大约 80 毫秒内分发完毕。

### 3.3 原子记录追加 (Atomic Record Appends)

GFS 提供了一种称为**记录追加（record append）**的原子追加操作。在传统的写入中，客户端指定数据要写入的偏移量。对同一区域的并发写入是不可序列化的：该区域最终可能包含来自多个客户端的数据片段。然而，在记录追加中，客户端仅指定数据。GFS 保证至少一次**原子地**（即作为一个连续的字节序列）将数据追加到文件中，偏移量由 GFS 选择，并将该偏移量返回给客户端。这类似于在 Unix 中以 `O_APPEND` 模式打开文件进行写入，但没有多个写入者并发操作时的竞争条件。

记录追加被我们的分布式应用程序大量使用，在这些应用中，不同机器上的许多客户端并发地向同一个文件追加数据。如果客户端使用传统的写入方式这样做，它们将需要额外复杂且昂贵的同步，例如通过分布式锁管理器。在我们的工作负载中，此类文件通常用作多生产者/单消费者队列，或包含来自许多不同客户端的合并结果。

记录追加是一种变更操作，遵循 3.1 节中的控制流，只是在主副本上多了一点额外的逻辑。客户端将数据推送到文件最后一个块的所有副本。然后，它向主副本发送请求。主副本检查将记录追加到当前块是否会导致该块超过最大尺寸（64 MB）。如果是，它会将该块**填充（pads）**到最大尺寸，告诉从副本做同样的事情，并回复客户端指示该操作应在下一个块上重试。（记录追加被限制为最大块尺寸的四分之一，以保持最坏情况下的碎片在可接受的水平。）如果记录适合放入最大尺寸内（这是常见情况），主副本将数据追加到其副本，告诉从副本在它写入的确切偏移量处写入数据，最后向客户端回复成功。

如果记录追加在任何副本上失败，客户端将重试该操作。结果是，同一个块的副本可能包含不同的数据，可能包括同一记录的整体或部分的重复。GFS 不保证所有副本在字节级别上完全相同（bytewise identical）。它只保证数据作为一个原子单元被至少写入一次。这个属性很容易从一个简单的观察中得出：如果操作报告成功，数据必须已经在某个块的所有副本上的相同偏移量处被写入。此外，在此之后，所有副本的长度至少都达到了记录的末尾，因此任何未来的记录都会被分配一个更高的偏移量或一个不同的块，即使后来不同的副本成为了主副本。就我们的一致性保证而言，成功的记录追加操作写入数据的区域是**已定义的**（因此是一致的），而中间的区域是**不一致的**（因此是未定义的）。我们的应用程序可以处理不一致的区域，正如我们在 2.7.2 节中所讨论的那样。

### 3.4 快照 (Snapshot)

快照操作几乎瞬间完成文件或目录树（“源”）的副本制作，同时尽量减少对正在进行的变更的干扰。我们的用户使用它来快速创建巨大数据集的分支副本（通常是对那些副本的副本，递归地进行），或者在试验那些稍后可以轻松提交或回滚的更改之前，对当前状态进行检查点操作。

像 AFS [5] 一样，我们使用标准的**写时复制（copy-on-write）**技术来实现快照。当 Master 收到快照请求时，它首先撤销它即将进行快照的文件中块的任何未过期的租约（outstanding leases）。这确保了随后对这些块的任何写入都需要与 Master 交互以找到租约持有者。这将给 Master 一个机会先创建块的新副本。

在租约被撤销或过期后，Master 将该操作记录到磁盘日志中。然后，它通过复制源文件或目录树的元数据，将此日志记录应用到其内存状态中。新创建的快照文件指向与源文件相同的块。

在快照操作之后，当客户端第一次想要写入块 ( C ) 时，它向 Master 发送请求以查找当前的租约持有者。Master 注意到块 ( C ) 的引用计数大于 1。它推迟回复客户端请求，而是选择一个新的块句柄 ( C' )。然后，它要求每个拥有 ( C ) 当前副本的 Chunkserver 创建一个名为 ( C' ) 的新块。通过在与原始块相同的 Chunkserver 上创建新块，我们要确保数据可以在本地复制，而不是通过网络（我们的磁盘速度大约是我们 100 Mb 以太网链路的三倍）。从这时起，请求处理与任何块没有什么不同：Master 授予其中一个副本对新块 ( C' ) 的租约并回复客户端，客户端可以正常写入该块，而不知道它刚刚是从现有的块创建出来的。

## 4. Master 操作 (Master Operation)

Master 执行所有的命名空间操作。此外，它管理整个系统中的块副本：它做出放置决策，创建新的块及其副本，并协调各种系统范围内的活动，以保持块的充分复制、平衡所有 Chunkserver 的负载并回收未使用的存储空间。我们现在讨论每一个主题。

### 4.1 命名空间管理和锁定 (Namespace Management and Locking)

许多 Master 操作可能需要很长时间：例如，快照操作必须撤销快照所涵盖的所有块上的 Chunkserver 租约。我们不希望在这些操作运行时延迟其他的 Master 操作。因此，我们允许多个操作处于活动状态，并在命名空间的区域上使用锁来确保正确的序列化。

与许多传统文件系统不同，GFS 没有用于列出目录下所有文件的“每目录（per-directory）”数据结构。它也不支持同一文件或目录的别名（即 Unix 术语中的硬链接或符号链接）。GFS 在逻辑上将其命名空间表示为一个将全路径名映射到元数据的查找表。通过前缀压缩，这个表可以高效地在内存中表示。命名空间树中的每个节点（无论是绝对文件名还是绝对目录名）都有一个关联的读写锁（Read-Write Lock）。

每个 Master 操作在运行前都要获取一组锁。通常，如果操作涉及 `/d1/d2/.../dn/leaf`，它将获取目录名 `/d1`，`/d1/d2`，...，`/d1/d2/.../dn` 上的**读锁（Read Locks）**，以及全路径名 `/d1/d2/.../dn/leaf` 上的**读锁或写锁（Write Lock）**。请注意，根据操作的不同，`leaf` 可能是一个文件或一个目录。

我们现在说明这种锁定机制如何防止在 `/home/user` 正在被快照到 `/save/user` 时创建文件 `/home/user/foo`。

* **快照操作**获取 `/home` 和 `/save` 上的**读锁**，以及 `/home/user` 和 `/save/user` 上的**写锁**。
* **文件创建**获取 `/home` 和 `/home/user` 上的**读锁**，以及 `/home/user/foo` 上的**写锁**。

这两个操作将被正确地序列化，因为它们试图获取 `/home/user` 上的冲突锁（快照需要写锁，创建文件需要读锁）。文件创建不需要父目录上的写锁，因为没有“目录”或类似 inode 的数据结构需要保护以免被修改。名称上的读锁足以保护父目录不被删除。

这种锁定方案的一个优点是它允许在同一目录中进行并发变更。例如，可以在同一目录中并发执行多个文件创建：每个创建操作都获取目录名上的读锁和文件名上的写锁。目录名上的读锁足以防止目录被删除、重命名或快照。文件名上的写锁将创建同名文件的尝试序列化。

由于命名空间可能有许多节点，读写锁对象是惰性分配的（lazily allocated），一旦不再使用就会被删除。此外，锁是按一致的全序获取的以防止死锁：首先按命名空间树中的层级排序，在同一层级内按字典序排序。

### 4.2 副本放置 (Replica Placement)

一个 GFS 集群在多个层面上都是高度分布式的。它通常拥有分布在许多机器机架（Racks）上的数百个 Chunkserver。这些 Chunkserver 可能会被来自同一或不同机架的数百个客户端访问。不同机架上的两台机器之间的通信可能跨越一个或多个网络交换机。此外，进出机架的带宽可能小于机架内所有机器的聚合带宽。多级分布为分发数据以实现可扩展性、可靠性和可用性带来了独特的挑战。

块副本放置策略服务于两个目的：**最大化数据可靠性和可用性**，以及**最大化网络带宽利用率**。为了这两个目的，仅仅将副本分散到不同的机器上是不够的，这只能防范磁盘或机器故障，并充分利用每台机器的网络带宽。我们还必须**将块副本分散到不同的机架上**。这确保了即使整个机架损坏或离线（例如，由于网络交换机或电源电路等共享资源的故障），块的某些副本仍能存活并保持可用。这也意味着，针对一个块的流量，特别是读取流量，可以利用多个机架的聚合带宽。另一方面，写入流量必须流经多个机架，这是我们愿意做出的权衡。

### 4.3 创建，重新复制，再平衡 (Creation, Re-replication, Rebalancing)

创建块副本有三个原因：块创建、重新复制和再平衡。

当 Master **创建**一个块时，它选择在哪里放置初始的空副本。它考虑几个因素：

1. 我们希望将新副本放置在磁盘空间利用率低于平均水平的 Chunkserver 上。随着时间的推移，这将平衡各 Chunkserver 的磁盘利用率。
2. 我们希望限制每个 Chunkserver 上“最近”创建的数量。虽然创建本身很便宜，但它可靠地预示了即将到来的大量写入流量，因为块是在写入需要时创建的，而在我们“一次追加，多次读取”的工作负载中，它们一旦完全写入通常实际上就变成了只读的。
3. 如上所述，我们希望将块的副本分散到不同的机架上。

一旦可用副本的数量低于用户指定的目标，Master 就会**重新复制（Re-replicates）**一个块。这可能由各种原因发生：Chunkserver 变得不可用，它报告其副本可能已损坏，其磁盘之一因错误而被禁用，或者复制目标被增加了。每个需要重新复制的块都根据几个因素确定优先级。

* 一个是它离复制目标有多远。例如，我们给丢失了两个副本的块比只丢失了一个副本的块更高的优先级。
* 此外，我们优先重新复制即时（live）文件的块，而不是最近删除的文件的块（见 4.4 节）。
* 最后，为了尽量减少故障对运行中应用程序的影响，我们提高任何正在阻塞客户端进度的块的优先级。

Master 选择优先级最高的块，并通过指示某个 Chunkserver 直接从现有的有效副本复制块数据来“克隆”它。新副本的放置目标与创建时类似：平衡磁盘空间利用率，限制任何单个 Chunkserver 上的活动克隆操作，并将副本分散到各机架。为了防止克隆流量压倒客户端流量，Master 限制了集群和每个 Chunkserver 的活动克隆操作数量。此外，每个 Chunkserver 通过限制其对源 Chunkserver 的读取请求来限制其在每个克隆操作上花费的带宽。

最后，Master 定期**再平衡（Rebalances）**副本：它检查当前的副本分布，并为了更好的磁盘空间和负载平衡而移动副本。同样通过这个过程，Master 逐渐填满一个新的 Chunkserver，而不是瞬间用新块和随之而来的大量写入流量淹没它。新副本的放置标准与上面讨论的类似。此外，Master 还必须选择删除哪个现有副本。通常，它倾向于删除那些空闲空间低于平均水平的 Chunkserver 上的副本，以平衡磁盘空间使用。

### 4.4 垃圾回收 (Garbage Collection)

在文件被删除后，GFS 不会立即回收可用的物理存储空间。它仅在文件和块级别的常规垃圾回收期间**惰性地（lazily）**这样做。我们发现这种方法使系统更简单、更可靠。

#### 4.4.1 机制 (Mechanism)

当应用程序删除一个文件时，Master 会像记录其他变更一样立即记录该删除操作。然而，它不会立即回收资源，而是将文件重命名为一个包含删除时间戳的**隐藏名称**。在 Master 定期扫描文件系统命名空间期间，如果此类隐藏文件存在超过三天（该间隔可配置），它将删除这些文件。在此之前，该文件仍然可以通过新的特殊名称被读取，并且可以通过将其重命名回正常名称来**撤销删除（undeleted）**。当隐藏文件从命名空间中被移除时，其内存中的元数据即被擦除。这有效地切断了它与其所有块的链接。

在类似的块命名空间定期扫描中，Master 会识别**孤儿块（orphaned chunks）**（即那些无法从任何文件访问到的块）并擦除这些块的元数据。在与 Master 定期交换的心跳（HeartBeat）消息中，每个 Chunkserver 会报告它拥有的块的一个子集，Master 会回复那些在 Master 元数据中不再存在的块的标识。Chunkserver 可以自由删除这些块的副本。

#### 4.4.2 讨论 (Discussion)

虽然在编程语言的背景下，分布式垃圾回收是一个需要复杂解决方案的难题，但在我们的案例中它相当简单。我们可以轻松识别对块的所有引用：它们都在由 Master 专门维护的文件到块的映射中。我们也可以轻松识别所有的块副本：它们是每个 Chunkserver 上指定目录下的 Linux 文件。任何 Master 不知道的此类副本都是“垃圾”。

与**急切删除（eager deletion）**相比，垃圾回收式的存储回收方法提供了几个优势：

1. **第一**，在大规模分布式系统中，组件故障很常见，这种方法简单且可靠。块创建可能在某些 Chunkserver 上成功但在其他上失败，留下 Master 不知道存在的副本。副本删除消息可能会丢失，Master 必须记住在故障（无论是它自己的还是 Chunkserver 的）之后重新发送它们。垃圾回收提供了一种统一且可靠的方式来清理任何已知不再有用的副本。
2. **第二**，它将存储回收合并到 Master 的常规后台活动中，例如命名空间的定期扫描和与 Chunkserver 的握手。因此，它是分批完成的，成本被分摊了。此外，它仅在 Master 相对空闲时才进行。Master 可以更迅速地响应需要及时关注的客户端请求。
3. **第三**，回收存储的延迟提供了防止意外、不可逆删除的安全网。

根据我们的经验，主要的缺点是延迟有时会阻碍用户在存储紧张时微调使用的努力。重复创建和删除临时文件的应用程序可能无法立即重用存储空间。我们通过在已删除文件被显式再次删除时加快存储回收来解决这些问题。我们还允许用户对命名空间的不同部分应用不同的复制和回收策略。例如，用户可以指定某个目录树内的文件中的所有块都不进行复制存储，并且任何删除的文件都会立即且不可撤销地从文件系统状态中移除。

### 4.5 陈旧副本检测 (Stale Replica Detection)

如果 Chunkserver 发生故障并在其停机期间错过了对块的变更，块副本可能会变得**陈旧（stale）**。对于每个块，Master 维护一个**块版本号（chunk version number）**以区分最新副本和陈旧副本。

每当 Master 授予一个块的新租约时，它都会增加块版本号并通知最新的副本。Master 和这些副本都会在其持久状态中记录新的版本号。这发生在任何客户端被通知之前，因此也发生在它开始写入块之前。如果另一个副本当前不可用，其块版本号将不会被推进。当该 Chunkserver 重启并报告其块集合及关联的版本号时，Master 将检测到该 Chunkserver 拥有一个陈旧的副本。如果 Master 看到的版本号大于其记录中的版本号，Master 会假设它在授予租约时失败了，因此将较高的版本号视为最新的。

Master 在其常规垃圾回收中移除陈旧副本。在此之前，当回复客户端对块信息的请求时，它实际上认为陈旧副本根本不存在。作为另一种保障，当 Master 通知客户端哪个 Chunkserver 持有块的租约，或者指示 Chunkserver 在克隆操作中从另一个 Chunkserver 读取块时，Master 会包含块版本号。客户端或 Chunkserver 在执行操作时会验证版本号，以确保它始终访问的是最新数据。

## 5. 容错与诊断 (Fault Tolerance and Diagnosis)

在设计该系统时，我们面临的最大挑战之一是如何处理频繁的组件故障。组件的质量和数量共同使得这些问题更多地成为一种常态而非例外：我们既不能完全信任机器，也不能完全信任磁盘。组件故障可能导致系统不可用，或者更糟糕的是，导致数据损坏。我们讨论如何应对这些挑战，以及我们在系统中内置了哪些工具来在问题不可避免地发生时进行诊断。

### 5.1 高可用性 (High Availability)

在 GFS 集群的数百台服务器中，任何给定的时间都有一些必然是不可用的。我们通过两个简单但有效的策略保持整个系统的高可用性：**快速恢复**和**复制**。

#### 5.1.1 快速恢复 (Fast Recovery)

Master 和 Chunkserver 都被设计为无论它们如何终止，都能在几秒钟内恢复其状态并启动。实际上，我们不区分正常终止和异常终止；服务器通常就是通过杀死进程来进行常规关闭的。客户端和其他服务器会经历一次小的停顿，因为它们对未完成的请求超时，重新连接到重启后的服务器，然后重试。6.2.2 节报告了观察到的启动时间。

#### 5.1.2 块复制 (Chunk Replication)

如前所述，每个块在不同机架的多个 Chunkserver 上进行复制。用户可以为文件命名空间的不同部分指定不同的复制级别。默认为三。当 Chunkserver 离线或通过校验和验证检测到损坏的副本时（见 5.2 节），Master 会根据需要克隆现有的副本，以保持每个块被充分复制。虽然复制已经很好地为我们服务，但我们正在探索其他形式的跨服务器冗余，如奇偶校验或纠删码，以满足我们日益增长的只读存储需求。我们预计在我们这个非常松散耦合的系统中实现这些更复杂的冗余方案是具有挑战性但可管理的，因为我们的流量主要是追加和读取，而不是小规模的随机写入。

#### 5.1.3 Master 复制 (Master Replication)

Master 状态也进行复制以保证可靠性。其操作日志和检查点被复制到多台机器上。对状态的变更只有在其日志记录被刷新到本地以及所有 Master 副本的磁盘后，才被认为是提交了。为了简单起见，一个 Master 进程仍负责所有的变更以及诸如垃圾回收等改变系统内部状态的后台活动。当它发生故障时，它可以几乎瞬间重启。如果其机器或磁盘发生故障，GFS 外部的监控基础设施会在其他地方启动一个新的 Master 进程，并使用复制的操作日志。客户端仅使用 Master 的规范名称（例如 `gfs-test`），这是一个 DNS 别名，如果 Master 被重新安置到另一台机器，该别名可以更改。

此外，**“影子” Master (Shadow Masters)** 即使在主 Master 宕机时也提供对文件系统的只读访问。它们是影子，而不是镜像，因为它们可能会稍微落后于主 Master，通常是几分之一秒。它们增强了那些未被频繁修改的文件或不介意获得稍微陈旧结果的应用程序的读取可用性。实际上，由于文件内容是从 Chunkserver 读取的，应用程序不会观察到陈旧的文件内容。在短时间内可能陈旧的是文件元数据，如目录内容或访问控制信息。

为了保持自身信息的更新，影子 Master 读取不断增长的操作日志的副本，并完全像主 Master 一样对其数据结构应用相同的变更序列。像主 Master 一样，它在启动时（以及之后不频繁地）轮询 Chunkserver 以定位块副本，并与它们频繁交换握手消息以监控其状态。它仅在因主 Master 决定创建和删除副本而导致的副本位置更新方面依赖于主 Master。

### 5.2 数据完整性 (Data Integrity)

每个 Chunkserver 使用**校验和 (Checksumming)** 来检测存储数据的损坏。鉴于一个 GFS 集群通常在数百台机器上拥有数千个磁盘，它经常经历导致读写路径上数据损坏或丢失的磁盘故障。（见第 7 节的一个原因。）我们可以利用其他块副本从损坏中恢复，但通过跨 Chunkserver 比较副本来检测损坏是不切实际的。此外，不一致的副本可能是合法的：GFS 变更的语义，特别是前面讨论的原子记录追加，并不保证副本完全相同。因此，每个 Chunkserver 必须通过维护校验和来独立验证其自身副本的完整性。

一个块被分解为 64 KB 的块（Block）。每个块都有一个对应的 32 位校验和。与其他元数据一样，校验和保存在内存中，并与日志一起持久存储，与用户数据分开。

对于读取，Chunkserver 在向请求者（无论是客户端还是另一个 Chunkserver）返回任何数据之前，会验证与读取范围重叠的数据块的校验和。因此，Chunkserver 不会将损坏传播到其他机器。如果一个块与记录的校验和不匹配，Chunkserver 会向请求者返回错误，并向 Master 报告不匹配。作为响应，请求者将从其他副本读取，而 Master 将从另一个副本克隆该块。在有效的新副本就位后，Master 指示报告不匹配的 Chunkserver 删除其副本。

校验和对读取性能几乎没有影响，原因有几个。由于我们的大多数读取至少跨越几个块，我们只需要读取和校验相对少量的额外数据用于验证。GFS 客户端代码通过尝试在校验和块边界处对齐读取来进一步减少这种开销。此外，Chunkserver 上的校验和查找和比较是在没有任何 I/O 的情况下完成的，并且校验和计算通常可以与 I/O 重叠。

校验和计算针对追加到块末尾的写入（相对于覆盖现有数据的写入）进行了大量优化，因为这在我们的工作负载中占主导地位。我们只是增量更新最后一个部分校验和块的校验和，并为追加填充的任何全新校验和块计算新的校验和。即使最后一个部分校验和块已经损坏而我们现在未能检测到，新的校验和值也将与存储的数据不匹配，并且当该块下次被读取时，损坏将照常被检测到。

相比之下，如果写入覆盖了块的现有范围，我们必须读取并验证被覆盖范围的第一个和最后一个块，然后执行写入，最后计算并记录新的校验和。如果我们在部分覆盖它们之前不验证第一个和最后一个块，新的校验和可能会掩盖未被覆盖区域中存在的损坏。

在空闲期间，Chunkserver 可以扫描并验证不活跃块的内容。这允许我们检测很少被读取的块中的损坏。一旦检测到损坏，Master 可以创建一个新的未损坏副本并删除损坏的副本。这防止了不活跃但已损坏的块副本欺骗 Master，使其认为它拥有足够多的有效块副本。

### 5.3 诊断工具 (Diagnostic Tools)

广泛而详细的诊断日志在问题隔离、调试和性能分析方面提供了无法估量的帮助，同时只产生了极小的成本。没有日志，很难理解机器之间瞬时的、不可重复的交互。GFS 服务器生成诊断日志，记录许多重要事件（如 Chunkserver 的上线和下线）以及所有的 RPC 请求和回复。这些诊断日志可以自由删除，而不会影响系统的正确性。然而，我们会尽可能在空间允许的情况下保留这些日志。

RPC 日志包括线路上发送的确切请求和响应，除了被读写的文件数据。通过匹配请求与回复并整理不同机器上的 RPC 记录，我们可以重建整个交互历史来诊断问题。日志还作为负载测试和性能分析的跟踪记录。

日志记录对性能的影响极小（且带来的好处远超成本），因为这些日志是顺序且异步写入的。最近的事件也保存在内存中，可用于持续的在线监控。

## 6. 测量 (Measurements)

在本节中，我们将展示一些微基准测试（micro-benchmarks），以说明 GFS 架构和实现中固有的瓶颈，同时也展示来自 Google 正在使用的真实集群的一些数据。

### 6.1 微基准测试 (Micro-benchmarks)

我们在一个由 1 个 Master、2 个 Master 副本、16 个 Chunkserver 和 16 个客户端组成的 GFS 集群上测量了性能。请注意，这种配置是为了方便测试而设置的。典型的集群有数百个 Chunkserver 和数百个客户端。

所有机器都配置了双 1.4 GHz PIII 处理器、2 GB 内存、两个 80 GB 5400 rpm 磁盘，以及连接到 HP 2524 交换机的 100 Mbps 全双工以太网连接。所有 19 台 GFS 服务器机器连接到一个交换机，所有 16 台客户端机器连接到另一个。两个交换机通过 1 Gbps 的链路连接。

#### 6.1.1 读取 (Reads)

N 个客户端同时从文件系统读取。每个客户端从一个 320 GB 的文件集中读取一个随机选择的 4 MB 区域。这重复 256 次，以便每个客户端最终读取 1 GB 的数据。Chunkserver 的总内存只有 32 GB，因此我们预计 Linux 缓冲区缓存的命中率最多为 10%。我们的结果应该接近冷缓存（cold cache）的结果。

图 3(a)（注：指原文图表）显示了 N 个客户端的聚合读取速率及其理论上限。当两个交换机之间的 1 Gbps 链路饱和时，上限峰值为 125 MB/s 的聚合速率；或者当每个客户端的 100 Mbps 网络接口饱和时，上限为每个客户端 12.5 MB/s，取两者中适用的限制。当只有一个客户端读取时，观察到的读取速率为 10 MB/s，或者是单客户端上限的 80%。对于 16 个读取者，聚合读取速率达到 94 MB/s，约为 125 MB/s 链路限制的 75%，即每个客户端 6 MB/s。效率从 80% 下降到 75%，是因为随着读取者数量的增加，多个读取者同时从同一个 Chunkserver 读取的概率也增加了。

#### 6.1.2 写入 (Writes)

N 个客户端同时向 N 个不同的文件写入。每个客户端通过一系列 1 MB 的写入向一个新文件写入 1 GB 的数据。聚合写入速率及其理论上限如图 3(b) 所示。上限在 67 MB/s 处趋于平稳，因为我们需要将每个字节写入 16 个 Chunkserver 中的 3 个，每个 Chunkserver 都有 12.5 MB/s 的输入连接。

一个客户端的写入速率为 6.3 MB/s，大约是上限的一半。造成这种情况的主要罪魁祸首是我们的网络协议栈。它与我们将数据推送到块副本所使用的流水线方案互动得不是很好。数据从一个副本传播到另一个副本的延迟降低了整体写入速率。

对于 16 个客户端，聚合写入速率达到 35 MB/s（或每个客户端 2.2 MB/s），大约是理论上限的一半。与读取的情况一样，随着客户端数量的增加，多个客户端并发写入同一个 Chunkserver 的可能性也变大了。此外，对于 16 个写入者来说，碰撞的可能性比 16 个读取者更大，因为每个写入涉及三个不同的副本。

写入速度比我们希望的要慢。在实践中，这并不是一个主要问题，因为即使它增加了单个客户端所看到的延迟，它也不会显着影响系统向大量客户端提供的聚合写入带宽。

#### 6.1.3 记录追加 (Record Appends)

图 3(c) 显示了记录追加的性能。N 个客户端同时向单个文件追加数据。性能受限于存储文件最后一个块的 Chunkserver 的网络带宽，而与客户端的数量无关。对于一个客户端，它从 6.0 MB/s 开始，对于 16 个客户端下降到 4.8 MB/s，这主要是由于拥塞和不同客户端看到的网络传输速率的差异。

我们的应用程序倾向于并发生成多个这样的文件。换句话说，N 个客户端同时向 M 个共享文件追加数据，其中 N 和 M 都在几十或几百的数量级。因此，我们实验中的 Chunkserver 网络拥塞在实践中并不是一个严重的问题，因为一个客户端可以在写入一个文件时取得进展，而另一个文件的 Chunkserver 即使正忙也没关系。

### 6.2 真实世界集群 (Real World Clusters)

我们现在检查 Google 内部正在使用的两个集群，它们代表了其他几个类似的集群。集群 A 经常被一百多名工程师用于研发。典型的任务由人类用户发起，运行时间长达数小时。它读取几 MB 到几 TB 的数据，转换或分析数据，并将结果写回集群。集群 B 主要用于生产数据处理。任务持续时间更长，并且持续生成和处理数 TB 的数据集，仅偶尔有人工干预。在这两种情况下，单个“任务”都由许多机器上的许多进程组成，同时读取和写入许多文件。

#### 6.2.1 存储 (Storage)

如表中前五项所示，两个集群都有数百个 Chunkserver，支持数 TB 的磁盘空间，并且相当满但也未完全满。“已用空间”包括所有块副本。实际上所有文件都复制了三次。因此，集群分别存储了 18 TB 和 52 TB 的文件数据。

这两个集群的文件数量相似，尽管 B 有更大比例的死文件，即被删除或被新版本替换但其存储尚未被回收的文件。它也有更多的块，因为它的文件往往更大。

#### 6.2.2 元数据 (Metadata)

Chunkserver 总共存储了数十 GB 的元数据，主要是用户数据 64 KB 块的校验和。保存在 Chunkserver 上的唯一其他元数据是 4.5 节中讨论的块版本号。

保存在 Master 上的元数据要小得多，只有数十 MB，或者平均每个文件大约 100 字节。这与我们的假设一致，即 Master 的内存大小在实践中并不限制系统的容量。大多数每个文件的元数据是以前缀压缩形式存储的文件名。其他元数据包括文件所有权和权限、从文件到块的映射以及每个块的当前版本。此外，对于每个块，我们存储当前的副本位置和一个用于实现写时复制的引用计数。

每个单独的服务器，无论是 Chunkserver 还是 Master，都只有 50 到 100 MB 的元数据。因此恢复很快：在服务器能够回答查询之前，只需几秒钟即可从磁盘读取这些元数据。然而，Master 在一段时间内（通常是 30 到 60 秒）会有些受限，直到它从所有 Chunkserver 获取到块位置信息。

#### 6.2.3 读写速率 (Read and Write Rates)

表 3 显示了不同时间段的读写速率。进行这些测量时，两个集群都已经运行了大约一周。（集群最近刚重启以升级到新版本的 GFS。）

自重启以来，平均写入速率低于 30 MB/s。当我们进行这些测量时，B 正处于一阵写入活动中，产生约 100 MB/s 的数据，这产生了 300 MB/s 的网络负载，因为写入被传播到三个副本。

读取速率远高于写入速率。正如我们假设的那样，总工作负载包含的读取多于写入。两个集群都处于繁重的读取活动中。特别是，A 在过去一周一直保持着 580 MB/s 的读取速率。其网络配置可以支持 750 MB/s，因此它正在有效地利用其资源。集群 B 可以支持 1300 MB/s 的峰值读取速率，但其应用程序仅使用了 380 MB/s。

#### 6.2.4 Master 负载 (Master Load)

表 3（注：指原文图表）还显示，发送到 Master 的操作速率约为每秒 200 到 500 个操作。Master 可以轻松跟上这个速率，因此它不是这些工作负载的瓶颈。

在 GFS 的早期版本中，Master 偶尔会成为某些工作负载的瓶颈。它把大部分时间花在顺序扫描大型目录（包含数十万个文件）以查找特定文件上。此后，我们更改了 Master 的数据结构，允许在命名空间中进行高效的二分搜索。现在，它可以轻松支持每秒数千次文件访问。如果有必要，我们可以通过在命名空间数据结构前放置名称查找缓存来进一步加快速度。

#### 6.2.5 恢复时间 (Recovery Time)

在一个 Chunkserver 发生故障后，一些块将变得副本不足（under-replicated），必须进行克隆以恢复其复制级别。恢复所有这些块所需的时间取决于资源的数量。在一个实验中，我们在集群 B 中杀死了一个 Chunkserver。该 Chunkserver 拥有约 15,000 个块，包含 600 GB 数据。为了限制对运行中应用程序的影响并为调度决策提供余地，我们的默认参数将该集群限制为 91 个并发克隆（Chunkserver 数量的 40%），其中每个克隆操作最多允许消耗 6.25 MB/s (50 Mbps)。所有块在 23.2 分钟内恢复，有效复制速率为 440 MB/s。

在另一个实验中，我们杀死了两个 Chunkserver，每个大约有 16,000 个块和 660 GB 数据。这种双重故障导致 266 个块只剩下一个副本。这 266 个块被以更高的优先级进行克隆，并在 2 分钟内全部恢复到至少 2 倍的复制，从而使集群处于可以容忍另一个 Chunkserver 故障而不会丢失数据的状态。

### 6.3 工作负载细分 (Workload Breakdown)

在本节中，我们将详细介绍两个 GFS 集群的工作负载细分，这两个集群与 6.2 节中的集群具有可比性但不完全相同。集群 X 用于研发，而集群 Y 用于生产数据处理。

#### 6.3.1 方法论和注意事项 (Methodology and Caveats)

这些结果仅包括**客户端发起的请求**，以便它们反映我们的应用程序为整个文件系统生成的工作负载。它们不包括用于执行客户端请求或内部后台活动（如转发写入或再平衡）的服务器间请求。

I/O 操作的统计数据是基于从 GFS 服务器记录的实际 RPC 请求中启发式重建的信息。例如，GFS 客户端代码可能会将一个读取操作分解为多个 RPC 以增加并行度，我们从中推断出原始读取。由于我们的访问模式是高度程式化的，我们预计任何误差都在噪音范围内。应用程序的显式日志记录可能会提供稍微更准确的数据，但在后勤上不可能为此重新编译和重启数千个运行中的客户端，而且从那么多机器上收集结果也很麻烦。

人们应该小心不要从我们的工作负载中过度概括。由于 Google 完全控制 GFS 及其应用程序，应用程序倾向于针对 GFS 进行调优，反之，GFS 也是为这些应用程序设计的。这种相互影响可能也存在于通用应用程序和文件系统之间，但在我们的案例中这种影响可能更为显著。

#### 6.3.2 Chunkserver 工作负载 (Chunkserver Workload)

表 4 显示了按大小分布的操作。**读取大小呈现双峰分布（bimodal distribution）。** 小规模读取（小于 64 KB）来自搜索密集型客户端，它们在巨大文件中查找小块数据。大规模读取（超过 512 KB）来自遍历整个文件的长顺序读取。

在集群 Y 中，**大量的读取根本不返回任何数据**。我们的应用程序，尤其是生产系统中的应用程序，经常将文件用作生产者-消费者队列。生产者并发地向文件追加数据，而消费者读取文件末尾。当消费者超过生产者时，偶尔不会返回任何数据。集群 X 显示这种情况较少，因为它通常用于短期的包含数据分析的任务，而不是长期的分布式应用程序。

**写入大小也呈现双峰分布。** 大规模写入（超过 256 KB）通常源于写入者内部的大量缓冲。缓冲较少数据、检查点或同步更频繁，或者仅仅生成较少数据的写入者造成了较小的写入（小于 64 KB）。

至于**记录追加**，集群 Y 看到的**大记录追加**的百分比比集群 X 高得多，因为使用集群 Y 的生产系统针对 GFS 进行了更积极的调优。

表 5 显示了各种大小的操作传输的总数据量。对于所有类型的操作，较大的操作（超过 256 KB）通常占传输字节的大部分。小规模读取（小于 64 KB）确实传输了一小部分但显著的读取数据量，这是由于随机搜索工作负载造成的。

#### 6.3.3 追加与写入 (Appends versus Writes)

**记录追加被大量使用**，尤其是在我们的生产系统中。对于集群 X，写入与记录追加的比例按传输字节计算为 108:1，按操作计数计算为 8:1。对于生产系统使用的集群 Y，该比例分别为 3.7:1 和 2.5:1。此外，这些比例表明，对于两个集群，记录追加往往比写入更大。然而，对于集群 X，在测量期间记录追加的总体使用率相当低，因此结果可能因一两个具有特定缓冲区大小选择的应用程序而出现偏差。

正如预期的那样，我们的数据变更工作负载**以追加而非覆盖为主**。我们测量了主副本上被覆盖的数据量。这近似于客户端故意覆盖先前写入的数据而不是追加新数据的情况。对于集群 X，覆盖占变更字节的不到 0.0001% 和变更操作的不到 0.0003%。对于集群 Y，这两个比例均为 0.05%。虽然这很微小，但仍高于我们的预期。事实证明，这些覆盖大部分来自于因错误或超时导致的客户端重试。它们本身不是工作负载的一部分，而是重试机制的结果。

#### 6.3.4 Master 工作负载 (Master Workload)

表 6 显示了对 Master 请求类型的细分。大多数请求是查询用于读取的**块位置（FindLocation）**和用于数据变更的**租约持有者信息（FindLeaseLocker）**。

集群 X 和 Y 看到的 **Delete（删除）** 请求数量有显着差异，因为集群 Y 存储的是定期重新生成并替换为新版本的生产数据集。这种差异的一部分进一步隐藏在 **Open（打开）** 请求的差异中，因为文件的旧版本可能会通过以从头开始写入的模式（Unix open 术语中的模式 "w"）打开而被隐式删除。

**FindMatchingFiles** 是一个模式匹配请求，支持 "ls" 和类似的文件系统操作。与 Master 的其他请求不同，它可能会处理命名空间的大部分，因此可能很昂贵。集群 Y 看到它的频率要高得多，因为自动化数据处理任务倾向于检查文件系统的部分内容以了解全局应用程序状态。相比之下，集群 X 的应用程序处于更明确的用户控制之下，通常预先知道所有所需文件的名称。

## 7. 经验 (Experiences)

在构建和部署 GFS 的过程中，我们经历了各种各样的问题，有些是操作上的，有些是技术上的。

最初，GFS 被设想为我们生产系统的后端文件系统。随着时间的推移，其用途演变为包括研发任务。它起初几乎不支持权限和配额之类的功能，但现在包含了这些功能的基本形式。虽然生产系统纪律严明且受控，但用户有时并非如此。需要更多的基础设施来防止用户相互干扰。

我们遇到的一些最大的问题与磁盘和 Linux 相关。我们的许多磁盘向 Linux 驱动程序声称它们支持一系列 IDE 协议版本，但实际上只对较新的版本响应可靠。由于协议版本非常相似，这些驱动器大多时候都能工作，但偶尔的不匹配会导致驱动器和内核对驱动器状态产生分歧。由于内核中的问题，这会**默默地破坏数据**。这个问题促使我们使用**校验和**来检测数据损坏，同时我们修改了内核以处理这些协议不匹配。

早些时候，由于 `fsync()` 的成本，我们在 Linux 2.2 内核上遇到了一些问题。它的成本与文件大小成正比，而不是与修改部分的大小成正比。这对于我们的大型操作日志来说是一个问题，尤其是在我们实现检查点之前。我们在一段时间内通过使用同步写入绕过了这个问题，并最终迁移到了 Linux 2.4。

另一个 Linux 问题是一个单一的**读写锁（reader-writer lock）**，任何线程在从磁盘换入页面（读锁）或在 `mmap()` 调用中修改地址空间（写锁）时都必须持有该锁。我们在轻负载下看到了系统出现短暂的超时，并努力寻找资源瓶颈或偶发的硬件故障。最终，我们发现这个单一的锁在磁盘线程正在换入先前映射的数据时，阻止了主网络线程将新数据映射到内存中。由于我们要么主要受限于网络接口，而不是内存复制带宽，我们通过用 `pread()` 替换 `mmap()` 并付出额外复制的代价绕过了这个问题。

尽管偶尔会出现问题，但 Linux 代码的可用性一次又一次地帮助我们探索和理解系统行为。在适当的时候，我们改进内核并与开源社区分享这些更改。

## 8. 相关工作 (Related Work)

像其他大型分布式文件系统（如 AFS [5]）一样，GFS 提供了一个位置无关的命名空间，这使得数据可以为了负载平衡或容错而被透明地移动。与 AFS 不同，GFS 将文件数据分布在存储服务器上，方式更类似于 xFS [1] 和 Swift [3]，以提供聚合性能和增加容错能力。

由于磁盘相对便宜且复制比更复杂的 RAID [9] 方法更简单，GFS 目前仅使用**复制**进行冗余，因此比 xFS 或 Swift 消耗更多的原始存储空间。

与 AFS、xFS、Frangipani [12] 和 Intermezzo [6] 等系统相比，GFS **不提供文件系统接口以下的任何缓存**。我们的目标工作负载在单次应用程序运行中几乎没有重用，因为它们要么流式传输大数据集，要么在其中随机搜索并每次读取少量数据。

一些分布式文件系统，如 Frangipani、xFS、明尼苏达大学的 GFS [11] 和 GPFS [10]，移除了中心化服务器，依赖分布式算法来实现一致性和管理。我们选择**中心化方法**是为了简化设计、提高可靠性并获得灵活性。特别是，中心化 Master使得实现复杂的块放置和复制策略变得容易得多，因为 Master 已经拥有大部分相关信息并控制其变化。我们通过保持 Master 状态较小并在其他机器上完全复制它来解决容错问题。可扩展性和高可用性（针对读取）目前由我们的影子 Master 机制提供。对 Master 状态的更新通过追加到预写日志（write-ahead log）来持久化。因此，我们可以采用像 Harp [7] 中的主副本方案（primary-copy scheme），以提供比我们要么当前方案更强的一致性保证的高可用性。

我们在解决与 Lustre [8] 类似的问题，即向大量客户端提供聚合性能。然而，我们通过关注我们应用程序的需求而不是构建符合 POSIX 标准的文件系统，大大简化了这个问题。此外，GFS 假设存在大量不可靠的组件，因此**容错**是我们设计的核心。

GFS 最类似于 NASD 架构 [4]。虽然 NASD 架构基于网络附加磁盘驱动器，但 GFS 使用商用机器作为 Chunkserver，就像 NASD 原型中所做的那样。与 NASD 的工作不同，我们的 Chunkserver 使用惰性分配的固定大小块，而不是变长对象。此外，GFS 实现了诸如再平衡、复制和恢复等生产环境所需的功能。

与明尼苏达大学的 GFS 和 NASD 不同，我们不寻求改变存储设备的模型。我们专注于使用现有的商用组件解决复杂分布式系统的日常数据处理需求。

由原子记录追加启用的生产者-消费者队列解决的问题与 River [2] 中的分布式队列类似。虽然 River 使用分布在机器上的基于内存的队列和仔细的数据流控制，但 GFS 使用一个可以被许多生产者并发追加的持久文件。River 模型支持 m 对 n 的分布式队列，但缺乏持久存储带来的容错能力，而 GFS 仅能高效地支持 m 对 1 的队列。多个消费者可以读取同一个文件，但它们必须协调以划分传入的负载。

## 9. 结论 (Conclusions)

Google 文件系统展示了在商用硬件上支持大规模数据处理工作负载所必不可少的品质。虽然某些设计决策是针对我们独特的环境做出的，但许多决策可能适用于具有类似规模和成本意识的数据处理任务。

我们首先根据当前和预期的应用工作负载以及技术环境，重新审视了传统的文件系统假设。我们的观察结果导致了设计空间中截然不同的点。我们将组件故障视为常态而非例外，针对主要以追加方式写入（可能是并发的）然后读取（通常是顺序的）的巨大文件进行优化，并扩展和放宽了标准文件系统接口以改进整个系统。

我们的系统通过持续监控、复制关键数据以及快速自动恢复来提供容错能力。块复制使我们能够容忍 Chunkserver 故障。这些故障的频繁发生促使我们设计了一种新颖的在线修复机制，该机制定期且透明地修复损坏，并尽快补偿丢失的副本。此外，我们使用校验和来检测磁盘或 IDE 子系统级别的数据损坏，考虑到系统中磁盘的数量，这种损坏变得非常普遍。

我们的设计为执行各种任务的许多并发读取者和写入者提供了很高的聚合吞吐量。我们通过将流经 Master 的文件系统控制与直接在 Chunkserver 和客户端之间进行的数据传输分离开来实现这一点。通过使用大尺寸的块和块租约（将数据变更的权力下放给主副本），Master 在常见操作中的参与度被降至最低。这使得一个简单的、中心化的 Master 成为可能，而不会成为瓶颈。我们相信，网络协议栈的改进将消除当前单个客户端所看到的写入吞吐量限制。

GFS 成功满足了我们的存储需求，并在 Google 内部被广泛用作研发以及生产数据处理的存储平台。它是一个重要的工具，使我们能够继续在整个 Web 的规模上创新和解决问题。

## 致谢 (Acknowledgments)

我们要感谢以下人员对系统或论文的贡献。Brain Bershad（我们的指导者）和匿名审稿人给了我们宝贵的意见和建议。Anurag Acharya、Jeff Dean 和 David des Jardins 参与了早期设计。Fay Chang 致力于跨 Chunkserver 的副本比较工作。Guy Edjlali 致力于存储配额工作。Markus Gutschke 致力于测试框架和安全增强工作。David Kramer 致力于性能增强工作。Fay Chang、Urs Hoelzle、Max Ibel、Sharon Perl、Rob Pike 和 Debby Wallach 对论文的早期草稿发表了评论。我们在 Google 的许多同事勇敢地将他们的数据托付给一个新的文件系统，并给了我们有用的反馈。Yoshka 帮助进行了早期测试。

## 参考文献 (References)

[1] Thomas Anderson, Michael Dahlin, Jeanna Neefe, David Patterson, Drew Roselli, and Randolph Wang. **Serverless network file systems.** In *Proceedings of the 15th ACM Symposium on Operating System Principles*, pages 109–126, Copper Mountain Resort, Colorado, December 1995.

[2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. **Cluster I/O with River: Making the fast case common.** In *Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99)*, pages 10–22, Atlanta, Georgia, May 1999.

[3] Luis-Felipe Cabrera and Darrell D. E. Long. **Swift: Using distributed disk striping to provide high I/O data rates.** *Computer Systems*, 4(4):405–436, 1991.

[4] Garth A. Gibson, David F. Nagle, Khalil Amiri, Jeff Butler, Fay W. Chang, Howard Gobioff, Charles Hardin, Erik Riedel, David Rochberg, and Jim Zelenka. **A cost-effective, high-bandwidth storage architecture.** In *Proceedings of the 8th Architectural Support for Programming Languages and Operating Systems*, pages 92–103, San Jose, California, October 1998.

[5] John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satyanarayanan, Robert Sidebotham, and Michael West. **Scale and performance in a distributed file system.** *ACM Transactions on Computer Systems*, 6(1):51–81, February 1988.

[6] **InterMezzo.** [http://www.inter-mezzo.org](http://www.inter-mezzo.org), 2003.

[7] Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. **Replication in the Harp file system.** In *13th Symposium on Operating System Principles*, pages 226–238, Pacific Grove, CA, October 1991.

[8] **Lustre.** [http://www.lustre.org](http://www.lustre.org), 2003.

[9] David A. Patterson, Garth A. Gibson, and Randy H. Katz. **A case for redundant arrays of inexpensive disks (RAID).** In *Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data*, pages 109–116, Chicago, Illinois, September 1988.

[10] Frank Schmuck and Roger Haskin. **GPFS: A shared-disk file system for large computing clusters.** In *Proceedings of the First USENIX Conference on File and Storage Technologies*, pages 231–244, Monterey, California, January 2002.

[11] Steven R. Soltis, Thomas M. Ruwart, and Matthew T. O’Keefe. **The Gobal File System.** In *Proceedings of the Fifth NASA Goddard Space Flight Center Conference on Mass Storage Systems and Technologies*, College Park, Maryland, September 1996.

[12] Chandramohan A. Thekkath, Timothy Mann, and Edward K. Lee. **Frangipani: A scalable distributed file system.** In *Proceedings of the 16th ACM Symposium on Operating System Principles*, pages 224–237, Saint-Malo, France, October 1997.
