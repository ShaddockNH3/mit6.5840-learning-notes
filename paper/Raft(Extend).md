# In Search of an Understandable Consensus Algorithm (Extended Version)

> [寻找一种易于理解的共识算法（扩展版）](http://nil.csail.mit.edu/6.5840/2025/papers/raft-extended.pdf)

Diego Ongaro and John Ousterhout

Stanford University

## 摘要

Raft 是一种用于管理复制日志（replicated log）的共识算法。

它产生的结果等同于 (multi-)Paxos 算法，且其效率与 Paxos 相当，但它的结构与 Paxos 截然不同；这使得 Raft 比 Paxos 更容易理解，并且为构建实际系统提供了更好的基础。

为了增强可理解性，Raft 将共识的关键要素——例如领导者选举（leader election）、日志复制（log replication）和安全性（safety）——进行了分离，并且它强制执行更强的一致性程度，以减少必须考虑的状态数量。

用户研究的结果表明，对于学生而言，Raft 比 Paxos 更容易学习。此外，Raft 还包含了一种用于更改集群成员资格（cluster membership）的新机制，该机制利用重叠的多数派（overlapping majorities）来保证安全性。

## 1. 引言 (Introduction)

共识算法（Consensus algorithms）允许一组机器作为一个协调一致的整体进行工作，即使其中的部分成员出现故障，该组机器仍能继续生存。正因如此，它们在构建可靠的大规模软件系统中扮演着关键角色。在过去的十年里，Paxos [15, 16] 主导了关于共识算法的讨论：大多数共识算法的实现要么基于 Paxos，要么受到它的影响，而且 Paxos 已经成为教授学生共识算法的主要工具。

不幸的是，尽管有无数的尝试试图让 Paxos 变得更平易近人，但它仍然相当难以理解。此外，其架构需要复杂的修改才能支持实际系统。结果就是，无论是系统构建者还是学生，在面对 Paxos 时都感到十分吃力。

在亲自与 Paxos “搏斗”之后，我们着手寻找一种新的共识算法，旨在为系统构建和教育提供更好的基础。我们的方法有些不同寻常，因为我们的首要目标是**可理解性（understandability）**：我们能否定义一种用于实际系统的共识算法，并以一种比 Paxos 明显更容易学习的方式来描述它？此外，我们希望该算法能促进直觉的发展，这对于系统构建者来说至关重要。这不仅意味着算法要能工作，更重要的是要让人显而易见地明白它是**为什么**能工作的。

这项工作的成果就是一种名为 **Raft** 的共识算法。在设计 Raft 时，我们应用了特定的技术来提高可理解性，包括**分解（decomposition）**（Raft 将领导者选举、日志复制和安全性分离开来）和**状态空间缩减（state space reduction）**（相对于 Paxos，Raft 减少了非确定性的程度以及服务器之间可能出现不一致的方式）。一项涉及两所大学 43 名学生的用户研究表明，Raft 比 Paxos 明显更容易理解：在学习了这两种算法后，其中 33 名学生回答关于 Raft 的问题比回答关于 Paxos 的问题要好。

> *（注：原文页边灰条内容）* 本技术报告是 [32] 的扩展版本；补充材料在页边空白处以灰色条标记。发布于 2014 年 5 月 20 日。

Raft 在许多方面与现有的共识算法相似（最引人注目的是 Oki 和 Liskov 的 Viewstamped Replication [29, 22]），但它有几个新颖的特性：

* **强领导者（Strong leader）：** Raft 使用了一种比其他共识算法更强的领导形式。例如，日志条目（log entries）只从领导者流向其他服务器。这简化了复制日志的管理，并使得 Raft 更容易理解。
* **领导者选举（Leader election）：** Raft 使用随机计时器来选举领导者。这仅仅在任何共识算法都已经要求的心跳（heartbeats）机制上增加了一少部分机制，却能简单而迅速地解决冲突。
* **成员变更（Membership changes）：** Raft 用于更改集群中服务器集合的机制使用了一种新的**联合共识（joint consensus）**方法，即两种不同配置的多数派在转换期间会发生重叠。这允许集群在配置变更期间继续正常运行。

我们认为，无论是出于教育目的还是作为实现的基础，Raft 都优于 Paxos 和其他共识算法。它比其他算法更简单、更容易理解；它的描述足够完整，可以满足实际系统的需求；它拥有多个开源实现并已被多家公司使用；其安全性属性已被形式化规范并得到证明；且其效率与其他算法相当。

论文的其余部分将介绍复制状态机问题（第 2 节），讨论 Paxos 的优点和缺点（第 3 节），描述我们通往可理解性的通用方法（第 4 节），展示 Raft 共识算法（第 5-8 节），评估 Raft（第 9 节），并讨论相关工作（第 10 节）。

## 2. 复制状态机 (Replicated State Machines)

共识算法通常产生于**复制状态机（replicated state machines）**的背景之下 [37]。在这种方法中，一组服务器上的状态机计算相同状态的相同副本，并且即使部分服务器宕机，它们也能继续运行。复制状态机用于解决分布式系统中的各种容错问题。例如，拥有单个集群领导者（cluster leader）的大规模系统，如 GFS [8]、HDFS [38] 和 RAMCloud [33]，通常使用一个独立的复制状态机来管理领导者选举并存储必须在领导者崩溃后幸存的配置信息。复制状态机的例子包括 Chubby [2] 和 ZooKeeper [11]。

复制状态机通常使用**复制日志（replicated log）**来实现，如图 1 所示。每个服务器存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。每个日志都包含相同顺序的相同命令，因此每个状态机都处理相同的命令序列。由于状态机是**确定性的（deterministic）**，因此每个状态机都计算出相同的状态和相同的输出序列。

保持复制日志的一致性是共识算法的工作。服务器上的共识模块（consensus module）接收来自客户端的命令并将其添加到自己的日志中。它与其他服务器上的共识模块进行通信，以确保每个日志最终都包含相同顺序的相同请求，即使某些服务器发生故障也是如此。一旦命令被正确复制，每个服务器的状态机就会按日志顺序处理它们，并将输出返回给客户端。结果是，这些服务器看起来就像形成了一个单一的、高度可靠的状态机。

用于实际系统的共识算法通常具有以下属性：

* 它们确保在所有**非拜占庭（non-Byzantine）**条件下（包括网络延迟、分区、以及数据包丢失、重复和乱序）的**安全性（safety）**（即永远不会返回错误的结果）。
* 只要**多数派（majority）**的服务器在运行并且能够彼此通信以及与客户端通信，它们就是功能完全的（可用的）。因此，一个典型的由五台服务器组成的集群可以容忍任何两台服务器的故障。假设服务器通过停机来发生故障；它们稍后可以从稳定存储（stable storage）中的状态恢复并重新加入集群。
* 它们不依赖时序（timing）来确保日志的一致性：错误的时钟和极端的网络延迟在最坏的情况下只会导致可用性问题。
* 在通常情况下，只要集群的大多数成员对单轮远程过程调用（RPC）做出了响应，命令就可以完成；少数慢速服务器不需要影响整体系统性能。

## 3. Paxos 出了什么问题？ (What’s wrong with Paxos?)

在过去的十年里，Leslie Lamport 的 **Paxos 协议 [15]** 几乎成了**共识（consensus）**的代名词：它是课程中最常教授的协议，大多数共识算法的实现都以它为起点。Paxos 首先定义了一个能够就**单个决策（single decision）**（例如单个复制日志条目）达成一致的协议。我们将这个子集称为**单决议 Paxos（single-decree Paxos）**。然后，Paxos 组合了该协议的多个实例，以促进一系列决策（例如日志）的达成（即 **Multi-Paxos**）。Paxos 确保了**安全性（safety）**和**活性（liveness）**，并且支持集群成员资格的变更。其正确性已被证明，且在正常情况下是高效的。

不幸的是，Paxos 有两个显著的缺点。

**第一个缺点是 Paxos 异常难以理解。** 其完整的解释 [15] 以晦涩难懂而闻名；很少有人能成功理解它，而且都需要付出巨大的努力。因此，出现了一些试图用更简单的术语解释 Paxos 的尝试 [16, 20, 21]。这些解释集中在单决议子集上，但它们仍然具有挑战性。在对 NSDI 2012 与会者的一次非正式调查中，我们发现很少有人对 Paxos 感到得心应手，即使是经验丰富的研究人员也是如此。我们自己也曾与 Paxos 苦苦挣扎；直到阅读了几个简化的解释并设计了我们自己的替代协议之后（这个过程花了将近一年时间），我们才完全理解了这个协议。

我们假设 Paxos 的不透明性源于它选择**单决议子集**作为其基础。单决议 Paxos 是密集且微妙的：它被分为两个阶段，这两个阶段没有简单的直观解释，也不能被独立理解。正因如此，很难建立关于单决议协议**为什么**能工作的直觉。Multi-Paxos 的组合规则又增加了显著的额外复杂性和微妙之处。我们认为，就多个决策（即日志而不是单个条目）达成共识的整体问题，可以用其他更直接、更显而易见的方式进行分解。

**Paxos 的第二个问题是，它没有为构建实际实现提供良好的基础。** 一个原因是对于 Multi-Paxos 并没有广泛达成一致的算法。Lamport 的描述主要关于单决议 Paxos；他草拟了 Multi-Paxos 的可能方法，但许多细节缺失了。虽然有一些尝试试图充实和优化 Paxos，如 [26]、[39] 和 [13]，但它们彼此不同，也与 Lamport 的草图不同。像 Chubby [4] 这样的系统实现了类 Paxos 算法，但在大多数情况下，它们的细节并未公开。

此外，Paxos 的架构对于构建实际系统来说是一个糟糕的架构；这也是单决议分解的另一个后果。例如，独立地选择一组日志条目然后将它们融合（melding）成一个顺序日志几乎没有什么好处；这只会增加复杂性。围绕日志来设计系统要简单和高效得多，在这种设计中，新条目按受限的顺序被顺序追加。另一个问题是，Paxos在其核心使用了一种对称的对等（peer-to-peer）方法（尽管它最终建议使用一种弱形式的领导权作为性能优化）。这在一个只做一个决定的简化世界中是有意义的，但很少有实际系统使用这种方法。如果必须做出一系列决定，那么先选举一个领导者，然后让领导者协调这些决定会更简单、更快速。

结果是，实际系统与 Paxos 几乎没有相似之处。每个实现都从 Paxos 开始，发现实现它的困难，然后开发出一个显著不同的架构。这是耗时且容易出错的，而且理解 Paxos 的困难加剧了这个问题。Paxos 的公式化表述可能对于证明关于其正确性的定理来说是不错的，但实际实现与 Paxos 如此不同，以至于这些证明几乎没有价值。来自 Chubby 实现者的以下评论是很典型的：

> “在 Paxos 算法的描述与现实世界系统的需求之间存在着巨大的鸿沟…… 最终的系统将基于一个未经证明的协议 [4]。”

由于这些问题，我们得出结论：Paxos 无论对于系统构建还是教育来说，都没有提供一个良好的基础。鉴于共识在大规模软件系统中的重要性，我们决定看看是否能设计一种比 Paxos 具有更好属性的替代共识算法。**Raft** 就是那个实验的结果。

## 4. 为可理解性而设计 (Designing for understandability)

我们在设计 Raft 时有几个目标：它必须为系统构建提供一个完整且实用的基础，以便显著减少开发人员所需的设计工作量；它必须在所有条件下都是安全的，并且在典型的操作条件下是可用的；它在常见操作中必须是高效的。**但我们需要实现的最重要的目标——也是最困难的挑战——是可理解性（understandability）。** 它必须能够让广大的受众舒适地理解。此外，必须能够建立关于该算法的直觉，以便系统构建者可以进行扩展，这在现实世界的实现中是不可避免的。

在 Raft 的设计过程中，有许多地方我们需要在不同的替代方案之间做出选择。在这些情况下，我们基于**可理解性**来评估替代方案：解释每一个替代方案有多难（例如，其状态空间有多复杂，它是否有微妙的隐含意义？），以及读者完全理解该方法及其隐含意义有多容易？

我们承认，这种分析具有高度的主观性；尽管如此，我们使用了两种普遍适用的技术。

**第一种技术是众所周知的“问题分解（problem decomposition）”方法：** 只要可能，我们就将问题划分为可以相对独立地解决、解释和理解的独立部分。例如，在 Raft 中，我们分离了**领导者选举（leader election）**、**日志复制（log replication）**、**安全性（safety）**和**成员变更（membership changes）**。

**我们的第二种方法是通过减少需要考虑的状态数量来简化状态空间（state space），** 使系统更加一致（coherent），并尽可能消除非确定性（nondeterminism）。具体来说，日志不允许出现空洞，并且 Raft 限制了日志之间可能出现不一致的方式。虽然在大多数情况下我们试图消除非确定性，但在某些情况下，非确定性实际上提高了可理解性。特别是，随机化方法引入了非确定性，但它们倾向于通过以类似的方式处理所有可能的选择（“随便选一个；没关系”）来减少状态空间。我们使用随机化来简化 Raft 的领导者选举算法。

## 5. Raft 共识算法 (The Raft consensus algorithm)

Raft 是一种用于管理第 2 节所述形式的复制日志的算法。图 2 以浓缩的形式总结了该算法以供参考，图 3 列出了该算法的关键属性；本节的其余部分将分段讨论这些图表中的要素。

Raft 通过首先选举一位**独特的领导者（distinguished leader）**，然后赋予该领导者管理复制日志的全部责任来实现共识。领导者从客户端接收日志条目（log entries），将其复制到其他服务器上，并告诉服务器何时将日志条目应用到其状态机是安全的。拥有领导者简化了复制日志的管理。例如，领导者可以决定将新条目放置在日志中的什么位置，而无需咨询其他服务器，并且数据以一种简单的方式从领导者流向其他服务器。领导者可能会发生故障或与其他服务器断开连接，在这种情况下，会选举出一名新的领导者。

鉴于这种领导者方法，Raft 将共识问题分解为三个相对独立的子问题，将在随后的子节中进行讨论：

* **领导者选举（Leader election）：** 当现有的领导者发生故障时，必须选出一个新的领导者（第 5.2 节）。
* **日志复制（Log replication）：** 领导者必须接受来自客户端的日志条目并在整个集群中复制它们，强制其他日志与其自己的日志保持一致（第 5.3 节）。
* **安全性（Safety）：** Raft 的关键安全属性是图 3 中的**状态机安全属性（State Machine Safety Property）**：如果任何服务器已将特定的日志条目应用到其状态机，那么没有其他服务器可以针对相同的日志索引应用不同的命令。第 5.4 节描述了 Raft 如何确保此属性；该解决方案涉及对第 5.2 节中描述的选举机制进行的额外限制。

### 5.1 Raft 基础 (Raft basics)

一个 Raft 集群包含多台服务器；5 台是一个典型的数量，这允许系统容忍 2 台服务器的故障。在任何给定时间，每台服务器都处于以下三种状态之一：**领导者（leader）**、**跟随者（follower）**或**候选人（candidate）**。在正常操作中，只有一位领导者，其他所有服务器都是跟随者。跟随者是被动的：它们自己不发出任何请求，而只是简单地响应来自领导者和候选人的请求。领导者处理所有客户端请求（如果客户端联系跟随者，跟随者会将其重定向到领导者）。第三种状态，即候选人，用于选举新领导者，如第 5.2 节所述。图 4 显示了这些状态及其转换；下文将讨论这些转换。

Raft 将时间划分为任意长度的**任期（terms）**，如图 5 所示。任期用连续的整数编号。每个任期都以一次**选举（election）**开始，在选举中，一名或多名候选人尝试成为领导者，如第 5.2 节所述。如果一名候选人赢得了选举，那么它将在该任期的剩余时间内担任领导者。在某些情况下，选举会导致**选票被瓜分（split vote）**。在这种情况下，该任期将在没有领导者的情况下结束；一个新的任期（以及一次新的选举）将很快开始。Raft 确保在一个给定的任期内**至多**有一位领导者。

不同的服务器可能会在不同的时间观察到任期之间的转换，并且在某些情况下，服务器可能观察不到一次选举甚至整个任期。任期在 Raft 中充当**逻辑时钟（logical clock）** [14]，它们允许服务器检测过时的信息，例如过期的领导者。每台服务器都存储一个**当前任期号（current term number）**，该编号随时间单调递增。每当服务器进行通信时，都会交换当前任期号；如果一台服务器的当前任期号小于另一台服务器，那么它会将自己的当前任期号更新为较大的那个值。如果候选人或领导者发现自己的任期已过时，它会立即恢复到跟随者状态。如果服务器收到一个带有过时任期号的请求，它会拒绝该请求。

Raft 服务器使用**远程过程调用（RPCs）**进行通信，基本的共识算法只需要两种类型的 RPC。**请求投票 RPC（RequestVote RPCs）** 由候选人在选举期间发起（第 5.2 节），而**追加条目 RPC（AppendEntries RPCs）** 由领导者发起，用于复制日志条目并提供一种心跳形式（第 5.3 节）。第 7 节增加了第三种 RPC，用于在服务器之间传输快照（snapshots）。如果服务器没有及时收到响应，它们会重试 RPC，并且它们会并行发出 RPC 以获得最佳性能。

### 5.2 领导者选举 (Leader election)

Raft 使用一种**心跳（heartbeat）**机制来触发领导者选举。当服务器启动时，它们以跟随者（follower）身份开始。只要跟随者从领导者或候选人那里收到有效的 RPC，它就会保持在跟随者状态。领导者定期向所有跟随者发送心跳（不携带日志条目的 AppendEntries RPC），以维持其权威。如果跟随者在一段被称为**选举超时（election timeout）**的时间内没有收到任何通信，它就会认为没有可用的领导者，并开始一次选举以选出新的领导者。

要开始一次选举，跟随者会增加其当前任期号并转换到**候选人（candidate）**状态。然后，它会投票给自己，并并行地向集群中的每台其他服务器发出 RequestVote RPC。候选人会一直保持这种状态，直到发生以下三件事之一：(a) 它赢得了选举，(b) 另一台服务器确立了自己的领导者地位，或者 (c) 一段时间过去了但没有产生获胜者。下文将分别讨论这些结果。

**如果候选人收到来自整个集群中多数派服务器针对同一任期的投票，它就赢得了选举。** 在给定的任期内，每台服务器按照先到先得的原则，至多投票给一名候选人（注：第 5.4 节增加了对投票的额外限制）。**多数派规则（majority rule）**确保在特定任期内至多有一名候选人能够赢得选举（图 3 中的选举安全属性）。一旦候选人赢得选举，它就成为领导者。然后，它向所有其他服务器发送心跳消息，以确立其权威并阻止新的选举。

在等待投票期间，候选人可能会收到来自另一台声称是领导者的服务器发来的 AppendEntries RPC。如果该领导者的任期（包含在其 RPC 中）至少与候选人的当前任期一样大，那么候选人就承认该领导者是合法的，并回到跟随者状态。如果 RPC 中的任期小于候选人的当前任期，那么候选人会拒绝该 RPC 并继续处于候选人状态。

**第三种可能的结果是候选人既没有赢得也没有输掉选举：** 如果许多跟随者同时成为候选人，选票可能会被瓜分（split），导致没有任何候选人获得多数票。当这种情况发生时，每个候选人都会超时，并通过增加其任期号和发起另一轮 RequestVote RPC 来开始一次新的选举。然而，如果没有额外的措施，选票瓜分的情况可能会无限期地重复下去。

**Raft 使用随机选举超时（randomized election timeouts）来确保选票瓜分的情况很少发生，并且能迅速解决。** 为了从一开始就防止选票瓜分，选举超时是从一个固定的区间（例如 150–300ms）中随机选择的。这会将服务器分散开来，以便在大多数情况下只有一台服务器会超时；它会在其他任何服务器超时之前赢得选举并发送心跳。同样的机制也用于处理选票瓜分。每个候选人在选举开始时都会重置其随机选举超时时间，并且在开始下一次选举之前会等待该超时时间过去；这减少了在新选举中再次发生选票瓜分的可能性。第 9.3 节表明，这种方法能够迅速选出领导者。

选举是**可理解性（understandability）**如何指导我们在设计备选方案之间进行选择的一个例子。最初我们计划使用一种排名系统：每个候选人被分配一个唯一的排名，用于在竞争的候选人之间进行选择。如果候选人发现另一名候选人的排名更高，它就会回到跟随者状态，以便排名更高的候选人能够更容易地赢得下一次选举。我们发现这种方法在可用性方面产生了一些微妙的问题（如果排名较高的服务器发生故障，排名较低的服务器可能需要超时并再次成为候选人，但如果它这样做太快，可能会重置选举领导者的进度）。我们对算法进行了多次调整，但在每次调整后都会出现新的极端情况（corner cases）。最终我们要得出的结论是：**随机重试方法更加显而易见且易于理解。**

### 5.3 日志复制 (Log replication)

一旦领导者被选举出来，它就开始为客户端请求提供服务。每个客户端请求都包含一个要由复制状态机执行的命令。领导者将该命令作为一个新条目追加到它的日志中，然后并行地向每台其他服务器发出 AppendEntries RPC 以复制该条目。当该条目被安全复制后（如下文所述），领导者将该条目应用到其状态机，并将执行结果返回给客户端。如果跟随者崩溃或运行缓慢，或者如果网络数据包丢失，领导者会无限期地重试 AppendEntries RPC（即使在它已经向客户端做出响应之后），直到所有跟随者最终都存储了所有的日志条目。

日志的组织形式如图 6 所示。每个日志条目存储一个状态机命令，以及领导者接收到该条目时的任期号（term number）。日志条目中的任期号用于检测日志之间的不一致，并确保图 3 中的一些属性。每个日志条目还有一个整数索引（index）来标识其在日志中的位置。

领导者决定何时将日志条目应用到状态机是安全的；这样的条目被称为**已提交的（committed）**。Raft 保证已提交的条目是持久的，并且最终会被所有可用的状态机执行。一旦创建该条目的领导者将其复制到大多数服务器上（例如图 6 中的条目 7），该日志条目就被视为已提交。这也提交了领导者日志中所有之前的条目，包括由之前的领导者创建的条目。第 5.4 节讨论了在领导者变更后应用此规则时的一些微妙之处，并且它还表明这种提交的定义是安全的。领导者会追踪其知道的已提交的最高索引，并在未来的 AppendEntries RPC（包括心跳）中包含该索引，以便其他服务器最终能知道。一旦跟随者知道某个日志条目已提交，它就会将该条目应用到其本地状态机（按照日志顺序）。

我们设计了 Raft 日志机制，以保持不同服务器上日志之间的高度一致性（coherency）。这不仅简化了系统的行为并使其更具可预测性，而且是确保安全性的重要组成部分。Raft 维护以下属性，它们共同构成了图 3 中的**日志匹配属性（Log Matching Property）**：

* 如果在不同的日志中有两个条目拥有相同的索引和任期，那么它们存储了相同的命令。
* 如果在不同的日志中有两个条目拥有相同的索引和任期，那么这些日志在所有之前的条目上都是完全相同的。

第一个属性源于这样一个事实：领导者在给定的任期内，针对给定的日志索引至多创建一个条目，并且日志条目永远不会改变其在日志中的位置。第二个属性由 AppendEntries 执行的一个简单的一致性检查（consistency check）来保证。当发送 AppendEntries RPC 时，领导者会包含紧邻新条目之前的那个条目的索引和任期。如果跟随者在其日志中没有找到具有相同索引和任期的条目，它就会拒绝新的条目。这个一致性检查充当了一个归纳步骤：日志的初始空状态满足日志匹配属性，并且只要日志被扩展，一致性检查就会保持日志匹配属性。结果是，每当 AppendEntries 成功返回时，领导者就知道跟随者的日志与它自己的日志在新条目之前（包括新条目）是完全相同的。

在正常操作期间，领导者和跟随者的日志保持一致，因此 AppendEntries 的一致性检查永远不会失败。然而，领导者崩溃可能会导致日志不一致（旧的领导者可能没有完全复制其日志中的所有条目）。这些不一致可能会在一系列的领导者和跟随者崩溃中累积。图 7 说明了跟随者的日志可能与新领导者的日志不同的方式。跟随者可能缺少领导者上存在的条目，或者它可能拥有领导者上不存在的多余条目，或者两者兼而有之。日志中缺失和多余的条目可能会跨越多个任期。

在 Raft 中，领导者通过强制跟随者的日志复制它自己的日志来处理不一致。这意味着跟随者日志中的冲突条目将被来自领导者日志的条目覆盖。第 5.4 节将表明，当加上另一个限制时，这样做是安全的。

为了使跟随者的日志与自己的一致，领导者必须找到两个日志达成一致的最新日志条目，删除跟随者日志中该点之后的所有条目，并将领导者在该点之后的所有条目发送给跟随者。所有这些操作都是为了响应 AppendEntries RPC 执行的一致性检查而发生的。领导者为每个跟随者维护一个 `nextIndex`，这是领导者将发送给该跟随者的下一个日志条目的索引。当一个领导者刚上任时，它将所有 `nextIndex` 值初始化为其日志中最后一个索引之后的索引（图 7 中为 11）。如果跟随者的日志与领导者的不一致，AppendEntries 一致性检查将在下一次 AppendEntries RPC 中失败。在被拒绝后，领导者会递减 `nextIndex` 并重试 AppendEntries RPC。最终 `nextIndex` 会达到领导者和跟随者日志匹配的一个点。当这种情况发生时，AppendEntries 将会成功，这将删除跟随者日志中的任何冲突条目并追加来自领导者日志的条目（如果有的话）。一旦 AppendEntries 成功，跟随者的日志就与领导者的一致了，并且在该任期的剩余时间内都将保持这种状态。

如果需要，可以对协议进行优化以减少被拒绝的 AppendEntries RPC 的数量。例如，当拒绝一个 AppendEntries 请求时，跟随者可以包含冲突条目的任期以及它存储的该任期的第一个索引。有了这些信息，领导者可以递减 `nextIndex` 以绕过该任期内的所有冲突条目；这样，每个有冲突条目的任期只需要一个 AppendEntries RPC，而不是每个条目一个 RPC。在实践中，我们怀疑这种优化是否必要，因为故障很少发生，而且不太可能有很多不一致的条目。

有了这种机制，领导者在上任时不需要采取任何特殊行动来恢复日志一致性。它只是开始正常操作，日志会在 AppendEntries 一致性检查失败的响应中自动收敛。领导者永远不会覆盖或删除其自身日志中的条目（图 3 中的**领导者仅追加属性 [Leader Append-Only Property]**）。

这种日志复制机制展现了第 2 节中描述的理想的共识属性：只要大多数服务器处于正常运行状态，Raft 就可以接受、复制和应用新的日志条目；在正常情况下，一个新的条目可以通过单轮 RPC 复制到集群的大多数服务器；并且单个缓慢的跟随者不会影响性能。

### 5.4 安全性 (Safety)

前面的章节描述了 Raft 如何选举领导者和复制日志条目。然而，到目前为止描述的机制还不足以确保每个状态机按相同的顺序执行完全相同的命令。例如，当领导者提交了几个日志条目时，某个跟随者可能处于不可用状态，然后该跟随者可能会被选为领导者，并用新的条目覆盖这些（已提交的）条目；结果，不同的状态机可能会执行不同的命令序列。

本节通过增加一个关于哪些服务器可以当选领导者的限制来完善 Raft 算法。该限制确保了任何给定任期的领导者都包含之前任期中提交的所有条目（图 3 中的**领导者完整性属性 [Leader Completeness Property]**）。鉴于这个选举限制，我们随后使提交规则更加精确。最后，我们展示了领导者完整性属性的证明草图，并说明它是如何导致复制状态机的正确行为的。

#### 5.4.1 选举限制 (Election restriction)

在任何基于领导者的共识算法中，领导者最终都必须存储所有已提交的日志条目。在某些共识算法中，例如 Viewstamped Replication [22]，即使领导者最初不包含所有已提交的条目，它也可以被选举出来。这些算法包含额外的机制来识别缺失的条目并将其传输给新领导者，无论是在选举过程中还是在之后不久。不幸的是，这导致了相当大的额外机制和复杂性。Raft 使用了一种更简单的方法，它保证从选举的那一刻起，每个新领导者都包含了之前任期中所有已提交的条目，而无需将这些条目传输给领导者。这意味着日志条目只在一个方向上流动，即从领导者到跟随者，并且领导者永远不会覆盖其日志中现有的条目。

Raft 使用投票过程来阻止候选人赢得选举，除非它的日志包含所有已提交的条目。候选人必须联系集群的大多数成员才能当选，这意味着每个已提交的条目必须至少存在于这些服务器中的一个里面。如果候选人的日志至少与该多数派中的任何其他日志一样**“最新（up-to-date）”**（“最新”的精确定义见下文），那么它将持有所有已提交的条目。RequestVote RPC 实现了这一限制：RPC 包含了关于候选人日志的信息，如果投票者自己的日志比候选人的日志更新，它就会拒绝投票。

Raft 通过比较日志中最后一个条目的索引和任期来确定两个日志哪一个更新。如果两个日志的最后一个条目拥有不同的任期，那么拥有较晚任期的日志更新。如果日志以相同的任期结束，那么哪个日志更长，哪个就更新。

#### 5.4.2 提交之前任期的条目 (Committing entries from previous terms)

如第 5.3 节所述，一旦领导者将其当前任期的某个条目存储在多数派服务器上，它就知道该条目已提交。如果领导者在提交某个条目之前崩溃，未来的领导者将尝试完成该条目的复制。然而，一旦某个**之前任期（previous term）**的条目存储在多数派服务器上，领导者**不能**立即断定该条目已提交。图 8 说明了一种情况：一个旧的日志条目存储在多数派服务器上，但仍可能被未来的领导者覆盖。

为了消除像图 8 中那样的问题，**Raft 永远不会通过计算副本数来提交之前任期的日志条目。** 只有领导者**当前任期（current term）**的日志条目才通过计算副本数来提交；一旦当前任期的某个条目以这种方式被提交，那么由于日志匹配属性，所有之前的条目都会被间接提交。在某些情况下，领导者可以安全地断定一个较旧的日志条目已提交（例如，如果该条目存储在每台服务器上），但为了简单起见，Raft 采取了一种更保守的方法。

Raft 在提交规则中引入这种额外的复杂性，是因为当领导者复制之前任期的条目时，日志条目保留了它们**原始的任期号**。在其他共识算法中，如果新领导者重新复制之前“任期”的条目，它必须使用其新的“任期号”来进行。Raft 的方法使得推断日志条目变得更加容易，因为它们随时间和跨日志保持相同的任期号。此外，Raft 中的新领导者发送的之前任期的日志条目比其他算法要少（其他算法必须发送冗余的日志条目来对它们进行重新编号，然后才能提交它们）。

#### 5.4.3 安全性论证 (Safety argument)

给出了完整的 Raft 算法，我们要现在可以更精确地论证**领导者完整性属性（Leader Completeness Property）**是成立的（该论证基于安全性证明；见第 9.2 节）。我们假设领导者完整性属性**不成立**，然后我们要证明这会导出矛盾（反证法）。

假设任期 T 的领导者（leader T）提交了一个来自其任期的日志条目，但该日志条目没有被某个未来任期的领导者存储。考虑**最小的**任期 ( U > T )，其领导者（leader U）没有存储该条目。

1. 在该已提交条目在 leader U 选举之时，一定不存在于 leader U 的日志中（领导者从不删除或覆盖条目）。
2. leader T 将该条目复制到了集群的多数派中，而 leader U 获得了集群多数派的选票。因此，至少有一台服务器（“投票者”）既接受了来自 leader T 的条目，又投票给了 leader U，如图 9 所示。这个投票者是达成矛盾的关键。
3. 投票者必须在投票给 leader U **之前**接受了来自 leader T 的已提交条目；否则它就会拒绝来自 leader T 的 AppendEntries 请求（因为它的当前任期会比 T 高）。
4. 当投票者投票给 leader U 时，它仍然存储着该条目，因为每一个中间的领导者都包含该条目（根据假设），领导者从不移除条目，而跟随者只有在与领导者冲突时才移除条目。
5. 投票者将其选票投给了 leader U，所以 leader U 的日志必须至少与投票者的一样**“最新（up-to-date）”**。这会导致以下两个矛盾之一。
6. **首先**，如果投票者和 leader U 拥有相同的最后日志任期，那么 leader U 的日志必须至少与投票者的一样长，所以它的日志包含了投票者日志中的每一个条目。这是一个矛盾，因为投票者包含了该已提交条目，而假设 leader U 没有。
7. **否则**，leader U 的最后日志任期必须大于投票者的。此外，它肯定大于 T，因为投票者的最后日志任期至少是 T（它包含了来自任期 T 的已提交条目）。创建 leader U 最后日志条目的那个更早的领导者必须在其日志中包含该已提交条目（根据假设）。那么，根据日志匹配属性，leader U 的日志也必须包含该已提交条目，这又是一个矛盾。
8. 这完成了矛盾证明。因此，所有任期大于 T 的领导者必须包含所有来自任期 T 且在任期 T 中已提交的条目。
9. 日志匹配属性保证了未来的领导者也将包含被间接提交的条目，例如图 8(d) 中的索引 2。

鉴于领导者完整性属性，我们可以证明图 3 中的**状态机安全属性（State Machine Safety Property）**，即如果一台服务器已将给定索引处的日志条目应用到其状态机，则没有其他服务器会针对同一索引应用不同的日志条目。当服务器将日志条目应用到其状态机时，其日志直到该条目为止必须与领导者的日志完全相同，并且该条目必须已提交。现在考虑任何服务器应用给定日志索引的**最低**任期；日志完整性属性保证了所有更高任期的领导者都将存储相同的日志条目，因此在后续任期中应用该索引的服务器将应用相同的值。因此，状态机安全属性成立。

最后，Raft 要求服务器按日志索引顺序应用条目。结合状态机安全属性，这意味着所有服务器将把完全相同的一组日志条目应用到它们的状态机，并且顺序相同。

### 5.5 跟随者和候选人崩溃 (Follower and candidate crashes)

到目前为止，我们一直专注于领导者故障。跟随者和候选人的崩溃比领导者崩溃要容易处理得多，而且它们的处理方式相同。如果跟随者或候选人崩溃，那么发给它的未来的 RequestVote 和 AppendEntries RPC 将会失败。**Raft 通过无限期地重试来处理这些失败；** 如果崩溃的服务器重新启动，那么 RPC 将会成功完成。如果服务器在完成 RPC 之后但在响应之前崩溃，那么它在重新启动后将再次收到相同的 RPC。Raft 的 RPC 是**幂等的（idempotent）**，所以这不会造成任何伤害。例如，如果跟随者收到一个 AppendEntries 请求，其中包含了其日志中已经存在的日志条目，它会忽略新请求中的那些条目。

### 5.6 时序和可用性 (Timing and availability)

我们对 Raft 的要求之一是安全性不能依赖于时序（timing）：系统不能仅仅因为某些事件发生得比预期快或慢就产生错误的结果。然而，**可用性（availability）**（即系统及时响应客户端的能力）必然依赖于时序。例如，如果消息交换所需的时间比服务器崩溃之间的典型时间还要长，候选人将无法保持足够长的在线时间来赢得选举；没有一个稳定的领导者，Raft 就无法取得进展。

领导者选举是 Raft 中时序最关键的方面。只要系统满足以下时序要求，Raft 就能够选举并维持一个稳定的领导者：

[ \text{broadcastTime} \ll \text{electionTimeout} \ll \text{MTBF} ]

在这个不等式中，`broadcastTime` 是服务器并行地向集群中的每台服务器发送 RPC 并接收其响应所需的平均时间；`electionTimeout` 是第 5.2 节中描述的选举超时时间；`MTBF` 是单个服务器发生故障之间的平均时间（平均故障间隔时间）。

广播时间应该比选举超时时间小一个数量级，以便领导者能够可靠地发送心跳消息，以阻止跟随者开始选举；鉴于选举超时使用的随机化方法，这个不等式也使得选票瓜分的可能性变得很小。选举超时时间应该比 MTBF 小几个数量级，以便系统能够稳定地取得进展。当领导者崩溃时，系统将在大约选举超时的时间内不可用；我们希望这只占总时间的一小部分。

广播时间和 MTBF 是底层系统的属性，而选举超时时间是我们必须选择的。Raft 的 RPC 通常要求接收者将信息持久化到稳定存储中，因此广播时间可能在 0.5ms 到 20ms 之间，具体取决于存储技术。因此，**选举超时时间可能在 10ms 到 500ms 之间。** 典型的服务器 MTBF 是几个月或更长，这很容易满足时序要求。

## 6. 集群成员变更 (Cluster membership changes)

到目前为止，我们一直假设集群配置（参与共识算法的服务器集合）是固定的。在实践中，偶尔需要更改配置，例如在服务器发生故障时进行替换，或更改复制程度。虽然这可以通过让整个集群下线，更新配置文件，然后重启集群来完成，但这会导致集群在切换期间不可用。此外，如果有任何手动步骤，就有操作员出错的风险。为了避免这些问题，我们决定**自动化配置更改并将其纳入 Raft 共识算法中**。

为了使配置更改机制安全，在转换过程中**不能**有任何时刻可能选举出同一任期的两个领导者。不幸的是，任何让服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次性原子地切换所有服务器，因此集群可能会在转换期间分裂成两个独立的多数派（见图 10）。

为了确保安全性，配置更改必须使用**两阶段（two-phase）**方法。有多种方法可以实现这两个阶段。例如，某些系统（如 [22]）使用第一阶段禁用旧配置，使其无法处理客户端请求；然后第二阶段启用新配置。在 Raft 中，集群首先切换到一个称为**联合共识（joint consensus）**的过渡配置；一旦联合共识被提交，系统就会切换到新配置。联合共识结合了旧配置和新配置：

* 日志条目被复制到两个配置中的所有服务器。
* 来自任一配置的任何服务器都可以充当领导者。
* 达成一致（用于选举和条目提交）需要分别获得**旧配置和新配置的多数派**的支持。

联合共识允许单个服务器在不同时间在配置之间转换，而不会影响安全性。此外，联合共识允许集群在整个配置更改期间继续为客户端请求提供服务。

集群配置使用复制日志中的特殊条目进行存储和通信；图 11 说明了配置更改过程。当领导者收到将配置从 ( C_{old} ) 更改为 ( C_{new} ) 的请求时，它将联合共识的配置（图中为 ( C_{old,new} )）存储为日志条目，并使用前面描述的机制复制该条目。一旦给定的服务器将新配置条目添加到其日志中，它就会将该配置用于所有未来的决策（服务器总是使用其日志中的最新配置，而不管该条目是否已提交）。这意味着领导者将使用 ( C_{old,new} ) 的规则来确定 ( C_{old,new} ) 的日志条目何时被提交。如果领导者崩溃，新领导者可能会在 ( C_{old} ) 或 ( C_{old,new} ) 下被选出，具体取决于获胜的候选人是否收到了 ( C_{old,new} )。无论如何，( C_{new} ) 在此期间都无法做出单方面决定。

一旦 ( C_{old,new} ) 被提交，没有对方的批准，( C_{old} ) 和 ( C_{new} ) 都无法做出决定，并且**领导者完整性属性**确保只有拥有 ( C_{old,new} ) 日志条目的服务器才能当选为领导者。现在，领导者创建一个描述 ( C_{new} ) 的日志条目并将其复制到集群是安全的。同样，此配置将在每台服务器看到它时立即生效。当新配置根据 ( C_{new} ) 的规则被提交时，旧配置就变得无关紧要了，不在新配置中的服务器可以被关闭。如图 11 所示，没有任何时候 ( C_{old} ) 和 ( C_{new} ) 都能做出单方面决定；这保证了安全性。

关于重新配置，还有三个问题需要解决。

**第一个问题是新服务器最初可能不存储任何日志条目。** 如果它们以这种状态被添加到集群中，它们可能需要很长时间才能赶上，在此期间可能无法提交新的日志条目。为了避免可用性缺口，Raft 在配置更改之前引入了一个额外的阶段，在该阶段，新服务器作为**无投票权成员（non-voting members）**加入集群（领导者将日志条目复制给它们，但在计算多数派时不考虑它们）。一旦新服务器赶上了集群的其余部分，重新配置就可以如上所述进行。

**第二个问题是集群领导者可能不是新配置的一部分。** 在这种情况下，领导者一旦提交了 ( C_{new} ) 日志条目，就会下台（回到跟随者状态）。这意味着会有一段时间（在它提交 ( C_{new} ) 时），领导者正在管理一个不包括它自己的集群；它复制日志条目，但在计算多数派时不把自己计算在内。领导者转换发生在 ( C_{new} ) 被提交时，因为这是新配置可以独立运行的第一个时间点（总是可以从 ( C_{new} ) 中选出一个领导者）。在此之前，可能只有来自 ( C_{old} ) 的服务器才能当选领导者。

**第三个问题是移除的服务器（不在 ( C_{new} ) 中的那些）可能会扰乱集群。** 这些服务器将不会收到心跳，因此它们会超时并开始新的选举。然后它们会发送带有新任期号的 RequestVote RPC，这将导致当前的领导者恢复到跟随者状态。虽然最终会选出一个新领导者，但被移除的服务器会再次超时，过程会重复，导致可用性差。

为了防止这个问题，**当服务器确信当前存在领导者时，它们会忽略 RequestVote RPC。** 具体来说，如果服务器在收到当前领导者消息后的最小选举超时时间内收到 RequestVote RPC，它不会更新其任期或投出选票。这不会影响正常的选举，因为在正常选举中，每台服务器在开始选举之前至少等待最小选举超时时间。然而，这有助于避免被移除服务器的干扰：如果领导者能够将其心跳发送到集群，那么它就不会被更大的任期号赶下台。

## 7. 日志压缩 (Log compaction)

在正常操作期间，Raft 的日志会随着包含更多客户端请求而增长，但在实际系统中，它不能无限制地增长。随着日志变得越来越长，它会占用更多的空间，并且需要更多的时间来重放（replay）。如果没有某种机制来丢弃日志中积累的过时信息，最终会导致可用性问题。

**快照（Snapshotting）**是最简单的压缩方法。在快照中，整个当前系统状态被写入稳定存储上的快照中，然后丢弃到该点为止的整个日志。Chubby 和 ZooKeeper 中使用了快照技术，本节的其余部分将描述 Raft 中的快照技术。

增量压缩方法（Incremental approaches to compaction），如日志清理（log cleaning）[36] 和日志结构合并树（LSM trees）[30, 5]，也是可行的。这些方法一次只操作一部分数据，因此它们将压缩的负载更均匀地分散在时间上。它们首先选择一个积累了许多已删除和被覆盖对象的数据区域，然后将该区域中的活动对象更紧凑地重写并释放该区域。与快照相比，这需要大量的额外机制和复杂性，而快照通过始终操作整个数据集来简化问题。虽然日志清理需要修改 Raft，但状态机可以使用与快照相同的接口来实现 LSM 树。

图 12 显示了 Raft 中快照的基本思想。每台服务器独立地拍摄快照，只覆盖其日志中已提交的条目。大部分工作包括状态机将其当前状态写入快照。Raft 还在快照中包含少量的元数据：**最后的包含索引（last included index）**是快照替换的日志中最后一个条目的索引（状态机应用的最后一个条目），**最后的包含任期（last included term）**是该条目的任期。保留这些是为了支持快照之后的第一个日志条目的 AppendEntries 一致性检查，因为该条目需要前一个日志索引和任期。为了启用集群成员变更（第 6 节），快照还包含截至最后包含索引的日志中的最新配置。一旦服务器完成写入快照，它就可以删除直到最后包含索引的所有日志条目，以及任何先前的快照。

尽管服务器通常独立地拍摄快照，但领导者偶尔必须向落后的跟随者发送快照。当领导者已经丢弃了它需要发送给跟随者的下一个日志条目时，就会发生这种情况。幸运的是，这种情况在正常操作中不太可能发生：跟上领导者的跟随者应该已经有了这个条目。然而，一个异常缓慢的跟随者或加入集群的新服务器（第 6 节）则不会有。让这样的跟随者跟上进度的办法是领导者通过网络向其发送快照。

领导者使用一种名为 **InstallSnapshot** 的新 RPC 将快照发送给太落后的跟随者；见图 13。当跟随者收到带有此 RPC 的快照时，它必须决定如何处理其现有的日志条目。通常，快照将包含接收者日志中尚不存在的新信息。在这种情况下，跟随者会丢弃其整个日志；它全部被快照取代，并且可能包含与快照冲突的未提交条目。如果跟随者收到的快照描述了其日志的前缀（由于重传或错误），那么快照覆盖的日志条目将被删除，但快照之后的条目仍然有效且必须保留。

这种快照方法偏离了 Raft 的强领导者原则，因为跟随者可以在领导者不知情的情况下拍摄快照。然而，我们认为这种偏离是合理的。虽然拥有领导者有助于在达成共识时避免冲突的决策，但在拍摄快照时共识已经达成，因此不存在决策冲突。数据仍然只从领导者流向跟随者，只是跟随者现在可以重新组织它们的数据了。

我们考虑过一种基于领导者的替代方法，即只有领导者创建快照，然后将其发送给每个跟随者。然而，这有两个缺点。首先，将快照发送给每个跟随者会浪费网络带宽并减慢快照过程。每个跟随者都已经拥有生成自己快照所需的信息，而且对于服务器来说，从其本地状态生成快照通常比通过网络发送和接收快照要便宜得多。其次，领导者的实现会更加复杂。例如，领导者需要在向跟随者复制新日志条目的同时并行发送快照，以免阻塞新的客户端请求。

还有两个问题会影响快照性能。**首先，服务器必须决定何时进行快照。** 如果服务器快照太频繁，会浪费磁盘带宽和能量；如果快照太不频繁，它就有耗尽存储容量的风险，并且会增加重启期间重放日志所需的时间。一种简单的策略是当日志达到以字节为单位的固定大小时拍摄快照。如果此大小设置为明显大于快照的预期大小，那么快照的磁盘带宽开销将很小。

**第二个性能问题是写入快照可能需要花费大量时间，而且我们不希望这会延迟正常操作。** 解决方案是使用**写时复制（copy-on-write）**技术，以便在不影响正在写入的快照的情况下接受新的更新。例如，使用函数式数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如 Linux 上的 `fork`）来创建整个状态机的内存快照（我们的实现使用了这种方法）。

## 8. 客户端交互 (Client interaction)

本节描述了客户端如何与 Raft 交互，包括客户端如何找到集群领导者以及 Raft 如何支持**线性化语义（linearizable semantics）** [10]。这些问题适用于所有基于共识的系统，Raft 的解决方案与其他系统相似。

Raft 的客户端将其所有请求发送给领导者。当客户端首次启动时，它连接到一个随机选择的服务器。如果客户端的首选不是领导者，该服务器将拒绝客户端的请求，并提供它所知道的最近一位领导者的信息（AppendEntries 请求包含领导者的网络地址）。如果领导者崩溃，客户端请求将超时；然后客户端会再次尝试连接随机选择的服务器。

我们对 Raft 的目标是实现**线性化语义**（每个操作看起来都在其调用和响应之间的某个时间点，原子地、精确一次地执行）。然而，如前所述，Raft 可能会多次执行同一条命令：例如，如果领导者在提交日志条目之后但在响应客户端之前崩溃，客户端将向新领导者重试该命令，导致它被第二次执行。解决方案是客户端为每条命令分配唯一的序列号。然后，状态机跟踪为每个客户端处理的最新序列号以及相关的响应。如果它收到一条序列号已经被执行过的命令，它会立即响应而不重新执行该请求。

**只读操作（Read-only operations）**可以在不向日志中写入任何内容的情况下进行处理。然而，如果没有额外的措施，这会有返回过时数据的风险，因为响应该请求的领导者可能已经被它不知道的新领导者取代了。线性化读取必须不能返回过时数据，Raft 需要两个额外的预防措施来在不使用日志的情况下保证这一点。

**首先，领导者必须拥有关于哪些条目已提交的最新信息。** 领导者完整性属性保证领导者拥有所有已提交的条目，但在其任期开始时，它可能不知道这些是哪些。为了弄清楚，它需要提交一个来自其任期的条目。Raft 通过让每个领导者在其任期开始时向日志中提交一个空的 **no-op** 条目来处理这个问题。

**其次，领导者在处理只读请求之前必须检查自己是否已被罢免**（如果选出了更新的领导者，它的信息可能已过时）。Raft 通过让领导者在响应只读请求之前与集群的多数派交换心跳消息来处理这个问题。或者，领导者可以依赖心跳机制来提供一种形式的租约（lease）[9]，但这将依赖时序来保证安全性（它假设时钟偏差是有界的）。

## 9. 实现与评估 (Implementation and evaluation)

我们已经将 Raft 实现为一个复制状态机的一部分，该状态机用于存储 RAMCloud [33] 的配置信息，并协助 RAMCloud 协调器的故障转移。Raft 实现包含大约 2000 行 C++ 代码（不包括测试、注释或空行）。源代码是免费提供的 [23]。基于本文的草稿，还有大约 25 个处于不同开发阶段的独立第三方开源 Raft 实现 [34]。此外，各家公司正在部署基于 Raft 的系统 [34]。

本节的其余部分将使用三个标准来评估 Raft：**可理解性（understandability）**、**正确性（correctness）**和**性能（performance）**。

### 9.1 可理解性 (Understandability)

为了衡量 Raft 相对于 Paxos 的可理解性，我们进行了一项实验研究，对象是斯坦福大学高级操作系统课程和加州大学伯克利分校分布式计算课程的高年级本科生和研究生。我们录制了关于 Raft 和 Paxos 的视频讲座，并制作了相应的测验。Raft 讲座涵盖了本文的内容（日志压缩除外）；Paxos 讲座涵盖了足以创建一个等效复制状态机的材料，包括单决议 Paxos、多决议 Paxos、重新配置以及实践中所需的少量优化（如领导者选举）。测验测试了对算法的基本理解，并要求学生推导极端情况。每个学生观看一个视频，参加相应的测验，然后观看第二个视频，并参加第二个测验。大约一半的参与者先做 Paxos 部分，另一半先做 Raft 部分，以考虑个体表现差异和从研究的第一部分获得的经验。我们比较了参与者在每个测验中的分数，以确定参与者是否表现出对 Raft 的更好理解。

我们试图使 Paxos 和 Raft 之间的比较尽可能公平。该实验在两个方面有利于 Paxos：43 名参与者中有 15 名报告说之前有一些 Paxos 经验，而且 Paxos 视频比 Raft 视频长 14%。如表 1 所总结，我们已采取措施减轻潜在的偏差来源。我们所有的材料都可以供审查 [28, 31]。

平均而言，参与者在 Raft 测验中的得分比 Paxos 测验高 4.9 分（满分 60 分，Raft 平均分为 25.7，Paxos 平均分为 20.8）；图 14 显示了他们的个人得分。配对 t 检验表明，在 95% 的置信度下，Raft 分数的真实分布均值至少比 Paxos 分数的真实分布大 2.5 分。

我们还建立了一个线性回归模型，根据三个因素预测新学生的测验分数：他们参加了哪个测验、他们之前的 Paxos 经验程度以及他们学习算法的顺序。该模型预测，选择测验会产生 12.5 分的差异，有利于 Raft。这明显高于观察到的 4.9 分差异，因为许多实际学生都有 Paxos 经验，这对 Paxos 很有帮助，而对 Raft 的帮助稍小。奇怪的是，模型还预测，对于已经参加过 Paxos 测验的人来说，Raft 的得分会低 6.3 分；虽然我们不知道原因，但这在统计上似乎是显著的。

我们还在测验后调查了参与者，看看他们觉得哪个算法更容易实现或解释；这些结果如图 15 所示。绝大多数参与者报告 Raft 会更容易实现和解释（每个问题 41 人中有 33 人）。然而，这些自我报告的感觉可能不如参与者的测验分数可靠，而且参与者可能受到我们关于 Raft 更容易理解这一假设的知识的偏见影响。

关于 Raft 用户研究的详细讨论见 [31]。

### 9.2 正确性 (Correctness)

我们要为第 5 节中描述的共识机制开发了一个形式化规范和安全性证明。形式化规范 [31] 使用 TLA+ 规范语言 [17] 使图 2 中总结的信息完全精确。它大约有 400 行长，是证明的主题。对于任何实现 Raft 的人来说，它本身也是有用的。我们已经使用 TLA 证明系统 [7] 机械地证明了**日志完整性属性（Log Completeness Property）**。然而，该证明依赖于未经机械检查的不变量（例如，我们尚未证明规范的类型安全性）。此外，我们要为**状态机安全属性（State Machine Safety property）**编写了一个非形式化证明 [31]，它是完整的（仅依赖于规范）且相对精确（大约 3500 字长）。

### 9.3 性能 (Performance)

Raft 的性能与其他共识算法（如 Paxos）相似。对于性能来说，最重要的场景是已确立的领导者复制新的日志条目。Raft 使用最少的消息数量（从领导者到集群一半成员的一次往返）来实现这一点。进一步提高 Raft 的性能也是可能的。例如，它很容易支持批处理和流水线请求，以实现更高的吞吐量和更低的延迟。文献中已经针对其他算法提出了各种优化；其中许多可以应用于 Raft，但我们将其留给未来的工作。

我们使用我们的 Raft 实现来测量 Raft 领导者选举算法的性能，并回答两个问题。**首先，选举过程收敛得快吗？其次，在领导者崩溃后可以实现的最小停机时间（downtime）是多少？**

为了测量领导者选举，我们反复使一个由五台服务器组成的集群的领导者崩溃，并计时检测崩溃和选出新领导者所需的时间（见图 16）。为了生成最坏情况的场景，每次试验中的服务器具有不同的日志长度，因此某些候选人没有资格成为领导者。此外，为了促使选票瓜分，我们的测试脚本在终止其进程之前触发了来自领导者的心跳 RPC 的同步广播（这近似于领导者在崩溃之前复制新日志条目的行为）。领导者在其心跳间隔内均匀随机地崩溃，该间隔是所有测试的最小选举超时的一半。因此，最小可能的停机时间约为最小选举超时的一半。

图 16 的顶部图表显示，**在选举超时中加入少量的随机性足以避免选举中的选票瓜分。** 在没有随机性的情况下，由于许多选票瓜分，我们的测试中的领导者选举始终花费超过 10 秒。仅添加 5ms 的随机性就有显著帮助，导致中位停机时间为 287ms。使用更多的随机性可以改善最坏情况的行为：使用 50ms 的随机性，最坏情况下的完成时间（超过 1000 次试验）为 513ms。

图 16 的底部图表显示，可以通过减少选举超时来减少停机时间。使用 12–24ms 的选举超时，平均只需 35ms 即可选出领导者（最长的试验花费了 152ms）。然而，将超时降低到此点以下会违反 Raft 的时序要求：领导者难以在其他服务器开始新选举之前广播心跳。这会导致不必要的领导者变更并降低整体系统可用性。**我们建议使用保守的选举超时，例如 150–300ms；** 这样的超时不太可能导致不必要的领导者变更，并且仍将提供良好的可用性。

## 10. 相关工作 (Related work)

关于共识算法的出版物数不胜数，其中许多属于以下类别之一：

* Lamport 对 **Paxos** 的原始描述 [15]，以及试图更清楚地解释它的尝试 [16, 20, 21]。
* Paxos 的详细阐述，填补了缺失的细节并修改了算法，以便为实现提供更好的基础 [26, 39, 13]。
* 实现了共识算法的系统，如 **Chubby** [2, 4]、**ZooKeeper** [11, 12] 和 **Spanner** [6]。Chubby 和 Spanner 的算法尚未详细公布，尽管两者都声称基于 Paxos。ZooKeeper 的算法公布得较为详细，但它与 Paxos 有很大不同。
* 可应用于 Paxos 的性能优化 [18, 19, 3, 25, 1, 27]。
* Oki 和 Liskov 的 **Viewstamped Replication (VR)**，这是与 Paxos 大约同时开发的一种替代共识方法。其原始描述 [29] 与分布式事务协议交织在一起，但在最近的更新 [22] 中，核心共识协议已被分离出来。VR 使用一种基于领导者的方法，与 Raft 有许多相似之处。

**Raft 和 Paxos 之间最大的区别在于 Raft 的强领导者（strong leadership）：** Raft 将领导者选举作为共识协议的重要组成部分，并且它将尽可能多的功能集中在领导者身上。这种方法产生了一个更简单、更容易理解的算法。例如，在 Paxos 中，领导者选举与基本共识协议是**正交的（orthogonal）**：它仅作为性能优化，并不是达成共识所必需的。然而，这导致了额外的机制：Paxos 既包含用于基本共识的两阶段协议，也包含用于领导者选举的独立机制。相比之下，Raft 将领导者选举直接纳入共识算法，并将其用作共识两个阶段中的第一阶段。这导致机制比 Paxos 更少。

像 Raft 一样，VR 和 ZooKeeper 也是基于领导者的，因此它们拥有 Raft 优于 Paxos 的许多优势。然而，**Raft 的机制比 VR 或 ZooKeeper 更少**，因为它最大限度地减少了非领导者的功能。例如，Raft 中的日志条目只沿一个方向流动：即在 AppendEntries RPC 中从领导者向外流动。在 VR 中，日志条目双向流动（领导者可以在选举过程中接收日志条目）；这导致了额外的机制和复杂性。ZooKeeper 的公开描述也提到了向领导者传输和从领导者传出日志条目，但其实践似乎更像 Raft [35]。

**Raft 的消息类型比我们所知的任何其他基于共识的日志复制算法都要少。** 例如，我们统计了 VR 和 ZooKeeper 用于基本共识和成员变更的消息类型（不包括日志压缩和客户端交互，因为这些几乎与算法无关）。VR 和 ZooKeeper 各定义了 10 种不同的消息类型，而 Raft 只有 4 种消息类型（两种 RPC 请求及其响应）。Raft 的消息比其他算法稍微密集一些，但它们在整体上更简单。此外，VR 和 ZooKeeper 是在领导者变更期间传输整个日志的角度来描述的；为了使这些机制实用，还需要额外的消息类型来优化这些机制。

Raft 的强领导者方法简化了算法，但它排除了一些性能优化。例如，**Egalitarian Paxos (EPaxos)** 在某些条件下可以通过无领导者方法实现更高的性能 [27]。EPaxos 利用了状态机命令的**可交换性（commutativity）**。任何服务器都可以通过一轮通信提交命令，只要同时提议的其他命令与该命令是可交换的（即执行顺序不影响结果）。然而，如果并发提议的命令彼此不可交换，EPaxos 需要额外的一轮通信。因为任何服务器都可以提交命令，EPaxos 在服务器之间很好地平衡了负载，并且在广域网（WAN）设置中能够实现比 Raft 更低的延迟。然而，它给 Paxos 增加了显著的复杂性。

在其他工作中，已经提出或实现了几种不同的**集群成员变更**方法，包括 Lamport 的原始提议 [15]、VR [22] 和 SMART [24]。我们为 Raft 选择了**联合共识（joint consensus）**方法，因为它利用了共识协议的其余部分，因此成员变更所需的额外机制非常少。Lamport 基于 (\alpha) 的方法不适用于 Raft，因为它假设可以在没有领导者的情况下达成共识。与 VR 和 SMART 相比，Raft 的重新配置算法的优势在于，成员变更可以在不限制正常请求处理的情况下发生；相比之下，VR 在配置更改期间停止所有正常处理，而 SMART 对未完成请求的数量施加了类似 (\alpha) 的限制。Raft 的方法增加的机制也比 VR 或 SMART 少。

## 11. 结论 (Conclusion)

算法的设计通常以正确性、效率和/或简洁性作为主要目标。尽管这些都是值得追求的目标，但我们认为**可理解性（understandability）**同样重要。在开发人员将算法转化为实际实现之前，其他目标都无法实现，而实际实现必然会偏离并扩展已发布的形式。除非开发人员对算法有深刻的理解并能建立关于它的直觉，否则他们很难在实现中保留其理想的属性。

在本文中，我们解决了分布式共识的问题，在这个领域中，Paxos 这一被广泛接受但晦涩难懂的算法多年来一直困扰着学生和开发人员。我们开发了一种新的算法 **Raft**，并证明了它比 Paxos 更容易理解。我们还认为，Raft 为系统构建提供了更好的基础。将可理解性作为主要设计目标改变了我们设计 Raft 的方式；随着设计的进行，我们发现自己反复重用一些技术，例如**分解问题**和**简化状态空间**。这些技术不仅提高了 Raft 的可理解性，也使我们更容易确信其正确性。

## 12. 致谢 (Acknowledgments)

如果没有 Ali Ghodsi、David Mazières 以及伯克利 CS 294-91 和斯坦福 CS 240 课程学生的支持，这项用户研究是不可能完成的。Scott Klemmer 帮助我们要设计了用户研究，Nelson Ray 就统计分析向我们提供了建议。用户研究中的 Paxos 幻灯片大量借鉴了最初由 Lorenzo Alvisi 制作的幻灯片组。特别感谢 David Mazières 和 Ezra Hoch 发现了 Raft 中微妙的 Bug。

许多人对论文和用户研究材料提供了有益的反馈，包括 Ed Bugnion, Michael Chan, Hugues Evrard, Daniel Giffin, Arjun Gopalan, Jon Howell, Vimalkumar Jeyakumar, Ankita Kejriwal, Aleksandar Kracun, Amit Levy, Joel Martin, Satoshi Matsushita, Oleg Pesok, David Ramos, Robbert van Renesse, Mendel Rosenblum, Nicolas Schiper, Deian Stefan, Andrew Stone, Ryan Stutsman, David Terei, Stephen Yang, Matei Zaharia, 24 位匿名会议审稿人（含重复），特别是我们的指导人（shepherd）Eddie Kohler。Werner Vogels 在推特上发布了早期草稿的链接，这使 Raft 获得了极大的关注。

本工作得到了 Gigascale Systems Research Center 和 Multiscale Systems Center 的支持（这两个中心是 Focus Center Research Program 资助的六个研究中心中的两个，该计划是半导体研究公司的项目）；得到了 STARnet 的支持（这是由 MARCO 和 DARPA 赞助的半导体研究公司项目）；得到了国家科学基金会（National Science Foundation）第 0963859 号拨款的支持；以及来自 Facebook、Google、Mellanox、NEC、NetApp、SAP 和 Samsung 的资助。Diego Ongaro 获得了 Junglee Corporation 斯坦福研究生奖学金的支持。

## 参考文献 (References)

[1] BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., 和 LI, P. **作为高性能数据存储基础的 Paxos 复制状态机 (Paxos replicated state machines as the basis of a high-performance data store)**. 收录于 *Proc. NSDI’11*，USENIX 网络系统设计与实现会议 (2011), USENIX, pp. 141–154.

[2] BURROWS, M. **用于松耦合分布式系统的 Chubby 锁服务 (The Chubby lock service for loosely coupled distributed systems)**. 收录于 *Proc. OSDI’06*，操作系统设计与实现研讨会 (2006), USENIX, pp. 335–350.

[3] CAMARGOS, L. J., SCHMIDT, R. M., 和 PEDONE, F. **多协调者 Paxos (Multicoordinated Paxos)**. 收录于 *Proc. PODC’07*，ACM 分布式计算原理研讨会 (2007), ACM, pp. 316–317.

[4] CHANDRA, T. D., GRIESEMER, R., 和 REDSTONE, J. **Paxos 走向实践：一种工程视角 (Paxos made live: an engineering perspective)**. 收录于 *Proc. PODC’07*，ACM 分布式计算原理研讨会 (2007), ACM, pp. 398–407.

[5] CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., 和 GRUBER, R. E. **Bigtable：一种用于结构化数据的分布式存储系统 (Bigtable: a distributed storage system for structured data)**. 收录于 *Proc. OSDI’06*，USENIX 操作系统设计与实现研讨会 (2006), USENIX, pp. 205–218.

[6] CORBETT, J. C., DEAN, J., EPSTEIN, M., 等. **Spanner：谷歌的全球分布式数据库 (Spanner: Google’s globally-distributed database)**. 收录于 *Proc. OSDI’12*，USENIX 操作系统设计与实现会议 (2012), USENIX, pp. 251–264.

[7] COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., 和 VANZETTO, H. **TLA+ 证明 (TLA+ proofs)**. 收录于 *Proc. FM’12*，形式化方法研讨会 (2012), D. Giannakopoulou 和 D. Méry 编, 计算机科学讲义第 7436 卷, Springer, pp. 147–154.

[8] GHEMAWAT, S., GOBIOFF, H., 和 LEUNG, S.-T. **Google 文件系统 (The Google file system)**. 收录于 *Proc. SOSP’03*，ACM 操作系统原理研讨会 (2003), ACM, pp. 29–43.

[9] GRAY, C., 和 CHERITON, D. **租约：一种用于分布式文件缓存一致性的高效容错机制 (Leases: An efficient fault tolerant mechanism for distributed file cache consistency)**. 收录于 *第 12 届 ACM 操作系统原理研讨会论文集* (1989), pp. 202–210.

[10] HERLIHY, M. P., 和 WING, J. M. **线性化：并发对象的正确性条件 (Linearizability: a correctness condition for concurrent objects)**. *ACM 编程语言与系统汇刊* 12 (1990 年 7 月), 463–492.

[11] HUNT, P., KONAR, M., JUNQUEIRA, F. P., 和 REED, B. **ZooKeeper：互联网规模系统的无等待协调 (ZooKeeper: wait-free coordination for internet-scale systems)**. 收录于 *Proc ATC’10*，USENIX 年度技术会议 (2010), USENIX, pp. 145–158.

[12] JUNQUEIRA, F. P., REED, B. C., 和 SERAFINI, M. **Zab：用于主备系统的高性能广播 (Zab: High-performance broadcast for primary-backup systems)**. 收录于 *Proc. DSN’11*，IEEE/IFIP 国际可信系统与网络会议 (2011), IEEE 计算机协会, pp. 245–256.

[13] KIRSCH, J., 和 AMIR, Y. **面向系统构建者的 Paxos (Paxos for system builders)**. 技术报告 CNDS-2008-2, 约翰霍普金斯大学, 2008.

[14] LAMPORT, L. **分布式系统中的时间、时钟和事件顺序 (Time, clocks, and the ordering of events in a distributed system)**. *ACM 通讯* 21, 7 (1978 年 7 月), 558–565.

[15] LAMPORT, L. **兼职议会 (The part-time parliament)**. *ACM 计算机系统汇刊* 16, 2 (1998 年 5 月), 133–169.

[16] LAMPORT, L. **Paxos 变得简单 (Paxos made simple)**. *ACM SIGACT 新闻* 32, 4 (2001 年 12 月), 18–25.

[17] LAMPORT, L. **规范系统：面向软硬件工程师的 TLA+ 语言和工具 (Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers)**. Addison-Wesley, 2002.

[18] LAMPORT, L. **广义共识和 Paxos (Generalized consensus and Paxos)**. 技术报告 MSR-TR-2005-33, 微软研究院, 2005.

[19] LAMPORT, L. **快速 Paxos (Fast paxos)**. *分布式计算* 19, 2 (2006), 79–103.

[20] LAMPSON, B. W. **如何使用共识构建高可用系统 (How to build a highly available system using consensus)**. 收录于 *分布式算法*, O. Baboaglu 和 K. Marzullo 编. Springer-Verlag, 1996, pp. 1–17.

[21] LAMPSON, B. W. **Paxos 的 ABCD (The ABCD’s of Paxos)**. 收录于 *Proc. PODC’01*，ACM 分布式计算原理研讨会 (2001), ACM, pp. 13–13.

[22] LISKOV, B., 和 COWLING, J. **重新审视 Viewstamped Replication (Viewstamped replication revisited)**. 技术报告 MIT-CSAIL-TR-2012-021, 麻省理工学院, 2012 年 7 月.

[23] **LogCabin 源代码**. `http://github.com/logcabin/logcabin`.

[24] LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., 和 HOWELL, J. **迁移复制的有状态服务的 SMART 方法 (The SMART way to migrate replicated stateful services)**. 收录于 *Proc. EuroSys’06*，ACM SIGOPS/EuroSys 欧洲计算机系统会议 (2006), ACM, pp. 103–115.

[25] MAO, Y., JUNQUEIRA, F. P., 和 MARZULLO, K. **Mencius (孟子)：为 WAN 构建高效的复制状态机 (Mencius: building efficient replicated state machines for WANs)**. 收录于 *Proc. OSDI’08*，USENIX 操作系统设计与实现会议 (2008), USENIX, pp. 369–384.

[26] MAZIÈRES, D. **Paxos 变得实用 (Paxos made practical)**. `http://www.scs.stanford.edu/~dm/home/papers/paxos.pdf`, 2007 年 1 月.

[27] MORARU, I., ANDERSEN, D. G., 和 KAMINSKY, M. **平等主义议会中有更多共识 (There is more consensus in egalitarian parliaments)**. 收录于 *Proc. SOSP’13*，ACM 操作系统原理研讨会 (2013), ACM.

[28] **Raft 用户研究**. `http://ramcloud.stanford.edu/~ongaro/userstudy/`.

[29] OKI, B. M., 和 LISKOV, B. H. **Viewstamped Replication：一种支持高可用分布式系统的新主副本方法 (Viewstamped replication: A new primary copy method to support highly-available distributed systems)**. 收录于 *Proc. PODC’88*，ACM 分布式计算原理研讨会 (1988), ACM, pp. 8–17.

[30] O’NEIL, P., CHENG, E., GAWLICK, D., 和 ONEIL, E. **日志结构合并树 (The log-structured merge-tree, LSM-tree)**. *Acta Informatica* 33, 4 (1996), 351–385.

[31] ONGARO, D. **共识：连接理论与实践 (Consensus: Bridging Theory and Practice)**. 博士论文, 斯坦福大学, 2014 (进行中的工作). `http://ramcloud.stanford.edu/~ongaro/thesis.pdf`.

[32] ONGARO, D., 和 OUSTERHOUT, J. **寻找一种可理解的共识算法 (In search of an understandable consensus algorithm)**. 收录于 *Proc ATC’14*，USENIX 年度技术会议 (2014), USENIX.

[33] OUSTERHOUT, J., AGRAWAL, P., 等. **RAMCloud 的案例 (The case for RAMCloud)**. *ACM 通讯* 54 (2011 年 7 月), 121–130.

[34] **Raft 共识算法网站**. `http://raftconsensus.github.io`.

[35] REED, B. **个人通信 (Personal communications)**, 2013 年 5 月 17日.

[36] ROSENBLUM, M., 和 OUSTERHOUT, J. K. **日志结构文件系统的设计与实现 (The design and implementation of a log-structured file system)**. *ACM 计算机系统汇刊* 10 (1992 年 2 月), 26–52.

[37] SCHNEIDER, F. B. **使用状态机方法实现容错服务：教程 (Implementing fault-tolerant services using the state machine approach: a tutorial)**. *ACM 计算调查* 22, 4 (1990 年 12 月), 299–319.

[38] SHVACHKO, K., KUANG, H., RADIA, S., 和 CHANSLER, R. **Hadoop 分布式文件系统 (The Hadoop distributed file system)**. 收录于 *Proc. MSST’10*，海量存储系统与技术研讨会 (2010), IEEE 计算机协会, pp. 1–10.

[39] VAN RENESSE, R. **Paxos 变得适度复杂 (Paxos made moderately complex)**. 技术报告, 康奈尔大学, 2012.
