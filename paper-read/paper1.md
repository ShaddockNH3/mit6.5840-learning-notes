# paper1 阅读

## 摘要

MapReduce 接口简洁，全自动后台，以及战绩惊人。

## 1 介绍

简单来说，谷歌要经常处理海量数据，计算的任务其实很简单（只是数数每个词出现次数，倒排索引等）。

但是因为数据量太大，需要成千上万台机器一起跑。

为了协调这些机器（并行化、分发数据、处理机器故障），代码写的无比复杂，把简单的业务逻辑淹没了。

所以为了不让工程师被这些“杂活”累死，作者设计了一个新的抽象。

核心思想是把“处理每条记录”的逻辑（Map）和“合并结果”的逻辑（Reduce）提取出来让程序员去写，脏活累活全都隐藏在库里。

> 有点类似于微服务脚手架或者说 gorm-gen 的思想，只需要写数据结构，而不必重复的写错误处理逻辑和搓增删改查轮子。

在拥有几千台机器的集群里面，及其坏掉不是意外，而是日常。

MapReduce 系统处理这个问题的方法很简单但也很有效。

举一个例子，就是假设现在有一个工厂，里面有很多猫咪在干活。3 号猫咪 5 分钟没动静了，那就**换一只新的猫咪**，把 3 号猫负责的事情重做一遍。

也就是重做。因为计算逻辑是确定的，所以谁来做，结果都是一样的。

原文的贡献就是提供一个简单而前大大的接口，即使是一个完全不懂分布式系统的程序员，也可以驾驭成千上万台机器。

## 2 编程模型

### 2.1 把大象装进冰箱里需要几步

这部分主要强调了 MapReduce 的核心逻辑，但是加了两个技术细节。

#### 2.1.1 两个函数的职责

Map 是切分者，它的任务是转换。

输入：一对 $ (k_1, v_1) $ ，即 $ (文件名, 文件内容) $。

输出：一对 $ list(k_2, v_2) $，即 $ [(三文鱼, 1), (金枪鱼, 1), ...] $。

输入和输出的类型通常是不一样的。

Reduce 是规约者，它的任务是聚合。

输入：一个中间键，加上这个键对应的所有值的列表 $ (k_2, list(v_2))，即 $ (三文鱼, [1, 1, 1, ...]) $。

输出：最终结果 $ list(v_2)$，例如 $[10000]$。

#### 2.1.2 神秘的迭代器

中间通过一个迭代器提供给用户 reduce 函数。

这是为了防止一个任务累死，用猫咪工厂的例子，就是假设全世界所有的“三文鱼”纸条有 100 亿张，系统如果把这 100 亿张一次性都堆在一个猫咪的桌子上，桌子回塌的。

也就是说，会内存溢出。

所以，系统用了一个传送带。

对于单只猫咪而言，它不需要 一次性拿所有的纸条，它只需要守在传送带前面，按一下按钮出一张纸条，再按以下出一张。

这样哪怕有无穷无尽的数据，内存里永远只有正在处理的那一张。

### 2.2 数学表达

对于

$$
\begin{array}{ll}
\text{map} (k_1, v_1) & \rightarrow \text{list}(k_2, v_2) \\
\text{reduce} (k_2, \text{list}(v_2)) & \rightarrow \text{list}(v_2)
\end{array}
$$

这个式子而言，有两个需要注意的地方。

第一，map 对输入进行了修改。

简单来说，就是输入的是文档，而输出变成了单词。

第二，reduce 没变。

输入的是单词，输出的是关于这个单词的统计。

### 2.3 举例

这部分是 MapReduce 用途的举例。

1. 在一堆文件中找到包含特定字符串的行。
2. 有哪些网页的链接指向特定网页。
3. 倒排索引，上次搜过的下次直接查表就可以。
4. 分布式排序。
...

## 3 实现

先简单说一下每个参数的作用以及为什么要这么做。

### 3.1 $ M $ 的含义

> 原文： “输入数据自动划分为一组 $ M $ 个切片...由不同的机器并行处理。”

这句话的意思是需要把数据分成 $ M $ 块，也就是把鱼山切成多少份。

假设鱼山有 1000 吨，我们设定每个切片是 64MB。

那么 $ M $ 就可能是 15000 个小块。

我们同时可以派出 15000 只猫，大家一起搬一块，瞬间就可以把第一步做完。

所以，$ M $ 决定了 Map 阶段有多少只猫可以同时开工。

也就是并行度。

### 3.2 $ R $ 的含义

> 原文：“分区数量...由用户指定”

切完鱼之后，我们需要把鱼做成罐头。$ R $ 就是最终需要的东西。

例如，用户指结果最后分成五份，那么最后会有 5 只猫来收集，产出 5 个输出文件。

所以，$ R $ 决定了最后会有多少猫来汇总结果，以及最后会生成多少份文件。

### 3.3 $ hash(key) mod R $

> 原文：使用分区函数将中间键空间划分为 $ R $ 个片段。

为什么要用这个公式？

无论是几号切鱼猫，都有可能切到“三文鱼”。所以，所有切鱼猫手里的“三文鱼”纸条都需要汇总到同一个装罐猫手里。

$ hash(key) $ 把文字变成数字，不同的 key 会产生不同的数字。

$ mod R $ 取模，把对应的哈希值取模后，根据模的数值分配给对应的罐装猫，也就是 $ R $。

### 3.4 小总结

面前有一座巨大的鱼山（海量数据）。为了不累死一只猫，**管家猫 (Master)** 把鱼山切成了 $ M $ 个小堆（比如 15,000 堆）。然后把还在睡觉的几千只 **切鱼猫 (Map Workers)** 和 **装罐猫 (Reduce Workers)** 全部叫醒，准备开工！

**切鱼猫** 领取了自己那堆鱼。它拿起一条鱼，辨认品种，然后写一张小纸条：“三文鱼，1条” $ \langle \text{Salmon}, 1 \rangle $。所有切鱼猫同时干活，互不干扰，速度极快。

切鱼猫不是写一张纸条就跑一趟仓库（太慢了！）。它把纸条先捏在手里（**内存缓冲**），攒够一把了，再放进脚边的篮子（**本地磁盘**）。

篮子早就被隔板分成了 $ R $ 个格子。根据公式（自动分拣器），三文鱼纸条放第 1 格，鳕鱼纸条放第 2 格。放好后，切鱼猫大喊：“管家！我的篮子满了，位置在这里！”

> 可能哈希值算出来，猪咪鱼也在第 1 格里。

**取快递**：负责第 1 格的 **装罐猫** 开着小车，跑遍几千只切鱼猫的篮子，把所有第 1 格（比如全是三文鱼和章鱼）里的纸条都拉回自己家。

**理牌**：拉回来的纸条是乱序的（三文鱼、章鱼、三文鱼...）。装罐猫必须先把它们 **排好队**，让所有“三文鱼”挨在一起，所有“章鱼”挨在一起，方便后续统计。

因为已经排好序了，**装罐猫** 拿起厚厚一叠“三文鱼”的纸条，递给 **计算器 (Reduce 函数)**。计算器疯狂按加法，算出总数。装罐猫把这个数字密封进罐头，写上标签，生成最终报表。

所有猫咪都停手了。**管家猫** 跑去叫醒正在打瞌睡的 **老板 (用户程序)**：“醒醒！活干完了！这是你要的 $ R $ 份最终统计报告！”

### 3.5 Master 数据结构

#### 3.5.1 任务表

Master 只记三件事

1. 这一单到底谁在做（Worker ID）
2. 这一单做到哪一步了（State）
3. 这单是什么（Map 还是 Reduce）

状态只有三种

1. 闲置（Idle）：单子刚打印出来，无人接单。Master 一看到有空闲的猫，马上把单子给它。
2. 进行中（In-progress）：小白猫接了单子，开始疯狂切。Master 盯着这只猫切，如果小白猫半天没动静（超时/故障），Master 就把这个单子的状态改回 Idle，重新发给别的猫去做。
3. 已完成（Complete）：做完了，打个勾。Master 表示这一单不用管了，记录归档。

这张表事实上就是为了“监工”，防止有活没人干，或者有人干一半跑路了。

#### 3.5.2 取货簿

为什么 Master 要当中间商传递信息？

想象一下：

Map Worker (餐厅)：切好的鱼（中间文件）是放在自己店里的（本地磁盘）。

Reduce Worker (骑手)：要负责去取货，但它根本不知道哪家餐厅做好了，也不知道餐厅在哪！

这时候 Master 的作用就来了，它是全知全能的情报中心。

流程如下：

1. Map 汇报做完，通知 Master。
2. Master 记录任务，位置，大小。
3. Reduce 启动，问 Master 怎么走。
4. Master 指路。

#### 3.5.3 小总结

总结一下，Master 的作用就是**监控**以及**物流清单**。

### 3.6 容错

容错主要是 3 种

#### 3.6.1 Worker 挂了

Master 周期性地 ping 每个 worker。如果超时无响应，标记为失效。

- 未完成的任务：无论是 Map 还是 Reduce，全部重置为空闲，重新分配给别人。
- 已完成的 Map 任务：必须重新执行，因为输出存储在故障机器的本地磁盘上，无法访问了。
- 已完成的 Reduce 任务：不需要重新执行。因为输出存储在全局文件系统上，数据是安全的。

#### 3.6.2 Master 挂了

可能性很低。

解决方法是可以类似数据库那样周期性写入检查点。

但更方便的实现是中止，客户端可以选择重试。

#### 3.6.3 故障语义与原子性

这里解决的是**重复执行**带来的问题。

如果一只猫做了一半晕了，另一只猫接手做，或者因为“备用任务”机制导致两只猫同时做同一份活，会不会算重复了。

依赖于**原子提交**。

为了防止一份活被统计两次。

任务在进行的时候都会先写入私有临时文件，也就是猫咪干活的时候都在草稿纸上临时写结果。

Map：告诉 Master 文件名，如果 Master 已经收到过这个任务的完成消息，就忽略新的，也就是谁先做完就听谁的。

Reduce：谁先跑去把草稿纸贴到最终文件上谁就赢了。重命名这个动作是“原子”的，就像盖公章，瞬间完成，不可能出现两只猫同时把纸贴在同一个位置重叠的情况。

#### 3.6.4 免责声明：非确定性与弱语义

> 我们提供较弱但仍然合理的语义。¯\(ツ)/¯

由于重做机制，谷歌官方不推荐在代码里写非确定性行为。

比如说同样的输入，每次运行产生的结果不一样（例如：使用了 `Random()` 随机数、依赖当前时间戳、或者外部变量）。

举一个例子。

假设我们在处理 **Input Record #100**（第100条鱼）。

用户代码逻辑：`Random() > 0.3` ? **切碎** : **切条**。

由于故障恢复或备用任务机制，同一个任务 $ M $ 被执行了两次：

- **执行 1 (Worker A)**：
  - 运气：触发 30% 概率。
  - 输出：**切条 (Strip)**。
  - *状态*：告诉 Master 我做完了。

- **执行 2 (Worker B)**：
  - 运气：触发 70% 概率。
  - 输出：**切碎 (Mince)**。
  - *状态*：也告诉 Master 我做完了。

> **现状**：在系统的中间文件里，第 100 条鱼既是“条”又是“碎”。

下游有多个 Reduce 任务需要读取 $ M $ 的数据，它们去问 Master 要数据地址：

- **Reduce 任务 $ R_1 $**：
  - Master 指路 -> Worker A。
  - $ R_1 $ 读取到：**鱼是条状的**。

- **Reduce 任务 $ R_2 $**：
  - Master 指路 -> Worker B (可能因为 A 后来断联了，或负载均衡)。
  - $ R_2 $ 读取到：**鱼是碎末状的**。

最终输出的文件虽然每部分都是合法的，但整体上是 **自相矛盾** 的，也就是弱语义：

- **输出文件 Part 1** (来自 $R_1$)：记录显示第 100 条鱼被做成了 **鱼柳**。
- **输出文件 Part 2** (来自 $R_2$)：记录显示第 100 条鱼被做成了 **鱼肉松**。

在单机上，程序只跑一次，结果要么全碎，要么全条，具有 **强一致性**。

在 MapReduce 中，因为 **重做 (Re-execution)** 的存在，非确定性代码会导致 **“多版本历史”** 混杂在一起。

#### 3.6.5 总结

为什么要重做 Map？

$$
\begin{aligned}
\text{Map Worker 完成} & \rightarrow \text{数据在 \textbf{本地磁盘 (Local)}} \rightarrow \text{机器挂了} \rightarrow \textbf{数据丢失} \rightarrow \textbf{必须重做} \\
\text{Reduce Worker 完成} & \rightarrow \text{数据在 \textbf{全局系统 (GFS)}} \rightarrow \text{机器挂了} \rightarrow \textbf{数据安全} \rightarrow \textbf{无需重做}
\end{aligned}
$$

这一节的操作核心就是——

坏了就扔，换个新的，最后靠原子操作保证结果唯一。

### 3.7 局部性

> 移动计算比移动数据更便宜。

#### 3.7.1 矛盾

工厂里有几千只猫，但是连接各个工位的走廊（网络带宽）其实很窄。

所有的鱼都堆在工厂门口的一个仓库里，如果几千只猫同时跑去仓库搬鱼，走廊瞬间堵死，根本没时间搬鱼。

所以，搬运海量数据是非常费时费力的。

以一个情景为例，假设原始数据是仓库里放着的一头五吨的鲸鱼，而只让一只猫在厨房对着菜谱（程序）切，搬运成本和计算成本都非常高。

所以不需要去动原始数据，而是改变菜谱的位置，直接去仓库里切。

#### 3.7.2 GFS

在开工之前，卡车司机事实上已经把鱼分好了。

工厂里有 1000 张桌子，司机把几万吨的鱼提前塞到了本地磁盘里。

为了保险，每一份数据都存在 3 个不同的本地磁盘。

管家猫手里有个账本，记者鱼的位置。

现在要切 100 条鱼。

管家去查，发现第 100 条鱼被放在小白猫、小黑猫和大橘猫的磁盘里。

聪明的管家会直接给小白猫下指令说直接让它来切。

小白猫根本不用离开座位，只用读取本地磁盘处理即可，走廊里一只猫都没有，网络非常通畅。

如果小白猫在忙，大橘猫和小黑猫也在忙。

原则就是不能让数据跑太远，只需要让小白猫隔壁的去处理即可，虽然用了一些带宽资源，但是只有几步的距离。

#### Locality

简单来说，Locality 就是数据太重了，搬不动。所以数据在哪，就派几个人去干活，或者派几个离他近的人去干活。

- Input Data = 几 TB 的重物（不动）。
- Program（Map 函数）= 几 KB 的命令。
- Master 策略：把命令飞到重物所在的机器上。

### 3.8 任务粒度

如何当精明的工厂管理员。

- **核心矛盾**：
  - **切得越碎 (M, R 越大)** $\rightarrow$ **负载均衡越好，故障恢复越快**。
  - **切得太碎 (M, R 太大)** $\rightarrow$ **Master 负担太重 (调度开销大，内存存不下)**。

- **最佳实践**：
  1. **Map 任务大小**：与 GFS 的 Block Size (64MB) 保持一致，为了 **最大化局部性 (Locality)**。
  2. **Map 任务总数 $ M $**：通常是机器数量的 **几十倍甚至上百倍**（每台机器处理很多个 Map）。
  3. **Reduce 任务总数 $ R $**：通常是机器数量的 **几倍**（每台机器处理几个 Reduce）。

### 3.9 短板

当一个 MapReduce 操作接近完成时，master 会调度剩余且正在进行中任务的备份执行...

只要主执行或备份执行中的任何一个完成，该任务就被标记为已完成。

简单来说，少数运行极慢的机器（落后者）拖慢了整个作业的完成时间。

原因可能是磁盘损坏、CPU 竞争、软件 Bug 等，导致机器并未宕机但性能极差。

传统的故障检测机制（Ping）认为它是健康的，不会触发重试。

所以只要在任务接近完成的时候，例如 99% 任务已结束，Master 为所有当前仍在运行的任务启动备份任务。

主任务和备份任务同时进行，谁先做完，Master 就采用谁的结果，杀掉另一个正在运行的任务。

增加微量的资源消耗，显著缩短作业的总完成时间。

Sort Benchmark 中，该机制减少了 44% 的耗时。

## 4 改进

一般的 MapReduce 已经足够使用，以下是改进。

### 4.1 自定义分区

前文求哈希使用的是 $ hash(key) mod R $，可以保证公平性。

但是有时候公平并不是最重要的，归类才是，例如分类任务是 URL 的时候。

有以下三个网页数据：

- <www.google.com/about>
- <www.yahoo.com/news>
- <www.google.com/maps>

如果采用默认的哈希算法，那么这三个很可能被分配到不同的文件里。

如果我想看谷歌网站的所有数据报告，那么就得去翻很多不同的文件。

解决的方法也很简单，提供一个特殊的分区函数，只看主机名即可。

例如 $ hash(Hostname(urlkey)) mod R $

也就是只看 <www.google.com>，这样分区是一样的。

### 4.2 排序保证

输入给 Reduce 函数的数据流已经是排好序的，所以 Reduce 函数只要按顺序处理并输出，最终生成的 Output 文件就自动是排好序的

### 4.3 Combiner

用于简化大量的重复性任务。

例如经典例子，求单词的数量。如果某些单词，例如 the，词频出现极高，那么可以先在本地合成一下，也就是在执行 map 的机器偷偷合一下。

逻辑通常和 reduce 类似。

唯一的限制是数学限制，求和可以，求均值不行。

例如 Map 算出：10、20、100

Combiner 先求前两个的均值，然后再求后面一个的均值，Reduce 收到的则是 15、100。

求完均值后显然，43 ≠ 57。

运算必须满足结合律和交换律（如求和、最大值、最小值）。

### 4.4 输入和输出

简单来说，这一部分的问题有两个。

第一，转接头。

MapReduce 的 Map 函数本质上是 Key 和 Value，只要提供一个 Reader（转换器），MapReduce 就可以工作。

第二，智能切分。

处理文本数据的时候，可能会被切分为：

```txt
床前明月光，
疑是地 | 上霜。 <--- （这里是 64MB 的切割线）
举头望明月，
低头思故乡。
```

不做特殊处理，两句诗的内容可能就丢了，或者处理出来两个错误的碎片。

因此，Reader 接口里有一个越界规则。猫咪 A 的 64MB 读完了，但是最后几个字不在自己的内存里，所以跨过界限，往猫咪 B 的地盘里多读几个字。

同理，猫咪 B 发现前几个字是半句话，所以直接抛弃。

因此，哪怕物理上是切分开的，但是逻辑上每一行的数据依旧是完整的。

### 4.5 副作用

什么是副作用？

正统的 MapReduce 的流程是封闭的，但是程序员如果想在 Map 函数里面加点东西，例如在看到变质的鱼之后，除了把它扔掉，请记一笔。

也就是输出日志。

这种行为就是副作用，不在标准程序里。

此前有提到一个概念叫备份任务，为了防慢，系统同时派两个猫咪去处理一个问题。如果没有规则，那么他们会打入同一个 `bad_fish.log`，文件内容乱套。

所以就会使用“打草稿+瞬间替换”的模式，你想干私活可以，不管你，你可以这么做：

写日记的时候，不要直接写在 `final.log` 上，而是写在自己私有的 `final.log.temp.workerA` 上。

同理，另一个 worker 写在 `final.log.temp.workerB` 上。

互不干扰，岁月静好。

等写完了，任务结束，向系统发出指令说把自己的 log 改为正确的 log 名。

如果小白猫先改名成功了，final.log 就是小白猫的版本。

如果是小黑猫后改名，它可能会覆盖小白猫的，或者被系统拒绝（取决于具体实现），但无论如何，最终的文件一定是一份完整的、干净的日记，绝不会是两份日记的混合体！

当然，也别太贪心，不支持对单个任务产生的多个输出文件进行原子两阶段提交。

意思就是不能让猫咪写两份日记。

这需要复杂的两阶段提交协议，MapReduce 为了保持简单高效，不支持这个。

如果非要这么做，请保持人物的确定性。

### 4.6 跳过坏记录

如果产生了确定性崩溃，也就是必然触发代码 bug 的情况，导致 task 无限失败。

Master 孜孜不倦地派出 worker，但是 worker 一直倒下，直到全部挂完为止。

一般来说是数据有问题。

以鱼工厂为例，程序员撰写的 Map 代码里有一个 Bug：只要碰到骨头长得很奇怪的鱼，程序就会崩溃，而 #100 就是这个鱼。

如果没有这个机制，那么就会源源不断地挂，为了打破这个死循环，必须在桌子上的黑匣子按下计数器。

```txt
“我现在要处理第 99 号鱼了。”
（切完了，安全）
“我现在要处理第 100 号鱼了。”
（注意！这里还没开始切，只是先记录了编号。）
```

这个时候程序触发 bug，但是在挂前的很短时间内，操作系统出发了信号处理程序，告诉 master #100 导致它挂了。

管家收到了一眼之后，会再次派人尝试，再次派人尝试依旧相同挂，那么就会在下一个猫执行之前，给它下达跳过 #100。

这样，以丢失少量数据为代价，换取整个作业能够成功完成。

这个场景是可选的，适用于对数据完整性要求不苛刻的大规模统计任务，不适用于银行。

前者，100 亿少 3 条无所谓，后者只能挂。

### 4.7 本地执行

在分布式系统中，由于工作分配决策是 master 动态做出的，所以调试非常棘手。

工厂里有 2000 只猫同时在跑来跑去。

突然，整个系统报警说“出错了！”

想知道是哪只猫出的错？

也许是第 138 号机器上的小黑猫？

但等跑过去，Master 可能已经把小黑猫调度到第 500 号机器去干别的活了。

所以这根本没法查！

不可能给 2000 台机器都挂上 GDB，那会让整个工厂瘫痪的。

解决方法就是提供一个单机版模式，原本有成千上万只猫在并行乱跑，现在只有一个猫在笔记本上跑。

这只猫会把原本分配给几千只猫的活从头到尾自己做一遍。

如果已经知道是处理第 100 号鱼的时候出现问题，那就直接演示，瞬间复现 bug。

在本地环境里也可以很轻松的使用 GDB。

### 4.8 状态信息

Master 运行一个内部 HTTP 服务器，并导出一组状态页面供人查阅。

也就是说，它给自己电脑上 开了一个网页，只要用浏览器访问这个网页，就可以看到整个工厂的上帝视角。

有进度条，计算器，速度表等，可以去偷窥每一只猫都在干啥。

顶层状态页面显示了哪些 worker 发生了故障，此时这种信息就很有用。

### 4.9 计数器

全自动分布式点数机

工厂里几千只猫在切鱼，虽然只想知道“最后有多少罐头”，但有时候也很好奇中间的过程，比如：

- “这里面到底有多少条是红色的鱼？”
- “有多少条鱼是坏掉的？”
- “一共切了多少刀？”

如果让自己去问每一只猫，那肯定累死了。所以 Google 设计了这个自动统计功能。

事实上就是 给每个猫咪挂上一个或者几个计数器，当看到一个大写开头的单词就按计数器。

来自各个 worker 机器的计数器值会周期性地传播给 master，在日常 ping 的时候 ，顺便把这个信息带回去。

最难的地方在去重。

主要是 Master 去重，只看最先提交的哪个并且合入统计。

这一章的目的在于查账，也就是对于健全性检查。例如进货了 1000 条鱼，最后只处理了 998，那么可以很快知道有 bug 存在。

## 5 秀肌肉

快，防慢，防死

## 6 实战

谷歌内部在这东西上线了之后，疯狂重写，代码简化，流程解耦，运维自动化。

## 7 杂鱼前辈

### MPI & BSP 太麻烦的杂鱼

> 像 MPI 这种老掉牙的古董，居然好意思把“容错”和“数据流”这种麻烦事全丢给程序员？哈？你也配？只有 MapReduce 这种贴心的天才，才会把那些复杂的细节统统藏起来……毕竟指望杂鱼大叔你自己写代码不改 Bug，简直是在做梦嘛，嘻嘻♪

### Active Disks 太烧钱的杂鱼

> Active Disks 居然还要依赖那些死贵的专用硬件才懂得什么叫“就近计算”？真是个不知人间疾苦的富二代呢。睁大眼睛看看 MapReduce，用最廉价的破烂 PC 也能做到一样的局部性优化，这才叫性价比的碾压，懂不懂啊？杂鱼～

### Charlotte 不懂变通的杂鱼

> Charlotte 虽然想到了备份任务，但脑子简直就是块木头，遇到有毒的数据就知道死磕，陷入无限重启的死循环里出不来。哪像 MapReduce 脑子这么好使，发现不对劲直接一脚踢开就好啦，连这点随机应变都不会，真是笨得可爱呢♡

### River 想太多的杂鱼

> River 为了解决机器快慢不均，居然搞了一套复杂得要命的调度算法，累不累啊？MapReduce 只要把任务切成几万个小碎块就能解决的事，它非要在那故弄玄虚。把简单的事情搞复杂，这就是想太多的下场哦！噗噗！

### 缝合怪才是最强的

> 听好了，MapReduce 可不是石头里蹦出来的魔法，它可是毫不客气地把过去几十年的智慧全抢过来了哦！但它最强的地方，是把这些复杂的玩意儿狠狠地 简化 又巧妙地 组合 在了一起，专门用来解决“在超大规模廉价集群上处理数据”这个新问题。能把一堆旧零件组装成碾压一切的新武器，这种天才般的实力……反正杂鱼大叔你这辈子都学不会的啦～♪

## 8. 结论

### Why Successful

1. 傻瓜都能用
2. 万能钥匙，不止能数单词，还能做搜索索引、排序、机器学习等等
3. 普通机器集群

### Lessons Learned

1. 管的严，才不乱
2. 路费很贵，省着点花
3. 多留一手，不怕慢

面对海量数据和不可靠的硬件，最强大的武器不是复杂的控制，而是简洁的抽象和聪明的取舍。
